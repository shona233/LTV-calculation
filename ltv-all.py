import streamlit as st
import os
import pandas as pd
import numpy as np
from pathlib import Path
import warnings
import datetime
import tempfile
import zipfile
import io
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt
import matplotlib as mpl
import re
from matplotlib.font_manager import FontProperties

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl.styles.stylesheet')
warnings.filterwarnings('ignore', category=UserWarning,
                        message="Could not infer format, so each element will be parsed individually")

# è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="LTV Analytics Platform",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main-header {
        padding: 1.5rem 0rem 1rem 0rem;
        border-bottom: 2px solid #f0f2f6;
        margin-bottom: 2rem;
    }
    .metric-container {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
        margin: 0.5rem 0;
    }
    .status-card {
        background-color: white;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #e1e5e9;
        margin: 0.5rem 0;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    .progress-item {
        padding: 0.5rem 0;
        border-bottom: 1px solid #f0f2f6;
    }
    .stButton > button {
        width: 100%;
        border-radius: 0.25rem;
        border: 1px solid #1f77b4;
        background-color: #1f77b4;
        color: white;
        font-weight: 500;
    }
    .stButton > button:hover {
        background-color: #0d5aa7;
        border-color: #0d5aa7;
    }
    .sidebar .sidebar-content {
        background-color: #f8f9fa;
    }
    h1, h2, h3 {
        color: #2c3e50;
        font-weight: 600;
    }
    .stSelectbox label, .stMultiselect label, .stFileUploader label {
        font-weight: 500;
        color: #34495e;
    }
</style>
""", unsafe_allow_html=True)

# ä¸»æ ‡é¢˜
st.markdown('<div class="main-header">', unsafe_allow_html=True)
st.title("LTV Analytics Platform")
st.markdown("**ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†æç³»ç»Ÿ**")
st.markdown('</div>', unsafe_allow_html=True)

# ä¾§è¾¹æ å¯¼èˆª
st.sidebar.markdown("### åˆ†ææµç¨‹")
page = st.sidebar.selectbox(
    "é€‰æ‹©åˆ†ææ¨¡å—",
    [
        "æ•°æ®ä¸Šä¼ ä¸æ±‡æ€»", 
        "ç•™å­˜ç‡è®¡ç®—", 
        "LTæ‹Ÿåˆåˆ†æ", 
        "ARPUè®¡ç®—", 
        "LTVç»“æœæŠ¥å‘Š"
    ],
    label_visibility="collapsed"
)

# åˆå§‹åŒ–session state
session_keys = [
    'channel_mapping', 'merged_data', 'retention_data', 
    'lt_results', 'arpu_data', 'ltv_results'
]
for key in session_keys:
    if key not in st.session_state:
        st.session_state[key] = None

# ===== æ•°æ®æ•´åˆåŠŸèƒ½ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰=====
def standardize_output_columns(df):
    """æ ‡å‡†åŒ–è¾“å‡ºåˆ—ç»“æ„ï¼Œç¡®ä¿åŒ…å«æŒ‡å®šçš„åˆ—é¡ºåº"""
    target_columns = [
        'æ•°æ®æ¥æº', 'date', 'æ•°æ®æ¥æº_date',
        '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',
        '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',
        '21', '22', '23', '24', '25', '26', '27', '28', '29', '30',
        'æ•°æ®æ¥æº_æ—¥æœŸ', 'æ—¥æœŸ', 'å›ä¼ æ–°å¢æ•°', 'is_target_month', 'month', 'stat_date',
        'new', 'new_retain_1', 'new_retain_2', 'new_retain_3', 'new_retain_4', 'new_retain_5',
        'new_retain_6', 'new_retain_7', 'new_retain_8', 'new_retain_9', 'new_retain_10',
        'new_retain_11', 'new_retain_12', 'new_retain_13', 'new_retain_14', 'new_retain_15',
        'new_retain_16', 'new_retain_17', 'new_retain_18', 'new_retain_19', 'new_retain_20',
        'new_retain_21', 'new_retain_22', 'new_retain_23', 'new_retain_24', 'new_retain_25',
        'new_retain_26', 'new_retain_27', 'new_retain_28', 'new_retain_29', 'new_retain_30'
    ]

    result_df = pd.DataFrame()
    
    for col_name in target_columns:
        if col_name == 'æ•°æ®æ¥æº':
            result_df[col_name] = df[col_name] if col_name in df.columns else ''
        elif col_name == 'date':
            if col_name in df.columns:
                result_df[col_name] = df[col_name]
            elif 'stat_date' in df.columns:
                result_df[col_name] = df['stat_date']
            else:
                result_df[col_name] = ''
        elif col_name == 'æ•°æ®æ¥æº_date':
            data_source = df['æ•°æ®æ¥æº'] if 'æ•°æ®æ¥æº' in df.columns else ''
            date_col = df['date'] if 'date' in df.columns else (df['stat_date'] if 'stat_date' in df.columns else '')
            result_df[col_name] = data_source.astype(str) + date_col.astype(str)
        elif col_name == 'æ•°æ®æ¥æº_æ—¥æœŸ':
            data_source = df['æ•°æ®æ¥æº'] if 'æ•°æ®æ¥æº' in df.columns else ''
            date_col = df['æ—¥æœŸ'] if 'æ—¥æœŸ' in df.columns else (
                df['date'] if 'date' in df.columns else (df['stat_date'] if 'stat_date' in df.columns else ''))
            result_df[col_name] = data_source.astype(str) + date_col.astype(str)
        else:
            result_df[col_name] = df[col_name] if col_name in df.columns else ''

    for col in df.columns:
        if col not in target_columns:
            result_df[col] = df[col]

    return result_df

def integrate_excel_files_streamlit(uploaded_files, target_month=None):
    """Streamlitç‰ˆæœ¬çš„Excelæ–‡ä»¶æ•´åˆå‡½æ•°"""
    if target_month is None:
        today = datetime.datetime.now()
        first_day_of_current_month = today.replace(day=1)
        first_day_of_last_month = (first_day_of_current_month - datetime.timedelta(days=1)).replace(day=1)
        last_day_of_two_months_ago = first_day_of_last_month - datetime.timedelta(days=1)
        target_month = last_day_of_two_months_ago.strftime("%Y-%m")

    all_data = pd.DataFrame()
    processed_count = 0

    for uploaded_file in uploaded_files:
        source_name = os.path.splitext(uploaded_file.name)[0]
        
        try:
            xls = pd.ExcelFile(uploaded_file)
            sheet_names = xls.sheet_names

            ocpx_sheet = None
            channel_sheet = None

            for sheet in sheet_names:
                if "ocpxç›‘æµ‹ç•™å­˜æ•°" in sheet:
                    ocpx_sheet = sheet
                if "ç›‘æµ‹æ¸ é“å›ä¼ é‡" in sheet:
                    channel_sheet = sheet

            file_data = None

            if ocpx_sheet:
                ocpx_df = pd.read_excel(uploaded_file, sheet_name=ocpx_sheet)
                
                if channel_sheet:
                    channel_df = pd.read_excel(uploaded_file, sheet_name=channel_sheet)
                    if len(channel_df.columns) >= 2:
                        last_two_cols = channel_df.iloc[:, -2:]
                        ocpx_df = ocpx_df.copy()
                        for col_name in last_two_cols.columns:
                            ocpx_df[col_name] = last_two_cols[col_name].values[:len(ocpx_df)] if len(
                                last_two_cols) >= len(ocpx_df) else last_two_cols[col_name].values.tolist() + [
                                None] * (len(ocpx_df) - len(last_two_cols))
                
                file_data = ocpx_df
            else:
                file_data = pd.read_excel(uploaded_file, sheet_name=0)

            if file_data is not None and not file_data.empty:
                file_data_copy = file_data.copy()

                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°æ ¼å¼è¡¨
                has_stat_date = 'stat_date' in file_data_copy.columns
                retain_columns = [f'new_retain_{i}' for i in range(1, 31)]
                has_retain_columns = any(col in file_data_copy.columns for col in retain_columns)

                if has_stat_date and has_retain_columns:
                    standardized_data = file_data_copy.copy()

                    if 'new' in standardized_data.columns:
                        standardized_data['å›ä¼ æ–°å¢æ•°'] = standardized_data['new']

                    for i in range(1, 31):
                        retain_col = f'new_retain_{i}'
                        if retain_col in standardized_data.columns:
                            standardized_data[str(i)] = standardized_data[retain_col]

                    date_col = 'stat_date'
                    standardized_data[date_col] = pd.to_datetime(standardized_data[date_col], errors='coerce')
                    standardized_data[date_col] = standardized_data[date_col].dt.strftime('%Y-%m-%d')
                    standardized_data['æ—¥æœŸ'] = standardized_data[date_col]
                    standardized_data['month'] = standardized_data[date_col].str[:7]

                    filtered_data = standardized_data[standardized_data['month'] == target_month].copy()

                    if not filtered_data.empty:
                        filtered_data.insert(0, 'æ•°æ®æ¥æº', source_name)
                        filtered_data['date'] = filtered_data['stat_date']
                        all_data = pd.concat([all_data, filtered_data], ignore_index=True)
                        processed_count += 1

                else:
                    # å¤„ç†æ—§æ ¼å¼è¡¨çš„é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰
                    retention_col = None
                    for col in file_data_copy.columns:
                        if 'ç•™å­˜å¤©æ•°' in str(col):
                            retention_col = col
                            break

                    date_col = None
                    for col in file_data_copy.columns:
                        if 'æ—¥æœŸ' in str(col):
                            date_col = col
                            break

                    if len(file_data_copy.columns) >= 2:
                        second_col = file_data_copy.columns[1]
                        file_data_copy['å›ä¼ æ–°å¢æ•°'] = file_data_copy[second_col]

                    if date_col:
                        try:
                            file_data_copy[date_col] = pd.to_datetime(file_data_copy[date_col], errors='coerce')
                            file_data_copy['month'] = file_data_copy[date_col].dt.strftime('%Y-%m')
                            filtered_data = file_data_copy[file_data_copy['month'] == target_month].copy()
                            
                            if not filtered_data.empty:
                                filtered_data.insert(0, 'æ•°æ®æ¥æº', source_name)
                                if retention_col:
                                    filtered_data.rename(columns={retention_col: 'date'}, inplace=True)
                                else:
                                    filtered_data['date'] = filtered_data[date_col].dt.strftime('%Y-%m-%d')
                                
                                all_data = pd.concat([all_data, filtered_data], ignore_index=True)
                                processed_count += 1
                        except:
                            # å¦‚æœæ—¥æœŸå¤„ç†å¤±è´¥ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®
                            file_data_copy.insert(0, 'æ•°æ®æ¥æº', source_name)
                            if retention_col:
                                file_data_copy.rename(columns={retention_col: 'date'}, inplace=True)
                            all_data = pd.concat([all_data, file_data_copy], ignore_index=True)
                            processed_count += 1

        except Exception as e:
            st.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‡ºé”™: {str(e)}")

    if not all_data.empty:
        standardized_df = standardize_output_columns(all_data)
        return standardized_df, processed_count
    else:
        return None, 0

def parse_channel_mapping(channel_df):
    """è§£ææ¸ é“æ˜ å°„è¡¨ï¼Œæ”¯æŒæ–°çš„æ ¼å¼ï¼šç¬¬ä¸€åˆ—ä¸ºæ¸ é“åï¼Œåç»­åˆ—ä¸ºæ¸ é“å·"""
    pid_to_channel = {}
    
    for _, row in channel_df.iterrows():
        channel_name = str(row.iloc[0]).strip()  # ç¬¬ä¸€åˆ—æ˜¯æ¸ é“å
        
        # è·³è¿‡ç©ºè¡Œæˆ–æ— æ•ˆçš„æ¸ é“å
        if pd.isna(channel_name) or channel_name == '' or channel_name == 'nan':
            continue
            
        # å¤„ç†åç»­åˆ—çš„æ¸ é“å·
        for col_idx in range(1, len(row)):
            pid = row.iloc[col_idx]
            
            # å¤„ç†å„ç§å¯èƒ½çš„ç©ºå€¼è¡¨ç¤º
            if pd.isna(pid) or str(pid).strip() in ['', 'nan', 'ã€€', ' ']:
                continue
                
            pid_str = str(pid).strip()
            if pid_str:
                pid_to_channel[pid_str] = channel_name
    
    return pid_to_channel

# ===== LTæ‹Ÿåˆè®¡ç®—åŠŸèƒ½ =====
def power_function(x, a, b):
    """å¹‚å‡½æ•°ï¼šy = a * x^b"""
    return a * np.power(x, b)

def exponential_function(x, c, d):
    """æŒ‡æ•°å‡½æ•°ï¼šy = c * exp(d * x)"""
    return c * np.exp(d * x)

def calculate_cumulative_lt(days_array, rates_array, target_days):
    """è®¡ç®—æŒ‡å®šå¤©æ•°çš„ç´¯ç§¯LTå€¼"""
    result = {}
    for day in target_days:
        idx = np.searchsorted(days_array, day, side='right')
        if idx > 0:
            cumulative_lt = 1.0 + np.sum(rates_array[1:idx])
            result[day] = cumulative_lt
    return result

def calculate_lt(data, channel_name, lt_years=5, return_curve_data=False, key_days=None):
    """è®¡ç®—LTå€¼"""
    CHANNEL_RULES = {
        "åä¸º": {"stage_2": [30, 120], "stage_3_base": [120, 220]},
        "å°ç±³": {"stage_2": [30, 190], "stage_3_base": [190, 290]},
        "oppo": {"stage_2": [30, 160], "stage_3_base": [160, 260]},
        "vivo": {"stage_2": [30, 150], "stage_3_base": [150, 250]},
        "iphone": {"stage_2": [30, 150], "stage_3_base": [150, 250], "stage_2_func": "log"},
        "å…¶ä»–": {"stage_2": [30, 100], "stage_3_base": [100, 200]}
    }

    if re.search(r'\d+æœˆåä¸º$', channel_name):
        rules = CHANNEL_RULES["åä¸º"]
    elif re.search(r'\d+æœˆå°ç±³$', channel_name):
        rules = CHANNEL_RULES["å°ç±³"]
    elif re.search(r'\d+æœˆoppo$', channel_name) or re.search(r'\d+æœˆOPPO$', channel_name):
        rules = CHANNEL_RULES["oppo"]
    elif re.search(r'\d+æœˆvivo$', channel_name):
        rules = CHANNEL_RULES["vivo"]
    elif re.search(r'\d+æœˆ[iI][pP]hone$', channel_name):
        rules = CHANNEL_RULES["iphone"]
    else:
        rules = CHANNEL_RULES["å…¶ä»–"]
    
    stage_2_start, stage_2_end = rules["stage_2"]
    stage_3_base_start, stage_3_base_end = rules["stage_3_base"]
    max_days = lt_years * 365

    days = data["ç•™å­˜å¤©æ•°"].values
    rates = data["ç•™å­˜ç‡"].values

    fit_params = {}

    try:
        popt_power, _ = curve_fit(power_function, days, rates)
        a, b = popt_power
        fit_params["power"] = {"a": a, "b": b}

        days_full = np.arange(1, 31)
        rates_full = power_function(days_full, a, b)
        lt1_to_30 = np.sum(rates_full)
    except Exception as e:
        st.error(f"{channel_name} ç¬¬ä¸€é˜¶æ®µæ‹Ÿåˆå¤±è´¥ï¼š{e}")
        lt1_to_30 = 0.0

    try:
        days_stage_2 = np.arange(stage_2_start, stage_2_end + 1)
        rates_stage_2 = power_function(days_stage_2, a, b)
        lt_stage_2 = np.sum(rates_stage_2)
    except Exception as e:
        st.error(f"{channel_name} ç¬¬äºŒé˜¶æ®µé¢„æµ‹å¤±è´¥ï¼š{e}")
        lt_stage_2 = 0.0

    try:
        days_stage_3_base = np.arange(stage_3_base_start, stage_3_base_end + 1)
        rates_stage_3_base = power_function(days_stage_3_base, a, b)

        initial_c = rates_stage_3_base[0]
        initial_d = -0.001
        popt_exp, _ = curve_fit(
            exponential_function,
            days_stage_3_base,
            rates_stage_3_base,
            p0=[initial_c, initial_d],
            bounds=([0, -np.inf], [np.inf, 0])
        )
        c, d = popt_exp
        fit_params["exponential"] = {"c": c, "d": d}
        days_stage_3 = np.arange(stage_3_base_start, max_days + 1)
        rates_stage_3 = exponential_function(days_stage_3, c, d)
        lt_stage_3 = np.sum(rates_stage_3)
    except Exception as e:
        st.warning(f"{channel_name} ç¬¬ä¸‰é˜¶æ®µæŒ‡æ•°æ‹Ÿåˆå¤±è´¥ï¼Œä½¿ç”¨å¹‚å‡½æ•°é¢„æµ‹")
        days_stage_3 = np.arange(stage_3_base_start, max_days + 1)
        rates_stage_3 = power_function(days_stage_3, a, b)
        lt_stage_3 = np.sum(rates_stage_3)

    total_lt = 1.0 + lt1_to_30 + lt_stage_2 + lt_stage_3

    if return_curve_data:
        all_days = np.concatenate([days_full, days_stage_2, days_stage_3])
        all_rates = np.concatenate([rates_full, rates_stage_2, rates_stage_3])

        sort_idx = np.argsort(all_days)
        all_days = all_days[sort_idx]
        all_rates = all_rates[sort_idx]

        max_idx = np.searchsorted(all_days, lt_years * 365, side='right')
        all_days = all_days[:max_idx]
        all_rates = all_rates[:max_idx]

        key_days_lt = {}
        if key_days:
            key_days_lt = calculate_cumulative_lt(all_days, all_rates, key_days)

        return total_lt, all_days, all_rates, key_days_lt, fit_params

    return total_lt

def create_visualization_plots(visualization_data_2y, visualization_data_5y, original_data):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    figures = []
    
    # 1. æ‹Ÿåˆæ•ˆæœæ¯”è¾ƒå›¾
    channels = sorted(visualization_data_2y.keys(), key=lambda x: visualization_data_2y[x]['lt'])
    n_channels = len(channels)
    n_cols = 3
    n_rows = (n_channels + n_cols - 1) // n_cols

    fig1, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows), squeeze=False)
    
    for i, channel_name in enumerate(channels):
        row = i // n_cols
        col = i % n_cols
        ax = axes[row, col]

        data = visualization_data_2y[channel_name]

        if channel_name in original_data:
            ax.scatter(
                original_data[channel_name]["days"],
                original_data[channel_name]["rates"],
                color='red',
                s=50,
                alpha=0.7,
                label='å®é™…æ•°æ®'
            )

        fit_days = data["days"]
        fit_rates = data["rates"]
        idx_100 = np.searchsorted(fit_days, 100, side='right')
        ax.plot(
            fit_days[:idx_100],
            fit_rates[:idx_100],
            color='blue',
            linewidth=2,
            label='æ‹Ÿåˆæ›²çº¿'
        )

        ax.set_title(f'{channel_name} (LT={data["lt"]:.2f})')
        ax.set_xlabel('ç•™å­˜å¤©æ•°')
        ax.set_ylabel('ç•™å­˜ç‡')
        ax.set_ylim(0, 0.6)
        ax.set_yticks([0, 0.15, 0.3, 0.45, 0.6])
        ax.set_yticklabels(['0%', '15%', '30%', '45%', '60%'])
        ax.grid(True, ls="--", alpha=0.3)
        ax.legend()

    for i in range(len(channels), n_rows * n_cols):
        row = i // n_cols
        col = i % n_cols
        fig1.delaxes(axes[row, col])

    plt.tight_layout()
    figures.append(("æ‹Ÿåˆæ•ˆæœæ¯”è¾ƒ", fig1))

    # 2. 2å¹´LTæ›²çº¿
    sorted_channels_2y = sorted(visualization_data_2y.items(), key=lambda x: x[1]['lt'])
    fig2 = plt.figure(figsize=(14, 8))
    ax2 = fig2.add_subplot(111)
    colors = plt.cm.tab10.colors

    for idx, (channel_name, data) in enumerate(sorted_channels_2y):
        color = colors[idx % len(colors)]
        ax2.plot(
            data["days"],
            data["rates"],
            label=f"{channel_name} (LT={data['lt']:.2f})",
            color=color,
            linewidth=2
        )

    ax2.set_ylim(0, 0.6)
    ax2.set_yticks([0, 0.15, 0.3, 0.45, 0.6])
    ax2.set_yticklabels(['0%', '15%', '30%', '45%', '60%'])
    ax2.grid(True, ls="--", alpha=0.5)
    ax2.set_xlabel('ç•™å­˜å¤©æ•°')
    ax2.set_ylabel('ç•™å­˜ç‡')
    ax2.set_title('æ‰€æœ‰æ¸ é“2å¹´LTç•™å­˜æ›²çº¿æ¯”è¾ƒ (æŒ‰LTå€¼ä»ä½åˆ°é«˜æ’åº)')
    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    figures.append(("2å¹´LTæ›²çº¿", fig2))

    # 3. 5å¹´LTæ›²çº¿
    sorted_channels_5y = sorted(visualization_data_5y.items(), key=lambda x: x[1]['lt'])
    fig3 = plt.figure(figsize=(14, 8))
    ax3 = fig3.add_subplot(111)

    for idx, (channel_name, data) in enumerate(sorted_channels_5y):
        color = colors[idx % len(colors)]
        ax3.plot(
            data["days"],
            data["rates"],
            label=f"{channel_name} (LT={data['lt']:.2f})",
            color=color,
            linewidth=2
        )

    ax3.set_ylim(0, 0.6)
    ax3.set_yticks([0, 0.15, 0.3, 0.45, 0.6])
    ax3.set_yticklabels(['0%', '15%', '30%', '45%', '60%'])
    ax3.grid(True, ls="--", alpha=0.5)
    ax3.set_xlabel('ç•™å­˜å¤©æ•°')
    ax3.set_ylabel('ç•™å­˜ç‡')
    ax3.set_title('æ‰€æœ‰æ¸ é“5å¹´LTç•™å­˜æ›²çº¿æ¯”è¾ƒ (æŒ‰LTå€¼ä»ä½åˆ°é«˜æ’åº)')
    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    figures.append(("5å¹´LTæ›²çº¿", fig3))

    return figures

# ===== é¡µé¢é€»è¾‘ =====
if page == "æ•°æ®ä¸Šä¼ ä¸æ±‡æ€»":
    st.header("æ•°æ®ä¸Šä¼ ä¸æ±‡æ€»")
    st.markdown("ä¸Šä¼ æ¸ é“æ˜ å°„è¡¨å’Œç•™å­˜æ•°æ®æ–‡ä»¶ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ±‡æ€»å¹¶æ ‡å‡†åŒ–æ•°æ®æ ¼å¼ã€‚")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("æ¸ é“æ˜ å°„è¡¨")
        st.markdown("**æ ¼å¼è¦æ±‚ï¼š** ç¬¬ä¸€åˆ—ä¸ºæ¸ é“åç§°ï¼Œåç»­åˆ—ä¸ºå¯¹åº”çš„æ¸ é“å·")
        
        channel_mapping_file = st.file_uploader(
            "é€‰æ‹©æ¸ é“æ˜ å°„è¡¨æ–‡ä»¶",
            type=['xlsx', 'xls'],
            key="channel_mapping"
        )
        
        if channel_mapping_file:
            try:
                channel_mapping_df = pd.read_excel(channel_mapping_file)
                st.session_state.channel_mapping = channel_mapping_df
                
                # è§£ææ¸ é“æ˜ å°„
                pid_mapping = parse_channel_mapping(channel_mapping_df)
                
                st.success(f"æ¸ é“æ˜ å°„è¡¨ä¸Šä¼ æˆåŠŸï¼Œå…±è§£æå‡º {len(pid_mapping)} ä¸ªæ¸ é“å·æ˜ å°„")
                
                with st.expander("æŸ¥çœ‹æ˜ å°„è¡¨é¢„è§ˆ"):
                    st.dataframe(channel_mapping_df.head(10))
                    
                with st.expander("æŸ¥çœ‹è§£æåçš„æ¸ é“å·æ˜ å°„"):
                    mapping_preview = pd.DataFrame(list(pid_mapping.items()), 
                                                 columns=['æ¸ é“å·', 'æ¸ é“åç§°'])
                    st.dataframe(mapping_preview.head(20))
                    
            except Exception as e:
                st.error(f"è¯»å–æ¸ é“æ˜ å°„è¡¨å¤±è´¥ï¼š{e}")
        
        st.subheader("ç•™å­˜æ•°æ®æ–‡ä»¶")
        st.markdown("**æ”¯æŒæ ¼å¼ï¼š** OCPXæ ¼å¼å’ŒHUEæ ¼å¼çš„Excelæ–‡ä»¶")
        
        retention_files = st.file_uploader(
            "é€‰æ‹©ç•™å­˜æ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒå¤šæ–‡ä»¶ï¼‰",
            type=['xlsx', 'xls'],
            accept_multiple_files=True,
            key="retention_files"
        )
        
        target_month = st.text_input(
            "ç›®æ ‡åˆ†ææœˆä»½",
            value="",
            placeholder="YYYY-MM æ ¼å¼ï¼Œç•™ç©ºä½¿ç”¨é»˜è®¤æœˆä»½",
            help="æŒ‡å®šè¦åˆ†æçš„æœˆä»½ï¼Œæ ¼å¼å¦‚ï¼š2024-02"
        )
    
    with col2:
        st.markdown('<div class="status-card">', unsafe_allow_html=True)
        st.subheader("æ±‡æ€»è®¾ç½®")
        
        if retention_files:
            st.info(f"å·²é€‰æ‹© {len(retention_files)} ä¸ªæ–‡ä»¶")
            
        auto_month = datetime.datetime.now().strftime("%Y-%m")
        if not target_month:
            st.info(f"å°†ä½¿ç”¨é»˜è®¤æœˆä»½ï¼š{auto_month}")
        
        process_btn = st.button("å¼€å§‹æ•°æ®æ±‡æ€»", type="primary")
        st.markdown('</div>', unsafe_allow_html=True)
        
        if process_btn:
            if retention_files:
                with st.spinner("æ­£åœ¨æ±‡æ€»æ•°æ®ï¼Œè¯·ç¨å€™..."):
                    try:
                        target_month_val = target_month if target_month else None
                        merged_data, processed_count = integrate_excel_files_streamlit(
                            retention_files, target_month_val
                        )
                        
                        if merged_data is not None:
                            st.session_state.merged_data = merged_data
                            st.success(f"æ•°æ®æ±‡æ€»å®Œæˆ")
                            
                            # æ±‡æ€»ç»Ÿè®¡
                            col_a, col_b = st.columns(2)
                            with col_a:
                                st.metric("å¤„ç†æ–‡ä»¶æ•°", processed_count)
                            with col_b:
                                st.metric("æ±‡æ€»è®°å½•æ•°", len(merged_data))
                            
                            # æ•°æ®é¢„è§ˆ
                            st.subheader("æ•°æ®é¢„è§ˆ")
                            st.dataframe(merged_data.head(10))
                            
                            # æä¾›ä¸‹è½½
                            output = io.BytesIO()
                            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                                merged_data.to_excel(writer, sheet_name='åˆå¹¶æ•°æ®', index=False)
                            
                            st.download_button(
                                label="ä¸‹è½½æ±‡æ€»æ•°æ®",
                                data=output.getvalue(),
                                file_name=f"æ±‡æ€»æ•°æ®_{target_month or auto_month}.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
                        else:
                            st.error("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
                    except Exception as e:
                        st.error(f"æ•°æ®æ±‡æ€»å¤±è´¥ï¼š{e}")
            else:
                st.warning("è¯·å…ˆä¸Šä¼ ç•™å­˜æ•°æ®æ–‡ä»¶")

elif page == "ç•™å­˜ç‡è®¡ç®—":
    st.header("ç•™å­˜ç‡è®¡ç®—")
    st.markdown("åŸºäºæ±‡æ€»æ•°æ®è®¡ç®—å„æ¸ é“çš„æœˆå‡ç•™å­˜ç‡ã€‚")
    
    if st.session_state.merged_data is not None:
        merged_data = st.session_state.merged_data
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.info(f"å·²åŠ è½½æ±‡æ€»æ•°æ®ï¼š{len(merged_data)} æ¡è®°å½•")
            
            # æ˜¾ç¤ºæ¸ é“ç»Ÿè®¡
            channel_counts = merged_data['æ•°æ®æ¥æº'].value_counts()
            st.subheader("æ¸ é“æ•°æ®åˆ†å¸ƒ")
            st.dataframe(channel_counts.to_frame('è®°å½•æ•°').reset_index())
        
        with col2:
            st.markdown('<div class="status-card">', unsafe_allow_html=True)
            st.subheader("è®¡ç®—è®¾ç½®")
            
            if st.button("å¼€å§‹ç•™å­˜ç‡è®¡ç®—", type="primary"):
                with st.spinner("æ­£åœ¨è®¡ç®—ç•™å­˜ç‡..."):
                    try:
                        retention_results = {}
                        
                        # æŒ‰æ•°æ®æ¥æºï¼ˆæ¸ é“ï¼‰åˆ†ç»„
                        for channel_name in merged_data['æ•°æ®æ¥æº'].unique():
                            channel_data = merged_data[merged_data['æ•°æ®æ¥æº'] == channel_name].copy()
                            
                            # å‡†å¤‡ç•™å­˜ç‡æ•°æ®
                            retention_data = []
                            
                            for _, row in channel_data.iterrows():
                                if pd.notna(row.get('å›ä¼ æ–°å¢æ•°', 0)) and row.get('å›ä¼ æ–°å¢æ•°', 0) > 0:
                                    new_users = float(row['å›ä¼ æ–°å¢æ•°'])
                                    date_val = row.get('date', row.get('æ—¥æœŸ', ''))
                                    
                                    row_data = {'æ—¥æœŸ': date_val, 'æ–°å¢ç”¨æˆ·': new_users}
                                    
                                    # è®¡ç®—1-30å¤©ç•™å­˜ç‡
                                    for day in range(1, 31):
                                        retention_count = row.get(str(day), 0)
                                        if pd.notna(retention_count) and retention_count > 0:
                                            retention_rate = float(retention_count) / new_users * 100
                                            row_data[str(day)] = retention_rate
                                        else:
                                            row_data[str(day)] = 0
                                    
                                    retention_data.append(row_data)
                            
                            if retention_data:
                                df = pd.DataFrame(retention_data)
                                
                                # è®¡ç®—å‡å€¼
                                mean_row = {'æ—¥æœŸ': 'å‡å€¼', 'æ–°å¢ç”¨æˆ·': df['æ–°å¢ç”¨æˆ·'].mean()}
                                for day in range(1, 31):
                                    mean_row[str(day)] = df[str(day)].mean()
                                
                                df = pd.concat([df, pd.DataFrame([mean_row])], ignore_index=True)
                                
                                # æ·»åŠ æœˆå‡ç•™å­˜ç‡åˆ—
                                retention_summary = []
                                for day in range(1, 31):
                                    if mean_row[str(day)] > 0:
                                        retention_summary.append({
                                            'å¤©æ•°': day,
                                            'æœˆå‡ç•™å­˜ç‡': mean_row[str(day)]
                                        })
                                
                                retention_results[channel_name] = {
                                    'detail': df,
                                    'summary': pd.DataFrame(retention_summary)
                                }
                        
                        st.session_state.retention_data = retention_results
                        st.success("ç•™å­˜ç‡è®¡ç®—å®Œæˆ")
                        
                        # ç»“æœç»Ÿè®¡
                        st.metric("æˆåŠŸè®¡ç®—æ¸ é“æ•°", len(retention_results))
                        
                    except Exception as e:
                        st.error(f"ç•™å­˜ç‡è®¡ç®—å¤±è´¥ï¼š{e}")
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # æ˜¾ç¤ºè®¡ç®—ç»“æœ
        if st.session_state.retention_data:
            st.subheader("ç•™å­˜ç‡è®¡ç®—ç»“æœ")
            
            for channel_name, data in st.session_state.retention_data.items():
                with st.expander(f"{channel_name} - ç•™å­˜ç‡è¯¦æƒ…"):
                    
                    tab1, tab2 = st.tabs(["è¯¦ç»†æ•°æ®", "æ±‡æ€»ç»Ÿè®¡"])
                    
                    with tab1:
                        st.dataframe(data['detail'], use_container_width=True)
                    
                    with tab2:
                        st.dataframe(data['summary'], use_container_width=True)
    else:
        st.warning("è¯·å…ˆå®Œæˆæ•°æ®ä¸Šä¼ ä¸æ±‡æ€»")

elif page == "LTæ‹Ÿåˆåˆ†æ":
    st.header("LTæ‹Ÿåˆåˆ†æ")
    st.markdown("ä½¿ç”¨å¹‚å‡½æ•°å’ŒæŒ‡æ•°å‡½æ•°å¯¹ç•™å­˜æ•°æ®è¿›è¡Œæ‹Ÿåˆï¼Œè®¡ç®—ç”Ÿå‘½å‘¨æœŸä»·å€¼ã€‚")
    
    if st.session_state.retention_data is not None:
        col1, col2 = st.columns([3, 1])
        
        with col1:
            retention_data = st.session_state.retention_data
            st.info(f"å·²åŠ è½½ {len(retention_data)} ä¸ªæ¸ é“çš„ç•™å­˜ç‡æ•°æ®")
            
            # æ˜¾ç¤ºæ¸ é“åˆ—è¡¨
            st.subheader("å¾…åˆ†ææ¸ é“")
            channel_list = list(retention_data.keys())
            st.write(", ".join(channel_list))
        
        with col2:
            st.markdown('<div class="status-card">', unsafe_allow_html=True)
            st.subheader("åˆ†æè®¾ç½®")
            
            calculate_2y = st.checkbox("è®¡ç®—2å¹´LT", value=True)
            calculate_5y = st.checkbox("è®¡ç®—5å¹´LT", value=True)
            show_plots = st.checkbox("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨", value=True)
            
            if st.button("å¼€å§‹LTæ‹Ÿåˆåˆ†æ", type="primary"):
                with st.spinner("æ­£åœ¨è¿›è¡ŒLTæ‹Ÿåˆè®¡ç®—..."):
                    try:
                        key_days = [1, 7, 30, 60, 90, 100, 150, 200, 300]
                        
                        summary_data = []
                        key_days_summary = []
                        visualization_data_2y = {}
                        visualization_data_5y = {}
                        fit_params_data = {}
                        original_data = {}
                        
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        total_channels = len(retention_data)
                        
                        for idx, (channel_name, channel_data) in enumerate(retention_data.items()):
                            status_text.text(f"æ­£åœ¨å¤„ç†: {channel_name}")
                            progress_bar.progress((idx + 1) / total_channels)
                            
                            # å‡†å¤‡æ‹Ÿåˆæ•°æ®
                            summary_df = channel_data['summary']
                            
                            if len(summary_df) > 0:
                                # è½¬æ¢ä¸ºæ‹Ÿåˆå‡½æ•°éœ€è¦çš„æ ¼å¼
                                fit_data = pd.DataFrame({
                                    'ç•™å­˜å¤©æ•°': summary_df['å¤©æ•°'],
                                    'ç•™å­˜ç‡': summary_df['æœˆå‡ç•™å­˜ç‡'] / 100  # è½¬æ¢ä¸ºå°æ•°
                                })
                                
                                # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºå¯è§†åŒ–
                                original_data[channel_name] = {
                                    "days": fit_data["ç•™å­˜å¤©æ•°"].values,
                                    "rates": fit_data["ç•™å­˜ç‡"].values
                                }
                                
                                results = {}
                                
                                if calculate_2y:
                                    lt_2_years, days_2y, rates_2y, key_days_lt_2y, fit_params = calculate_lt(
                                        fit_data, channel_name, lt_years=2, return_curve_data=True, key_days=key_days
                                    )
                                    results['2å¹´LT'] = lt_2_years
                                    visualization_data_2y[channel_name] = {
                                        "days": days_2y,
                                        "rates": rates_2y,
                                        "lt": lt_2_years
                                    }
                                    fit_params_data[channel_name] = fit_params
                                
                                if calculate_5y:
                                    lt_5_years, days_5y, rates_5y, key_days_lt_5y, _ = calculate_lt(
                                        fit_data, channel_name, lt_years=5, return_curve_data=True, key_days=key_days
                                    )
                                    results['5å¹´LT'] = lt_5_years
                                    visualization_data_5y[channel_name] = {
                                        "days": days_5y,
                                        "rates": rates_5y,
                                        "lt": lt_5_years
                                    }
                                    
                                    # ä¿å­˜å…³é”®æ—¶é—´ç‚¹LTå€¼
                                    key_days_row = {"æ¸ é“åç§°": channel_name}
                                    for day in key_days:
                                        if day in key_days_lt_5y:
                                            key_days_row[f"{day}å¤©LT"] = key_days_lt_5y[day]
                                    key_days_summary.append(key_days_row)
                                
                                summary_row = {"æ¸ é“åç§°": channel_name}
                                summary_row.update(results)
                                summary_data.append(summary_row)
                        
                        # æ¸…ç†è¿›åº¦æ˜¾ç¤º
                        progress_bar.empty()
                        status_text.empty()
                        
                        # ä¿å­˜ç»“æœ
                        lt_results = {
                            'summary': pd.DataFrame(summary_data),
                            'key_days': pd.DataFrame(key_days_summary) if key_days_summary else pd.DataFrame(),
                            'fit_params': fit_params_data,
                            'visualization_2y': visualization_data_2y,
                            'visualization_5y': visualization_data_5y,
                            'original_data': original_data
                        }
                        
                        st.session_state.lt_results = lt_results
                        st.success("LTæ‹Ÿåˆåˆ†æå®Œæˆ")
                        
                    except Exception as e:
                        st.error(f"LTæ‹Ÿåˆåˆ†æå¤±è´¥ï¼š{e}")
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        if st.session_state.lt_results:
            st.subheader("LTåˆ†æç»“æœ")
            
            tab1, tab2, tab3, tab4 = st.tabs(["è®¡ç®—ç»“æœ", "å…³é”®æ—¶é—´ç‚¹", "æ‹Ÿåˆå‚æ•°", "å¯è§†åŒ–å›¾è¡¨"])
            
            with tab1:
                st.dataframe(st.session_state.lt_results['summary'], use_container_width=True)
            
            with tab2:
                if not st.session_state.lt_results['key_days'].empty:
                    st.dataframe(st.session_state.lt_results['key_days'], use_container_width=True)
                else:
                    st.info("æ— å…³é”®æ—¶é—´ç‚¹æ•°æ®")
            
            with tab3:
                if st.session_state.lt_results['fit_params']:
                    fit_params_list = []
                    for channel_name, params in st.session_state.lt_results['fit_params'].items():
                        row = {"æ¸ é“åç§°": channel_name}
                        if "power" in params:
                            row["å¹‚å‡½æ•°_a"] = f"{params['power']['a']:.6e}"
                            row["å¹‚å‡½æ•°_b"] = f"{params['power']['b']:.6f}"
                        if "exponential" in params:
                            row["æŒ‡æ•°å‡½æ•°_c"] = f"{params['exponential']['c']:.6e}"
                            row["æŒ‡æ•°å‡½æ•°_d"] = f"{params['exponential']['d']:.6f}"
                        fit_params_list.append(row)
                    
                    fit_params_df = pd.DataFrame(fit_params_list)
                    st.dataframe(fit_params_df, use_container_width=True)
                else:
                    st.info("æ— æ‹Ÿåˆå‚æ•°æ•°æ®")
            
            with tab4:
                if show_plots and st.session_state.lt_results['visualization_2y'] and st.session_state.lt_results['visualization_5y']:
                    try:
                        figures = create_visualization_plots(
                            st.session_state.lt_results['visualization_2y'],
                            st.session_state.lt_results['visualization_5y'],
                            st.session_state.lt_results['original_data']
                        )
                        
                        for title, fig in figures:
                            st.subheader(title)
                            st.pyplot(fig)
                    except Exception as e:
                        st.error(f"å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼š{e}")
                else:
                    st.info("è¯·åœ¨åˆ†æè®¾ç½®ä¸­å‹¾é€‰'ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨'")
            
            # æä¾›ç»“æœä¸‹è½½
            if st.button("ä¸‹è½½åˆ†ææŠ¥å‘Š"):
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    st.session_state.lt_results['summary'].to_excel(writer, sheet_name='LTæ€»ç»“', index=False)
                    if not st.session_state.lt_results['key_days'].empty:
                        st.session_state.lt_results['key_days'].to_excel(writer, sheet_name='å…³é”®æ—¶é—´ç‚¹LT', index=False)
                    if fit_params_list:
                        fit_params_df.to_excel(writer, sheet_name='æ‹Ÿåˆå‚æ•°', index=False)
                
                st.download_button(
                    label="ä¸‹è½½LTåˆ†ææŠ¥å‘Š",
                    data=output.getvalue(),
                    file_name="LTåˆ†ææŠ¥å‘Š.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
    else:
        st.warning("è¯·å…ˆå®Œæˆç•™å­˜ç‡è®¡ç®—")

elif page == "ARPUè®¡ç®—":
    st.header("ARPUè®¡ç®—")
    st.markdown("ä¸Šä¼ æ”¶å…¥æ•°æ®ï¼Œè®¡ç®—å„æ¸ é“çš„å¹³å‡æ¯ç”¨æˆ·æ”¶å…¥ã€‚")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ARPUæ•°æ®æ–‡ä»¶")
        st.markdown("**å¿…éœ€å­—æ®µï¼š** æœˆä»½ã€pidã€stat_dateã€instl_user_cntã€ad_all_rven_1d_m")
        
        arpu_file = st.file_uploader(
            "é€‰æ‹©ARPUæ•°æ®æ–‡ä»¶",
            type=['xlsx', 'xls'],
            key="arpu_file"
        )
        
        if arpu_file:
            try:
                arpu_df = pd.read_excel(arpu_file)
                st.success("ARPUæ•°æ®æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
                
                # æ£€æŸ¥å¿…è¦çš„åˆ—
                required_cols = ['æœˆä»½', 'pid', 'stat_date', 'instl_user_cnt', 'ad_all_rven_1d_m']
                missing_cols = [col for col in required_cols if col not in arpu_df.columns]
                
                if missing_cols:
                    st.error(f"ç¼ºå°‘å¿…è¦çš„åˆ—ï¼š{', '.join(missing_cols)}")
                else:
                    st.success("æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡")
                    
                    with st.expander("æ•°æ®é¢„è§ˆ"):
                        st.dataframe(arpu_df.head(10))
                    
                    # æœˆä»½ç­›é€‰
                    available_months = sorted(arpu_df['æœˆä»½'].unique())
                    st.subheader("åˆ†ææœˆä»½é€‰æ‹©")
                    
                    # é»˜è®¤é€‰æ‹©æœ€æ–°ä¸€å¹´çš„æ•°æ®
                    current_year = datetime.datetime.now().year
                    default_months = [m for m in available_months if str(current_year) in str(m)]
                    
                    selected_months = st.multiselect(
                        "é€‰æ‹©è¦åˆ†æçš„æœˆä»½",
                        available_months,
                        default=default_months[-12:] if len(default_months) >= 12 else default_months
                    )
                    
                    st.info(f"å·²é€‰æ‹© {len(selected_months)} ä¸ªæœˆä»½è¿›è¡Œåˆ†æ")
                    
            except Exception as e:
                st.error(f"è¯»å–ARPUæ–‡ä»¶å¤±è´¥ï¼š{e}")
    
    with col2:
        st.markdown('<div class="status-card">', unsafe_allow_html=True)
        st.subheader("è®¡ç®—è®¾ç½®")
        
        if 'arpu_df' in locals() and 'selected_months' in locals() and selected_months:
            if st.session_state.channel_mapping is not None:
                
                if st.button("å¼€å§‹ARPUè®¡ç®—", type="primary"):
                    with st.spinner("æ­£åœ¨è®¡ç®—ARPU..."):
                        try:
                            # ç­›é€‰æ•°æ®
                            filtered_arpu = arpu_df[arpu_df['æœˆä»½'].isin(selected_months)].copy()
                            
                            # æ•°æ®æ¸…æ´—
                            filtered_arpu['instl_user_cnt'] = pd.to_numeric(filtered_arpu['instl_user_cnt'], errors='coerce')
                            filtered_arpu['ad_all_rven_1d_m'] = pd.to_numeric(filtered_arpu['ad_all_rven_1d_m'], errors='coerce')
                            
                            # åˆ é™¤æ— æ•ˆæ•°æ®
                            filtered_arpu = filtered_arpu.dropna(subset=['instl_user_cnt', 'ad_all_rven_1d_m'])
                            
                            # ä½¿ç”¨æ”¹è¿›çš„æ¸ é“åŒ¹é…å‡½æ•°
                            pid_to_channel = parse_channel_mapping(st.session_state.channel_mapping)
                            
                            # æ·»åŠ æ¸ é“ååˆ—
                            filtered_arpu['æ¸ é“åç§°'] = filtered_arpu['pid'].astype(str).map(pid_to_channel)
                            
                            # è¿‡æ»¤æ‰æ— æ³•åŒ¹é…çš„æ¸ é“å·
                            matched_data = filtered_arpu[filtered_arpu['æ¸ é“åç§°'].notna()].copy()
                            
                            if len(matched_data) == 0:
                                st.error("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ¸ é“æ•°æ®ï¼Œè¯·æ£€æŸ¥æ¸ é“æ˜ å°„è¡¨å’Œæ•°æ®æ–‡ä»¶")
                            else:
                                # æŒ‰æ¸ é“æ±‡æ€»
                                arpu_summary = matched_data.groupby('æ¸ é“åç§°').agg({
                                    'instl_user_cnt': 'sum',
                                    'ad_all_rven_1d_m': 'sum'
                                }).reset_index()
                                
                                # è®¡ç®—ARPU
                                arpu_summary['ARPU'] = arpu_summary['ad_all_rven_1d_m'] / arpu_summary['instl_user_cnt']
                                
                                # å¤„ç†æ— æ•ˆå€¼
                                arpu_summary['ARPU'] = arpu_summary['ARPU'].fillna(0)
                                
                                st.session_state.arpu_data = arpu_summary
                                st.success("ARPUè®¡ç®—å®Œæˆ")
                                
                                # ç»Ÿè®¡ä¿¡æ¯
                                st.metric("åŒ¹é…æ¸ é“æ•°", len(arpu_summary))
                                st.metric("æ€»ç”¨æˆ·æ•°", f"{arpu_summary['instl_user_cnt'].sum():,.0f}")
                                st.metric("æ€»æ”¶å…¥", f"{arpu_summary['ad_all_rven_1d_m'].sum():,.2f}")
                                
                        except Exception as e:
                            st.error(f"ARPUè®¡ç®—å¤±è´¥ï¼š{e}")
            else:
                st.warning("è¯·å…ˆä¸Šä¼ æ¸ é“æ˜ å°„è¡¨")
        else:
            st.info("è¯·å…ˆä¸Šä¼ ARPUæ•°æ®æ–‡ä»¶å¹¶é€‰æ‹©åˆ†ææœˆä»½")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # æ˜¾ç¤ºARPUè®¡ç®—ç»“æœ
    if st.session_state.arpu_data is not None:
        st.subheader("ARPUè®¡ç®—ç»“æœ")
        
        arpu_summary = st.session_state.arpu_data
        
        # ç»“æœå±•ç¤º
        col_a, col_b = st.columns([2, 1])
        
        with col_a:
            st.dataframe(arpu_summary, use_container_width=True)
        
        with col_b:
            # ARPUæ’è¡Œæ¦œ
            st.subheader("ARPUæ’è¡Œæ¦œ")
            top_arpu = arpu_summary.nlargest(5, 'ARPU')[['æ¸ é“åç§°', 'ARPU']]
            for idx, row in top_arpu.iterrows():
                st.metric(row['æ¸ é“åç§°'], f"{row['ARPU']:.2f}")
        
        # æä¾›ä¸‹è½½
        if st.button("ä¸‹è½½ARPUè®¡ç®—ç»“æœ"):
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                arpu_summary.to_excel(writer, sheet_name='ARPUè®¡ç®—ç»“æœ', index=False)
            
            st.download_button(
                label="ä¸‹è½½ARPUæŠ¥å‘Š",
                data=output.getvalue(),
                file_name="ARPUè®¡ç®—ç»“æœ.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

elif page == "LTVç»“æœæŠ¥å‘Š":
    st.header("LTVç»“æœæŠ¥å‘Š")
    st.markdown("æ•´åˆLTåˆ†æå’ŒARPUè®¡ç®—ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆçš„ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼æŠ¥å‘Šã€‚")
    
    if st.session_state.lt_results is not None and st.session_state.arpu_data is not None:
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.success("æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
            
            # æ•°æ®æ¦‚è§ˆ
            lt_summary = st.session_state.lt_results['summary']
            arpu_summary = st.session_state.arpu_data
            
            col_info1, col_info2 = st.columns(2)
            with col_info1:
                st.metric("LTåˆ†ææ¸ é“æ•°", len(lt_summary))
            with col_info2:
                st.metric("ARPUè®¡ç®—æ¸ é“æ•°", len(arpu_summary))
        
        with col2:
            st.markdown('<div class="status-card">', unsafe_allow_html=True)
            st.subheader("ç”ŸæˆæŠ¥å‘Š")
            
            if st.button("ç”ŸæˆLTVç»“æœæŠ¥å‘Š", type="primary"):
                with st.spinner("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š..."):
                    try:
                        # åˆå¹¶LTå’ŒARPUæ•°æ®
                        ltv_results = lt_summary.merge(
                            arpu_summary[['æ¸ é“åç§°', 'ARPU']], 
                            on='æ¸ é“åç§°', 
                            how='left'
                        )
                        
                        # è®¡ç®—LTV
                        if '2å¹´LT' in ltv_results.columns:
                            ltv_results['2å¹´LTV'] = ltv_results['2å¹´LT'] * ltv_results['ARPU']
                        
                        if '5å¹´LT' in ltv_results.columns:
                            ltv_results['5å¹´LTV'] = ltv_results['5å¹´LT'] * ltv_results['ARPU']
                        
                        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
                        column_order = ['æ¸ é“åç§°']
                        if '5å¹´LT' in ltv_results.columns:
                            column_order.extend(['5å¹´LT', 'ARPU', '5å¹´LTV'])
                        if '2å¹´LT' in ltv_results.columns:
                            column_order.extend(['2å¹´LT', '2å¹´LTV'])
                        
                        # æ·»åŠ å…¶ä»–åˆ—
                        for col in ltv_results.columns:
                            if col not in column_order:
                                column_order.append(col)
                        
                        ltv_results = ltv_results[column_order]
                        
                        st.session_state.ltv_results = ltv_results
                        st.success("LTVç»“æœæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
                        
                    except Exception as e:
                        st.error(f"ç”ŸæˆLTVç»“æœå¤±è´¥ï¼š{e}")
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        if st.session_state.ltv_results is not None:
            st.subheader("æœ€ç»ˆLTVåˆ†æç»“æœ")
            
            ltv_results = st.session_state.ltv_results
            
            # ä¸»è¦ç»“æœè¡¨æ ¼
            st.dataframe(ltv_results, use_container_width=True)
            
            # åˆ†ææ´å¯Ÿ
            col_insight1, col_insight2 = st.columns(2)
            
            with col_insight1:
                if '5å¹´LTV' in ltv_results.columns:
                    st.subheader("5å¹´LTVè¡¨ç°")
                    top_5y_ltv = ltv_results.nlargest(5, '5å¹´LTV')[['æ¸ é“åç§°', '5å¹´LTV']]
                    
                    for idx, row in top_5y_ltv.iterrows():
                        st.metric(
                            row['æ¸ é“åç§°'], 
                            f"{row['5å¹´LTV']:.2f}",
                            help="5å¹´æœŸç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼"
                        )
            
            with col_insight2:
                if '2å¹´LTV' in ltv_results.columns:
                    st.subheader("2å¹´LTVè¡¨ç°")
                    top_2y_ltv = ltv_results.nlargest(5, '2å¹´LTV')[['æ¸ é“åç§°', '2å¹´LTV']]
                    
                    for idx, row in top_2y_ltv.iterrows():
                        st.metric(
                            row['æ¸ é“åç§°'], 
                            f"{row['2å¹´LTV']:.2f}",
                            help="2å¹´æœŸç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼"
                        )
            
            # ç»¼åˆåˆ†ææŠ¥å‘Šä¸‹è½½
            st.subheader("ä¸‹è½½å®Œæ•´æŠ¥å‘Š")
            
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                # ä¿å­˜æœ€ç»ˆç»“æœ
                ltv_results.to_excel(writer, sheet_name='LTVæœ€ç»ˆç»“æœ', index=False)
                
                # ä¿å­˜å…¶ä»–ç›¸å…³æ•°æ®
                if st.session_state.lt_results['key_days'] is not None and not st.session_state.lt_results['key_days'].empty:
                    st.session_state.lt_results['key_days'].to_excel(writer, sheet_name='å…³é”®æ—¶é—´ç‚¹LT', index=False)
                
                st.session_state.arpu_data.to_excel(writer, sheet_name='ARPUè®¡ç®—ç»“æœ', index=False)
                st.session_state.lt_results['summary'].to_excel(writer, sheet_name='LTåˆ†æç»“æœ', index=False)
            
            st.download_button(
                label="ä¸‹è½½å®Œæ•´LTVåˆ†ææŠ¥å‘Š",
                data=output.getvalue(),
                file_name=f"LTVåˆ†ææŠ¥å‘Š_{datetime.datetime.now().strftime('%Y%m%d')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
    
    elif st.session_state.lt_results is None:
        st.warning("è¯·å…ˆå®ŒæˆLTæ‹Ÿåˆåˆ†æ")
    elif st.session_state.arpu_data is None:
        st.warning("è¯·å…ˆå®ŒæˆARPUè®¡ç®—")
    
    # å¦‚æœå·²ç»æœ‰æœ€ç»ˆç»“æœï¼Œç›´æ¥æ˜¾ç¤º
    if st.session_state.ltv_results is not None:
        st.subheader("å½“å‰LTVåˆ†æç»“æœ")
        st.dataframe(st.session_state.ltv_results, use_container_width=True)

# ä¾§è¾¹æ è¿›åº¦çŠ¶æ€
st.sidebar.markdown("---")
st.sidebar.markdown("### åˆ†æè¿›åº¦")

progress_items = [
    ("æ¸ é“æ˜ å°„è¡¨", st.session_state.channel_mapping is not None),
    ("æ•°æ®æ±‡æ€»", st.session_state.merged_data is not None),
    ("ç•™å­˜ç‡è®¡ç®—", st.session_state.retention_data is not None),
    ("LTæ‹Ÿåˆåˆ†æ", st.session_state.lt_results is not None),
    ("ARPUè®¡ç®—", st.session_state.arpu_data is not None),
    ("LTVç»“æœæŠ¥å‘Š", st.session_state.ltv_results is not None)
]

for item_name, is_completed in progress_items:
    status_class = "âœ“" if is_completed else "â—‹"
    status_color = "#28a745" if is_completed else "#6c757d"
    
    st.sidebar.markdown(
        f'<div class="progress-item" style="color: {status_color};">'
        f'{status_class} {item_name}'
        f'</div>',
        unsafe_allow_html=True
    )

# åº•éƒ¨ä¿¡æ¯
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: #6c757d; font-size: 0.9rem;'>
        <p><strong>LTV Analytics Platform</strong></p>
        <p>ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†æç³»ç»Ÿ | æ”¯æŒæ•°æ®æ±‡æ€»ã€ç•™å­˜ç‡è®¡ç®—ã€LTæ‹Ÿåˆã€ARPUè®¡ç®—å’ŒLTVåˆ†æ</p>
    </div>
    """, 
    unsafe_allow_html=True
)
