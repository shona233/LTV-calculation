import streamlit as st
import os
import pandas as pd
import numpy as np
from pathlib import Path
import warnings
import datetime
import tempfile
import zipfile
import io
import matplotlib.pyplot as plt
import matplotlib as mpl
import re
from matplotlib.font_manager import FontProperties
import seaborn as sns
from scipy.optimize import curve_fit
import gc  # åƒåœ¾å›æ”¶
import difflib  # æ·»åŠ difflibå¯¼å…¥

# ==================== åŸºç¡€é…ç½® ====================
# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl.styles.stylesheet')
warnings.filterwarnings('ignore', category=UserWarning,
                        message="Could not infer format, so each element will be parsed individually")

# è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ - æ”¹ä¸ºè‹±æ–‡æ ‡ç­¾
def setup_chart_font():
    """è®¾ç½®å›¾è¡¨å­—ä½“ - ä½¿ç”¨è‹±æ–‡é¿å…ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜"""
    try:
        plt.rcParams['font.family'] = ['Arial', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        plt.rcParams['font.size'] = 10
        return True
    except Exception as e:
        plt.rcParams['font.family'] = ['DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        plt.rcParams['font.size'] = 10
        return False

# åˆå§‹åŒ–å­—ä½“è®¾ç½®
setup_chart_font()

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="LTV Analytics Platform",
    layout="wide",
    initial_sidebar_state="expanded",
    page_icon="ğŸ“Š"
)

# ==================== CSS æ ·å¼å®šä¹‰ ====================
st.markdown("""
<style>
    /* å…¨å±€æ ·å¼ */
    .main {
        background: #f8fafc;
        min-height: 100vh;
    }

    .block-container {
        padding: 1rem 1rem 3rem 1rem;
        max-width: 100%;
        background: transparent;
    }

    /* ä¸»æ ‡é¢˜åŒºåŸŸ */
    .main-header {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
        padding: 1.5rem;
        border-radius: 12px;
        box-shadow: 0 4px 20px rgba(30, 64, 175, 0.3);
        margin-bottom: 1.5rem;
        text-align: center;
        color: white;
    }

    .main-title {
        font-size: 2rem;
        font-weight: 700;
        color: white;
        margin-bottom: 0.5rem;
        text-shadow: 0 2px 4px rgba(0,0,0,0.3);
    }

    .main-subtitle {
        color: rgba(255,255,255,0.9);
        font-size: 1.2rem;
        font-weight: 400;
    }

    /* å¡ç‰‡æ ·å¼ */
    .glass-card {
        background: rgba(255, 255, 255, 0.95);
        border-radius: 12px;
        padding: 1.5rem;
        box-shadow: 0 8px 32px rgba(30, 64, 175, 0.1);
        border: 1px solid rgba(59, 130, 246, 0.2);
        margin-bottom: 1.5rem;
        backdrop-filter: blur(10px);
    }

    /* å¯¼èˆªæ­¥éª¤æ ·å¼ */
    .nav-container {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
        border-radius: 12px;
        padding: 1.5rem;
        margin-bottom: 1rem;
        color: white;
        box-shadow: 0 4px 20px rgba(30, 64, 175, 0.3);
    }

    /* æŒ‰é’®æ ·å¼ - ä¿®å¤æ‰€æœ‰çŠ¶æ€çš„é¢œè‰² */
    .stButton > button {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%) !important;
        color: white !important;
        border: none !important;
        border-radius: 8px !important;
        padding: 0.6rem 2rem !important;
        font-weight: 600 !important;
        transition: all 0.3s ease !important;
        box-shadow: 0 4px 15px rgba(30, 64, 175, 0.3) !important;
    }

    .stButton > button:hover {
        background: linear-gradient(135deg, #1d4ed8 0%, #2563eb 100%) !important;
        transform: translateY(-2px) !important;
        box-shadow: 0 6px 20px rgba(29, 78, 216, 0.4) !important;
        color: white !important;
    }
    
    .stButton > button:active {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%) !important;
        color: white !important;
        transform: translateY(0px) !important;
    }
    
    .stButton > button:focus {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%) !important;
        color: white !important;
        box-shadow: 0 4px 15px rgba(30, 64, 175, 0.3) !important;
        outline: none !important;
    }
    
    .stButton > button:focus:not(:active) {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%) !important;
        color: white !important;
    }
    
    /* é’ˆå¯¹ç‰¹å®šç±»å‹çš„æŒ‰é’® */
    div[data-testid="stButton"] > button {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%) !important;
        color: white !important;
    }
    
    div[data-testid="stButton"] > button:hover {
        background: linear-gradient(135deg, #1d4ed8 0%, #2563eb 100%) !important;
        color: white !important;
    }
    
    div[data-testid="stButton"] > button:active {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%) !important;
        color: white !important;
    }
    
    div[data-testid="stButton"] > button:focus {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%) !important;
        color: white !important;
        outline: none !important;
    }

    /* å­æ­¥éª¤æ ·å¼ */
    .sub-steps {
        font-size: 0.8rem;
        color: rgba(255,255,255,0.7);
        margin-top: 0.3rem;
        line-height: 1.2;
    }

    /* è­¦å‘Šæ–‡å­—é¢œè‰² */
    .warning-text {
        color: #f59e0b !important;
    }

    /* åŸç†è§£é‡Šæ¡†æ ·å¼ */
    .principle-box {
        background: rgba(59, 130, 246, 0.05);
        border: 1px solid rgba(59, 130, 246, 0.2);
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }

    .principle-title {
        color: #1e40af;
        font-weight: 600;
        font-size: 1rem;
        margin-bottom: 0.5rem;
    }

    .principle-content {
        color: #374151;
        font-size: 0.9rem;
        line-height: 1.5;
    }

    /* æç¤ºæ¡†æ ·å¼ */
    .step-tip {
        background: #eff6ff;
        border: 1px solid #bfdbfe;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
        border-left: 4px solid #3b82f6;
    }

    .step-tip-title {
        color: #1e40af;
        font-weight: 600;
        font-size: 0.95rem;
        margin-bottom: 0.5rem;
    }

    .step-tip-content {
        color: #374151;
        font-size: 0.85rem;
        line-height: 1.4;
    }

    /* ä¿®æ”¹æ•°æ®æ¥æºæ ‡ç­¾é¢œè‰²ä¸ºæµ…è“è‰² */
    .element-container .stScatter {
        color: #3b82f6 !important;
    }
    
    /* ä¿®æ”¹metricæ ‡ç­¾é¢œè‰² */
    [data-testid="metric-container"] > div > div > div > div {
        color: #3b82f6 !important;
    }
    
    /* ä¿®æ”¹æ•°æ®é¢„è§ˆä¸­çš„æ•°æ®æ¥æºä¸ºæµ…è“è‰² */
    .stDataFrame div[data-testid="stDataFrame"] {
        border: 1px solid rgba(59, 130, 246, 0.2);
    }
    
    /* éšè—é»˜è®¤çš„Streamlitå…ƒç´  */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# ==================== é»˜è®¤é…ç½®æ•°æ® ====================
DEFAULT_CHANNEL_MAPPING = {
    'æ€»ä½“': [],  # æ€»ä½“æ²¡æœ‰æ¸ é“å·ï¼Œæ˜¯æ‰€æœ‰å€¼çš„æ€»å’Œ
    'å®‰å“': [],  # å®‰å“æ²¡æœ‰æ¸ é“å·ï¼Œæ˜¯æ€»ä½“å‡å»iPhone
    'iPhone': ['9000'],  # iPhoneæ¸ é“å·9000
    'æ–°åª’ä½“': ['500345', '500346', '500447', '500449', '500450', '500531', '500542'],
    'åº”ç”¨å®': ['5007XS', '500349', '500350'],
    'é¼ä¹-ç››ä¸–6': ['500285'],
    'é¼ä¹-ç››ä¸–7': ['500286'],
    'é…·æ´¾': ['5108', '5528'],
    'æ–°ç¾-åŒ—äº¬1': ['500274'],
    'æ–°ç¾-åŒ—äº¬2': ['500275'],
    'A_æ·±åœ³è›‹ä¸2': ['500316'],
    'è£è€€': ['500297'],
    'åä¸º': ['5057'],
    'vivo': ['5237'],
    'å°ç±³': ['5599'],
    'OPPO': ['5115'],
    'ç½‘æ˜“': ['500471', '500480', '500481', '500482'],
    'åä¸ºéå•†åº—-å“ä¼—': ['500337', '500338', '500343', '500445', '500383', '500444', '500441'],
    'åä¸ºéå•†åº—-å¾®åˆ›': ['500543', '500544', '500545', '500546'],
    'é­…æ—': ['5072'],
    'OPPOéå•†åº—': ['500287', '500288'],
    'vivoéå•†åº—': ['5187'],
    'ç™¾åº¦sem--ç™¾åº¦æ—¶ä»£å®‰å“': ['500398', '500400', '500404'],
    'ç™¾åº¦sem--ç™¾åº¦æ—¶ä»£ios': ['500402', '500403', '500405'],
    'ç™¾é’è—¤-å®‰å“': ['500377', '500379', '500435', '500436', '500490', '500491', '500434', '500492'],
    'ç™¾é’è—¤-ios': ['500437'],
    'å°ç±³éå•†åº—': ['500170'],
    'åä¸ºéå•†åº—-æ˜Ÿç«': ['500532', '500533', '500534', '500537', '500538', '500539', '500540', '500541'],
    'å¾®åš-èœœæ©˜': ['500504', '500505'],
    'å¾®åš-å¤®å¹¿': ['500367', '500368', '500369'],
    'å¹¿ç‚¹é€š': ['500498', '500497', '500500', '500501', '500496', '500499'],
    'ç½‘æ˜“æ˜“æ•ˆ': ['500514', '500515', '500516']
}

# åˆ›å»ºåå‘æ˜ å°„ï¼šæ¸ é“å·->æ¸ é“åç§°
def create_reverse_mapping(channel_mapping):
    reverse_mapping = {}
    for channel_name, pids in channel_mapping.items():
        # è·³è¿‡æ²¡æœ‰æ¸ é“å·çš„ç‰¹æ®Šæ¸ é“ï¼ˆæ€»ä½“ã€å®‰å“ï¼‰
        if not pids:
            continue
        for pid in pids:
            reverse_mapping[str(pid)] = channel_name
    return reverse_mapping

# ==================== æ°¸ä¹…æ•°æ®å­˜å‚¨ç®¡ç† ====================
ADMIN_DATA_FILE = "admin_default_arpu_data.csv"

@st.cache_data
def load_admin_data_from_file():
    """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ç®¡ç†å‘˜ä¸Šä¼ çš„ARPUæ•°æ®"""
    try:
        if os.path.exists(ADMIN_DATA_FILE):
            df = pd.read_csv(ADMIN_DATA_FILE)
            return df
    except Exception as e:
        st.error(f"åŠ è½½ç®¡ç†å‘˜æ•°æ®æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
    return None

def save_admin_data_to_file(df):
    """ä¿å­˜ç®¡ç†å‘˜ä¸Šä¼ çš„ARPUæ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶"""
    try:
        df.to_csv(ADMIN_DATA_FILE, index=False, encoding='utf-8-sig')
        return True
    except Exception as e:
        st.error(f"ä¿å­˜ç®¡ç†å‘˜æ•°æ®æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
        return False

@st.cache_data
def get_builtin_arpu_data():
    """è·å–å†…ç½®çš„ARPUåŸºç¡€æ•°æ® - ä¼˜å…ˆä½¿ç”¨ç®¡ç†å‘˜ä¸Šä¼ çš„æ•°æ®"""
    # å°è¯•ä»æ–‡ä»¶è¯»å–ç®¡ç†å‘˜æ•°æ®
    admin_data = load_admin_data_from_file()
    if admin_data is not None:
        return admin_data.copy()
    
    # å¦‚æœæ²¡æœ‰ç®¡ç†å‘˜æ•°æ®ï¼Œè¿”å›ç¤ºä¾‹æ•°æ®
    return get_sample_arpu_data()

@st.cache_data
def get_sample_arpu_data():
    """ç”Ÿæˆç¤ºä¾‹ARPUæ•°æ®ï¼ˆå½“æ²¡æœ‰ç®¡ç†å‘˜ä¸Šä¼ æ•°æ®æ—¶ä½¿ç”¨ï¼‰"""
    # ç”Ÿæˆ2024å¹´1æœˆåˆ°2025å¹´4æœˆçš„æ‰€æœ‰æœˆä»½
    months = []
    for year in [2024, 2025]:
        start_month = 1 if year == 2024 else 1
        end_month = 12 if year == 2024 else 4
        for month in range(start_month, end_month + 1):
            months.append(f"{year}-{month:02d}")
    
    builtin_data = []
    
    # ä¸ºä¸»è¦æ¸ é“ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼ŒåŒ…å«iPhoneæ¸ é“å·9000
    sample_channels = ['9000', '5057', '5599', '5237', '5115', '500285', '500286']
    
    for pid in sample_channels:
        for month in months:
            # ç”Ÿæˆç¤ºä¾‹æ•°æ®
            base_users = {
                '9000': 12000, '5057': 8000, '5599': 6000, 
                '5237': 5500, '5115': 5000, '500285': 2000, '500286': 2200
            }.get(pid, 1000)
            
            base_revenue = {
                '9000': 600000, '5057': 320000, '5599': 240000,
                '5237': 220000, '5115': 200000, '500285': 80000, '500286': 88000
            }.get(pid, 40000)
            
            # æ·»åŠ æœˆåº¦æ³¢åŠ¨
            month_index = months.index(month)
            fluctuation = 1 + (month_index % 3 - 1) * 0.1
            
            users = int(base_users * fluctuation)
            revenue = int(base_revenue * fluctuation)
            
            builtin_data.append({
                'æœˆä»½': month,
                'pid': pid,
                'stat_date': f"{month}-15",
                'instl_user_cnt': users,
                'ad_all_rven_1d_m': revenue
            })
    
    return pd.DataFrame(builtin_data)

def load_admin_default_arpu_data():
    """ç®¡ç†å‘˜ä¸Šä¼ é»˜è®¤ARPUæ•°æ®çš„ç•Œé¢"""
    st.subheader("ç®¡ç†å‘˜åŠŸèƒ½ï¼šä¸Šä¼ é»˜è®¤ARPUæ•°æ®")
    
    st.markdown("""
    <div class="step-tip">
        <div class="step-tip-title">ç®¡ç†å‘˜æ•°æ®ä¸Šä¼ è¯´æ˜</div>
        <div class="step-tip-content">
        â€¢ ä¸Šä¼ åŒ…å«å®Œæ•´å†å²ARPUæ•°æ®çš„Excelæ–‡ä»¶ï¼ˆæ”¯æŒxlsxæ ¼å¼ï¼‰<br>
        â€¢ å¿…é¡»åŒ…å«åˆ—ï¼š<strong>æœˆä»½ã€pidã€stat_dateã€instl_user_cntã€ad_all_rven_1d_m</strong><br>
        â€¢ ä¸Šä¼ åå°†æ°¸ä¹…ä¿å­˜ï¼Œä¾›æ‰€æœ‰ç”¨æˆ·ä½¿ç”¨<br>
        â€¢ æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š<br>
        &nbsp;&nbsp;æœˆä»½: 2024-01, 2024-02...<br>
        &nbsp;&nbsp;pid: æ¸ é“å·<br>
        &nbsp;&nbsp;stat_date: ç»Ÿè®¡æ—¥æœŸ<br>
        &nbsp;&nbsp;instl_user_cnt: æ–°å¢ç”¨æˆ·æ•°<br>
        &nbsp;&nbsp;ad_all_rven_1d_m: æ”¶å…¥æ•°æ®
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # æ˜¾ç¤ºå½“å‰é»˜è®¤æ•°æ®çŠ¶æ€
    current_data = load_admin_data_from_file()
    if current_data is not None:
        st.success("å·²åŠ è½½ç®¡ç†å‘˜ä¸Šä¼ çš„é»˜è®¤æ•°æ®")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æ€»è®°å½•æ•°", f"{len(current_data):,}")
        with col2:
            st.metric("æ¸ é“æ•°é‡", current_data['pid'].nunique())
        with col3:
            st.metric("æœˆä»½æ•°é‡", current_data['æœˆä»½'].nunique())
        with col4:
            # è®¡ç®—æ•°æ®æ—¶é—´èŒƒå›´
            months = sorted(current_data['æœˆä»½'].unique())
            if months:
                time_range = f"{months[0]} è‡³ {months[-1]}"
                st.metric("æ—¶é—´èŒƒå›´", time_range)
        
        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
        with st.expander("æŸ¥çœ‹å½“å‰é»˜è®¤æ•°æ®é¢„è§ˆ", expanded=False):
            preview_data = optimize_dataframe_for_preview(current_data, max_rows=10)
            st.dataframe(preview_data, use_container_width=True)
            
        # æä¾›æ¸…é™¤é€‰é¡¹
        if st.button("æ¸…é™¤ç®¡ç†å‘˜æ•°æ®ï¼ˆæ¢å¤ç¤ºä¾‹æ•°æ®ï¼‰", help="æ¸…é™¤åå°†ä½¿ç”¨ç³»ç»Ÿç¤ºä¾‹æ•°æ®"):
            # åˆ é™¤æœ¬åœ°æ–‡ä»¶
            try:
                if os.path.exists(ADMIN_DATA_FILE):
                    os.remove(ADMIN_DATA_FILE)
                # æ¸…é™¤ç¼“å­˜
                st.cache_data.clear()
            except:
                pass
            st.success("å·²æ¸…é™¤ç®¡ç†å‘˜æ•°æ®ï¼Œæ¢å¤ä¸ºç³»ç»Ÿç¤ºä¾‹æ•°æ®")
            st.rerun()
    else:
        st.info("å½“å‰ä½¿ç”¨ç³»ç»Ÿç¤ºä¾‹æ•°æ®")
        sample_data = get_sample_arpu_data()
        st.metric("ç¤ºä¾‹æ•°æ®è®°å½•æ•°", f"{len(sample_data):,}")
    
    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    st.markdown("---")
    st.markdown("### ä¸Šä¼ æ–°çš„é»˜è®¤ARPUæ•°æ®")
    
    uploaded_default_file = st.file_uploader(
        "é€‰æ‹©åŒ…å«å®Œæ•´ARPUå†å²æ•°æ®çš„Excelæ–‡ä»¶",
        type=['xlsx', 'xls'],
        help="ä¸Šä¼ åå°†æ°¸ä¹…ä¿å­˜ä¸ºç³»ç»Ÿé»˜è®¤æ•°æ®ï¼Œä¾›æ‰€æœ‰ç”¨æˆ·ä½¿ç”¨",
        key="admin_default_arpu_upload"
    )
    
    if uploaded_default_file:
        try:
            with st.spinner("æ­£åœ¨è¯»å–å’ŒéªŒè¯Excelæ–‡ä»¶..."):
                # ä¼˜åŒ–çš„Excelè¯»å–
                uploaded_df = pd.read_excel(uploaded_default_file, engine='openpyxl')
                
                # éªŒè¯å¿…éœ€åˆ—
                required_cols = ['æœˆä»½', 'pid', 'stat_date', 'instl_user_cnt', 'ad_all_rven_1d_m']
                missing_cols = [col for col in required_cols if col not in uploaded_df.columns]
                
                if missing_cols:
                    st.error(f"æ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {', '.join(missing_cols)}")
                    st.info("æ–‡ä»¶åŒ…å«çš„åˆ—: " + ", ".join(uploaded_df.columns.tolist()))
                    return None
                
                # æ•°æ®æ¸…ç†å’Œæ ¼å¼åŒ–
                uploaded_df['pid'] = uploaded_df['pid'].astype(str).str.replace('.0', '', regex=False)
                uploaded_df['æœˆä»½'] = uploaded_df['æœˆä»½'].astype(str)
                
                # åŸºæœ¬æ•°æ®éªŒè¯
                if len(uploaded_df) == 0:
                    st.error("æ–‡ä»¶ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®")
                    return None
                
                # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
                st.success(f"æ–‡ä»¶è¯»å–æˆåŠŸï¼åŒ…å« {len(uploaded_df):,} æ¡è®°å½•")
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("æ€»è®°å½•æ•°", f"{len(uploaded_df):,}")
                with col2:
                    st.metric("æ¸ é“æ•°é‡", uploaded_df['pid'].nunique())
                with col3:
                    st.metric("æœˆä»½æ•°é‡", uploaded_df['æœˆä»½'].nunique())
                with col4:
                    # æ•°æ®æ—¶é—´èŒƒå›´
                    months = sorted(uploaded_df['æœˆä»½'].unique())
                    if months:
                        time_range = f"{months[0]} è‡³ {months[-1]}"
                        st.metric("æ—¶é—´èŒƒå›´", time_range)
                
                # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                st.markdown("**æ•°æ®é¢„è§ˆï¼š**")
                preview_uploaded = optimize_dataframe_for_preview(uploaded_df, max_rows=10)
                st.dataframe(preview_uploaded, use_container_width=True)
                
                # æ•°æ®è´¨é‡æ£€æŸ¥
                st.markdown("**æ•°æ®è´¨é‡æ£€æŸ¥ï¼š**")
                quality_checks = []
                
                # æ£€æŸ¥ç©ºå€¼
                null_counts = uploaded_df[required_cols].isnull().sum()
                if null_counts.sum() > 0:
                    quality_checks.append(f"å‘ç°ç©ºå€¼: {dict(null_counts[null_counts > 0])}")
                else:
                    quality_checks.append("æ— ç©ºå€¼")
                
                # æ£€æŸ¥æ•°å€¼åˆ—
                numeric_cols = ['instl_user_cnt', 'ad_all_rven_1d_m']
                for col in numeric_cols:
                    try:
                        uploaded_df[col] = pd.to_numeric(uploaded_df[col], errors='coerce')
                        invalid_count = uploaded_df[col].isnull().sum()
                        if invalid_count > 0:
                            quality_checks.append(f"{col} åˆ—æœ‰ {invalid_count} ä¸ªæ— æ•ˆæ•°å€¼")
                        else:
                            quality_checks.append(f"{col} åˆ—æ•°å€¼æ ¼å¼æ­£ç¡®")
                    except:
                        quality_checks.append(f"{col} åˆ—æ•°å€¼æ ¼å¼é”™è¯¯")
                
                # æ˜¾ç¤ºè´¨é‡æ£€æŸ¥ç»“æœ
                for check in quality_checks:
                    if "æ— ç©ºå€¼" in check or "æ•°å€¼æ ¼å¼æ­£ç¡®" in check:
                        st.success(check)
                    elif "å‘ç°ç©ºå€¼" in check or "æ— æ•ˆæ•°å€¼" in check:
                        st.warning(check)
                    else:
                        st.error(check)
                
                # ç¡®è®¤ä¸Šä¼ æŒ‰é’®
                st.markdown("---")
                col1, col2 = st.columns([1, 1])
                
                with col1:
                    if st.button("ç¡®è®¤è®¾ç½®ä¸ºé»˜è®¤æ•°æ®", type="primary", use_container_width=True):
                        # æ°¸ä¹…ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
                        if save_admin_data_to_file(uploaded_df):
                            # æ¸…é™¤ç¼“å­˜
                            st.cache_data.clear()
                            st.success("é»˜è®¤ARPUæ•°æ®å·²æ°¸ä¹…ä¿å­˜ï¼")
                            st.info("è¯¥æ•°æ®ç°åœ¨å°†ä½œä¸ºç³»ç»Ÿé»˜è®¤æ•°æ®ä½¿ç”¨ï¼Œæ‰€æœ‰ç”¨æˆ·éƒ½å¯ä»¥è®¿é—®ï¼Œä¸”æœåŠ¡é‡å¯åä»ç„¶æœ‰æ•ˆ")
                        else:
                            st.warning("æ–‡ä»¶ä¿å­˜å¤±è´¥")
                        st.rerun()
                
                with col2:
                    # æä¾›ä¸‹è½½æ¸…ç†åæ•°æ®çš„é€‰é¡¹
                    cleaned_csv = uploaded_df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ä¸‹è½½æ¸…ç†åçš„æ•°æ®",
                        data=cleaned_csv.encode('utf-8-sig'),
                        file_name=f"cleaned_arpu_data_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                
                return uploaded_df
                
        except Exception as e:
            st.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ï¼š{str(e)}")
            st.info("è¯·ç¡®ä¿æ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼ŒåŒ…å«æ‰€æœ‰å¿…éœ€åˆ—")
            return None
    
    return None

# ==================== æ—¥æœŸå¤„ç†å‡½æ•° ====================
def get_default_target_month():
    today = datetime.datetime.now()
    if today.month <= 2:
        target_year = today.year - 1
        target_month = today.month + 10
    else:
        target_year = today.year
        target_month = today.month - 2
    return f"{target_year}-{target_month:02d}"

# ==================== æ•°æ®ç±»å‹è½¬æ¢å‡½æ•° ====================
def safe_convert_to_numeric(value):
    """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•°å€¼ç±»å‹"""
    if pd.isna(value) or value == '' or value is None:
        return 0
    try:
        if isinstance(value, str):
            value = value.strip()
            if value == '' or value.lower() in ['nan', 'null', 'none']:
                return 0
        return pd.to_numeric(value, errors='coerce')
    except:
        return 0

# ==================== æ•°æ®é¢„è§ˆä¼˜åŒ–å‡½æ•° ====================
def optimize_dataframe_for_preview(df, max_rows=2):
    """ä¼˜åŒ–DataFrameé¢„è§ˆï¼šæœ‰å€¼çš„åˆ—æ”¾å‰é¢ï¼Œè·³è¿‡dateä¸º'æ—¥æœŸ'çš„è¡Œ"""
    preview_df = df.copy()
    
    # è·³è¿‡dateå€¼ä¸º"æ—¥æœŸ"çš„è¡Œ
    if 'date' in preview_df.columns:
        preview_df = preview_df[preview_df['date'] != 'æ—¥æœŸ']
    if 'æ—¥æœŸ' in preview_df.columns:
        preview_df = preview_df[preview_df['æ—¥æœŸ'] != 'æ—¥æœŸ']
    
    # å–å‰max_rowsè¡Œ
    preview_df = preview_df.head(max_rows)
    
    if preview_df.empty:
        return preview_df
    
    # è®¡ç®—æ¯åˆ—çš„éç©ºå€¼æ•°é‡
    non_null_counts = {}
    for col in preview_df.columns:
        non_null_count = preview_df[col].notna().sum()
        # æ’é™¤å…¨ä¸º0æˆ–ç©ºçš„æ•°å€¼åˆ—
        if preview_df[col].dtype in ['int64', 'float64']:
            non_zero_count = (preview_df[col] != 0).sum()
            non_null_counts[col] = non_null_count + non_zero_count
        else:
            non_null_counts[col] = non_null_count
    
    # æŒ‰éç©ºå€¼æ•°é‡æ’åºåˆ—
    sorted_columns = sorted(non_null_counts.keys(), key=lambda x: non_null_counts[x], reverse=True)
    
    # ç¡®ä¿'æ•°æ®æ¥æº'åˆ—åœ¨æœ€å‰é¢ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'æ•°æ®æ¥æº' in sorted_columns:
        sorted_columns.remove('æ•°æ®æ¥æº')
        sorted_columns.insert(0, 'æ•°æ®æ¥æº')
    
    return preview_df[sorted_columns]

# ==================== æ™ºèƒ½åŒ¹é…å‡½æ•° ====================
def calculate_similarity(str1, str2):
    """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ä¹‹é—´ï¼Œ1è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼‰"""
    return difflib.SequenceMatcher(None, str1.lower(), str2.lower()).ratio()

def find_best_match(file_name, channel_mapping, threshold=0.6):
    """æ‰¾åˆ°æ–‡ä»¶åçš„æœ€ä½³åŒ¹é…æ¸ é“åç§°"""
    best_match = None
    best_score = 0
    
    for channel_name in channel_mapping.keys():
        score = calculate_similarity(file_name, channel_name)
        if score > best_score and score >= threshold:
            best_score = score
            best_match = channel_name
    
    return best_match, best_score

def get_file_channel_suggestions(uploaded_files, channel_mapping):
    """è·å–æ–‡ä»¶çš„æ¸ é“åç§°å»ºè®®"""
    suggestions = {}
    
    for uploaded_file in uploaded_files:
        file_name = os.path.splitext(uploaded_file.name)[0].strip()
        
        # ç›´æ¥æ£€æŸ¥æ˜¯å¦å®Œå…¨åŒ¹é…
        if file_name in channel_mapping:
            continue
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ¸ é“å·
        reverse_mapping = create_reverse_mapping(channel_mapping)
        if file_name in reverse_mapping:
            continue
            
        # æ£€æŸ¥åŒ…å«å…³ç³»
        found_exact = False
        for channel_name in channel_mapping.keys():
            if channel_name in file_name or file_name in channel_name:
                found_exact = True
                break
                
        if found_exact:
            continue
            
        # ç›¸ä¼¼åº¦åŒ¹é…
        best_match, score = find_best_match(file_name, channel_mapping)
        if best_match:
            suggestions[file_name] = {
                'suggested_channel': best_match,
                'similarity_score': score,
                'file_object': uploaded_file
            }
    
    return suggestions

@st.cache_data
def parse_channel_mapping_from_excel(channel_file_content):
    """ä»ä¸Šä¼ çš„Excelæ–‡ä»¶è§£ææ¸ é“æ˜ å°„"""
    try:
        df = pd.read_excel(io.BytesIO(channel_file_content))
        channel_mapping = {}
        
        for _, row in df.iterrows():
            channel_name = str(row.iloc[0]).strip()
            if pd.isna(channel_name) or channel_name == '' or channel_name == 'nan':
                continue
                
            pids = []
            for col_idx in range(1, len(row)):
                pid = row.iloc[col_idx]
                if pd.isna(pid) or str(pid).strip() in ['', 'nan', 'ã€€', ' ']:
                    continue
                # ç¡®ä¿æ¸ é“å·ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œå»é™¤å°æ•°
                pid_str = str(int(float(pid))) if isinstance(pid, (int, float)) else str(pid).strip()
                if pid_str:
                    pids.append(pid_str)
            
            if pids:
                channel_mapping[channel_name] = pids
                    
        return channel_mapping
    except Exception as e:
        st.error(f"è§£ææ¸ é“æ˜ å°„æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
        return {}

# ==================== OCPXæ•°æ®åˆå¹¶å‡½æ•° ====================
def merge_ocpx_data(retention_data, new_users_data, target_month):
    """åˆå¹¶OCPXæ ¼å¼çš„ç•™å­˜æ•°æ®å’Œæ–°å¢æ•°æ®
    
    Args:
        retention_data: ocpxç›‘æµ‹ç•™å­˜æ•° sheetæ•°æ®
        new_users_data: ç›‘æµ‹æ¸ é“å›ä¼ é‡ sheetæ•°æ®  
        target_month: ç›®æ ‡æœˆä»½ (YYYY-MMæ ¼å¼)
    
    Returns:
        åˆå¹¶åçš„DataFrameæˆ–None
    """
    try:
        # å¤„ç†æ–°å¢æ•°æ® - ä»"ç›‘æµ‹æ¸ é“å›ä¼ é‡"sheet
        new_users_clean = new_users_data.copy()
        
        # æŸ¥æ‰¾"æ—¥æœŸ"å’Œ"å›ä¼ æ–°å¢æ•°"åˆ—
        date_col = None
        new_users_col = None
        
        # ç²¾ç¡®åŒ¹é…åˆ—å
        for col in new_users_clean.columns:
            col_str = str(col).strip()
            if col_str == 'æ—¥æœŸ':
                date_col = col
            elif col_str == 'å›ä¼ æ–°å¢æ•°':
                new_users_col = col
        
        if date_col is None:
            # æ¨¡ç³ŠåŒ¹é…æ—¥æœŸåˆ—
            for col in new_users_clean.columns:
                if 'æ—¥æœŸ' in str(col) or 'date' in str(col).lower():
                    date_col = col
                    break
                    
        if new_users_col is None:
            # æ¨¡ç³ŠåŒ¹é…æ–°å¢æ•°åˆ—
            for col in new_users_clean.columns:
                if 'å›ä¼ æ–°å¢æ•°' in str(col) or 'æ–°å¢' in str(col):
                    new_users_col = col
                    break
        
        if date_col is None or new_users_col is None:
            st.error('ç›‘æµ‹æ¸ é“å›ä¼ é‡è¡¨æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥æ˜¯å¦åŒ…å«"æ—¥æœŸ"å’Œ"å›ä¼ æ–°å¢æ•°"åˆ—')
            return None
        
        # æ¸…ç†æ–°å¢æ•°æ®
        new_users_dict = {}
        for _, row in new_users_clean.iterrows():
            try:
                date_val = row[date_col]
                
                # è·³è¿‡æ— æ•ˆè¡Œ
                if pd.isna(date_val) or str(date_val).strip().lower() in ['åˆè®¡', 'total', 'æ±‡æ€»', 'å°è®¡', '', 'nan']:
                    continue
                
                # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
                date_str = None
                if isinstance(date_val, str):
                    date_str = date_val.strip()
                    try:
                        parsed_date = pd.to_datetime(date_str)
                        date_str = parsed_date.strftime('%Y-%m-%d')
                    except:
                        pass
                else:
                    try:
                        date_str = pd.to_datetime(date_val).strftime('%Y-%m-%d')
                    except:
                        date_str = str(date_val)
                
                if date_str:
                    new_users_count = safe_convert_to_numeric(row[new_users_col])
                    if new_users_count > 0:
                        new_users_dict[date_str] = new_users_count
                        
            except Exception as e:
                continue
        
        # å¤„ç†ç•™å­˜æ•°æ® - ä»"ocpxç›‘æµ‹ç•™å­˜æ•°"sheet
        retention_clean = retention_data.copy()
        
        # æŸ¥æ‰¾"ç•™å­˜å¤©æ•°"åˆ—ä½œä¸ºæ—¥æœŸåˆ—
        retention_date_col = None
        for col in retention_clean.columns:
            col_str = str(col).strip()
            if col_str == 'ç•™å­˜å¤©æ•°':
                retention_date_col = col
                break
        
        if retention_date_col is None:
            # æ¨¡ç³ŠåŒ¹é…æ—¥æœŸåˆ—
            for col in retention_clean.columns:
                col_str = str(col).lower()
                if 'æ—¥æœŸ' in col_str or 'date' in col_str or 'å¤©æ•°' in col_str:
                    retention_date_col = col
                    break
        
        if retention_date_col is None:
            st.error('ocpxç›‘æµ‹ç•™å­˜æ•°è¡¨æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥æ˜¯å¦åŒ…å«"ç•™å­˜å¤©æ•°"åˆ—')
            return None
        
        # åˆå¹¶æ•°æ®
        merged_data = []
        
        for _, row in retention_clean.iterrows():
            try:
                date_val = row[retention_date_col]
                
                # è·³è¿‡æ— æ•ˆè¡Œ
                if pd.isna(date_val) or str(date_val).strip().lower() in ['åˆè®¡', 'total', 'æ±‡æ€»', 'å°è®¡', '', 'nan']:
                    continue
                
                # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
                date_str = None
                if isinstance(date_val, str):
                    date_str = date_val.strip()
                    try:
                        parsed_date = pd.to_datetime(date_str)
                        date_str = parsed_date.strftime('%Y-%m-%d')
                    except:
                        pass
                else:
                    try:
                        date_str = pd.to_datetime(date_val).strftime('%Y-%m-%d')
                    except:
                        date_str = str(date_val)
                
                if not date_str:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦åœ¨ç›®æ ‡æœˆä»½
                if target_month and not date_str.startswith(target_month):
                    continue
                
                # æ„å»ºåˆå¹¶åçš„è¡Œæ•°æ®
                merged_row = {
                    'date': date_str,
                    'stat_date': date_str,
                    'æ—¥æœŸ': date_str,
                    'å›ä¼ æ–°å¢æ•°': new_users_dict.get(date_str, 0)
                }
                
                # æ·»åŠ ç•™å­˜æ•°æ®ï¼ˆ1ã€2ã€3...åˆ—ï¼‰
                retention_found = False
                for col in retention_clean.columns:
                    col_str = str(col).strip()
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—åˆ—åï¼ˆ1ã€2ã€3...ï¼‰
                    if col_str.isdigit() and 1 <= int(col_str) <= 30:
                        retain_value = safe_convert_to_numeric(row[col])
                        merged_row[col_str] = retain_value
                        retention_found = True
                
                if retention_found:
                    merged_data.append(merged_row)
                    
            except Exception as e:
                continue
        
        if merged_data:
            result_df = pd.DataFrame(merged_data)
            st.success(f"OCPXæ•°æ®åˆå¹¶æˆåŠŸï¼Œå…±å¤„ç† {len(result_df)} æ¡è®°å½•")
            return result_df
        else:
            st.warning(f"æœªæ‰¾åˆ°ç›®æ ‡æœˆä»½ {target_month} çš„æœ‰æ•ˆæ•°æ®")
            return None
            
    except Exception as e:
        st.error(f"å¤„ç†OCPXæ•°æ®æ—¶å‡ºé”™ï¼š{str(e)}")
        return None

# ==================== æ–‡ä»¶æ•´åˆæ ¸å¿ƒå‡½æ•° - æ”¯æŒOCPXæ–°æ ¼å¼ - ä¼˜åŒ–ç‰ˆæœ¬ ====================
@st.cache_data
def integrate_excel_files_cached_with_mapping(file_names, file_contents, target_month, channel_mapping, confirmed_mappings):
    """ç¼“å­˜ç‰ˆæœ¬çš„æ–‡ä»¶æ•´åˆå‡½æ•° - æ”¯æŒOCPXæ–°æ ¼å¼å’Œæ™ºèƒ½æ˜ å°„ - ä¼˜åŒ–ç‰ˆæœ¬"""
    all_data = pd.DataFrame()
    processed_count = 0
    mapping_warnings = []

    for i, (file_name, file_content) in enumerate(zip(file_names, file_contents)):
        # ä»æ–‡ä»¶åä¸­æå–æ¸ é“åç§°ï¼ˆå»é™¤æ‰©å±•åå’Œå¤šä½™ç©ºæ ¼ï¼‰
        source_name = os.path.splitext(file_name)[0].strip()
        
        # æ¸ é“æ˜ å°„å¤„ç† - æ”¯æŒç”¨æˆ·ç¡®è®¤çš„æ™ºèƒ½åŒ¹é…
        mapped_source = source_name  # é»˜è®¤ä½¿ç”¨æ–‡ä»¶å
        
        # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·ç¡®è®¤çš„æ™ºèƒ½åŒ¹é…
        if source_name in confirmed_mappings:
            mapped_source = confirmed_mappings[source_name]
        # ç¬¬äºŒä¼˜å…ˆçº§ï¼šç›´æ¥æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åœ¨æ¸ é“æ˜ å°„çš„é”®ä¸­
        elif source_name in channel_mapping:
            mapped_source = source_name
        else:
            # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šæ£€æŸ¥æ–‡ä»¶åæ˜¯å¦æ˜¯æŸä¸ªæ¸ é“çš„æ¸ é“å·
            reverse_mapping = create_reverse_mapping(channel_mapping)
            if source_name in reverse_mapping:
                mapped_source = reverse_mapping[source_name]
            else:
                # ç¬¬å››ä¼˜å…ˆçº§ï¼šæ¨¡ç³ŠåŒ¹é… - æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«æ¸ é“åç§°
                found_match = False
                for channel_name in channel_mapping.keys():
                    if channel_name in source_name or source_name in channel_name:
                        mapped_source = channel_name
                        found_match = True
                        break
                
                if not found_match:
                    # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œä¿æŒåŸæ–‡ä»¶åå¹¶è®°å½•è­¦å‘Š
                    mapping_warnings.append(f"æ–‡ä»¶ '{source_name}' æœªåœ¨æ¸ é“æ˜ å°„è¡¨ä¸­æ‰¾åˆ°å¯¹åº”é¡¹")

        try:
            # ä»å†…å­˜ä¸­è¯»å–Excelæ–‡ä»¶ - ä¼˜åŒ–è¯»å–æ–¹å¼
            file_data = None
            
            with io.BytesIO(file_content) as buffer:
                xls = pd.ExcelFile(buffer, engine='openpyxl')
                sheet_names = xls.sheet_names

                # æŸ¥æ‰¾OCPXæ ¼å¼çš„å·¥ä½œè¡¨ - ç²¾ç¡®åŒ¹é…
                retention_sheet = None
                new_users_sheet = None
                
                # æŸ¥æ‰¾ç•™å­˜æ•°æ®è¡¨ - ç²¾ç¡®åŒ¹é…"ocpxç›‘æµ‹ç•™å­˜æ•°"
                for sheet in sheet_names:
                    if sheet.strip() == "ocpxç›‘æµ‹ç•™å­˜æ•°":
                        retention_sheet = sheet
                        break
                
                # æŸ¥æ‰¾æ–°å¢æ•°æ®è¡¨ - ç²¾ç¡®åŒ¹é…"ç›‘æµ‹æ¸ é“å›ä¼ é‡"
                for sheet in sheet_names:
                    if sheet.strip() == "ç›‘æµ‹æ¸ é“å›ä¼ é‡":
                        new_users_sheet = sheet
                        break
                
                # å¦‚æœæ‰¾åˆ°OCPXæ ¼å¼çš„è¡¨ï¼Œä½¿ç”¨æ–°çš„å¤„ç†æ–¹æ³•
                if retention_sheet and new_users_sheet:
                    try:
                        # å¤„ç†OCPXæ ¼å¼æ•°æ®
                        retention_data = pd.read_excel(buffer, sheet_name=retention_sheet, engine='openpyxl')
                        new_users_data = pd.read_excel(buffer, sheet_name=new_users_sheet, engine='openpyxl')
                        
                        # åˆå¹¶OCPXæ•°æ®
                        file_data = merge_ocpx_data(retention_data, new_users_data, target_month)
                        if file_data is not None and not file_data.empty:
                            file_data.insert(0, 'æ•°æ®æ¥æº', mapped_source)
                            all_data = pd.concat([all_data, file_data], ignore_index=True)
                            processed_count += 1
                        continue
                    except Exception as e:
                        st.warning(f"OCPXæ ¼å¼å¤„ç†å¤±è´¥ï¼Œå°†å°è¯•ä¼ ç»Ÿæ ¼å¼ï¼š{str(e)}")
                
                # å¦‚æœåªæ‰¾åˆ°ç•™å­˜æ•°æ®è¡¨ï¼ŒæŒ‰åŸæœ‰æ–¹å¼å¤„ç†
                if retention_sheet:
                    try:
                        file_data = pd.read_excel(buffer, sheet_name=retention_sheet, engine='openpyxl')
                    except:
                        file_data = pd.read_excel(buffer, sheet_name=0, engine='openpyxl')
                else:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
                    file_data = pd.read_excel(buffer, sheet_name=0, engine='openpyxl')
            
            if file_data is not None and not file_data.empty:
                # ä¼ ç»Ÿæ ¼å¼æ•°æ®å¤„ç†é€»è¾‘ - å¢å¼ºå…¼å®¹æ€§
                file_data_copy = file_data.copy()
                
                # æ£€æµ‹å¹¶å¤„ç†æ•°æ®æ ¼å¼
                has_stat_date = 'stat_date' in file_data_copy.columns
                retain_columns = [f'new_retain_{i}' for i in range(1, 31)]
                has_retain_columns = any(col in file_data_copy.columns for col in retain_columns)

                if has_stat_date and has_retain_columns:
                    # ä¼ ç»Ÿæ ¼å¼è¡¨å¤„ç†ï¼ˆstat_date + new + new_retain_Xæ ¼å¼ï¼‰
                    standardized_data = file_data_copy.copy()
                    
                    # å¤„ç†æ–°å¢æ•°æ®åˆ— - å¢å¼ºåŒ¹é…
                    new_col_found = False
                    for col in ['new', 'æ–°å¢', 'æ–°å¢ç”¨æˆ·', 'users']:
                        if col in standardized_data.columns:
                            standardized_data['å›ä¼ æ–°å¢æ•°'] = standardized_data[col].apply(safe_convert_to_numeric)
                            new_col_found = True
                            break
                    
                    if not new_col_found:
                        if len(standardized_data.columns) > 1:
                            standardized_data['å›ä¼ æ–°å¢æ•°'] = standardized_data.iloc[:, 1].apply(safe_convert_to_numeric)
                        else:
                            continue

                    # å¤„ç†ç•™å­˜æ•°æ®åˆ—ï¼šnew_retain_1 -> 1, new_retain_2 -> 2, ...
                    for i in range(1, 31):
                        retain_col = f'new_retain_{i}'
                        if retain_col in standardized_data.columns:
                            standardized_data[str(i)] = standardized_data[retain_col].apply(safe_convert_to_numeric)

                    # å¤„ç†æ—¥æœŸåˆ— - å¢å¼ºæ—¥æœŸå¤„ç†
                    date_col = 'stat_date'
                    try:
                        standardized_data[date_col] = pd.to_datetime(standardized_data[date_col], errors='coerce')
                        standardized_data[date_col] = standardized_data[date_col].dt.strftime('%Y-%m-%d')
                        standardized_data['æ—¥æœŸ'] = standardized_data[date_col]
                        standardized_data['month'] = standardized_data[date_col].str[:7]
                    except:
                        continue

                    # æŒ‰ç›®æ ‡æœˆä»½ç­›é€‰æ•°æ®
                    filtered_data = standardized_data[standardized_data['month'] == target_month].copy()

                    if not filtered_data.empty:
                        filtered_data.insert(0, 'æ•°æ®æ¥æº', mapped_source)
                        if 'stat_date' in filtered_data.columns:
                            filtered_data['date'] = filtered_data['stat_date']
                        all_data = pd.concat([all_data, filtered_data], ignore_index=True)
                        processed_count += 1
                else:
                    # å…¶ä»–æ ¼å¼è¡¨å¤„ç†ï¼ˆå…¼å®¹è€ç‰ˆæœ¬ï¼‰ - å¢å¼ºå¤„ç†
                    
                    # æŸ¥æ‰¾å…³é”®åˆ— - å¢å¼ºåŒ¹é…
                    report_users_col = None
                    users_keywords = ['å›ä¼ æ–°å¢æ•°', 'new', 'æ–°å¢', 'ç”¨æˆ·æ•°', 'æ–°å¢ç”¨æˆ·']
                    for col in file_data_copy.columns:
                        col_str = str(col).lower()
                        if any(keyword.lower() in col_str for keyword in users_keywords):
                            report_users_col = col
                            break

                    if report_users_col:
                        file_data_copy['å›ä¼ æ–°å¢æ•°'] = file_data_copy[report_users_col].apply(safe_convert_to_numeric)
                    else:
                        # ä½¿ç”¨ç¬¬äºŒåˆ—ä½œä¸ºæ–°å¢æ•°
                        if len(file_data_copy.columns) > 1:
                            file_data_copy['å›ä¼ æ–°å¢æ•°'] = file_data_copy.iloc[:, 1].apply(safe_convert_to_numeric)

                    # ç¡®ä¿æ•°å­—åˆ—åï¼ˆ1ã€2ã€3...ï¼‰è¢«æ­£ç¡®å¤„ç†
                    for i in range(1, 31):
                        col_name = str(i)
                        if col_name in file_data_copy.columns:
                            file_data_copy[col_name] = file_data_copy[col_name].apply(safe_convert_to_numeric)

                    # å¤„ç†æ—¥æœŸåˆ— - å¢å¼ºåŒ¹é…
                    date_col = None
                    date_keywords = ['æ—¥æœŸ', 'date', 'æ—¶é—´', 'ç»Ÿè®¡æ—¥æœŸ', 'stat_date']
                    for col in file_data_copy.columns:
                        col_str = str(col).lower()
                        if any(keyword.lower() in col_str for keyword in date_keywords):
                            date_col = col
                            break

                    if date_col:
                        try:
                            file_data_copy[date_col] = pd.to_datetime(file_data_copy[date_col], errors='coerce')
                            file_data_copy['month'] = file_data_copy[date_col].dt.strftime('%Y-%m')
                            filtered_data = file_data_copy[file_data_copy['month'] == target_month].copy()
                        except:
                            # å¦‚æœæ—¥æœŸå¤„ç†å¤±è´¥ï¼Œå°è¯•å­—ç¬¦ä¸²æˆªå–
                            file_data_copy['month'] = file_data_copy[date_col].apply(
                                lambda x: str(x)[:7] if isinstance(x, str) and len(str(x)) >= 7 else None
                            )
                            filtered_data = file_data_copy[file_data_copy['month'] == target_month].copy()
                    else:
                        # å¦‚æœæ²¡æ‰¾åˆ°æ—¥æœŸåˆ—ï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®
                        filtered_data = file_data_copy.copy()

                    if not filtered_data.empty:
                        filtered_data.insert(0, 'æ•°æ®æ¥æº', mapped_source)
                        if date_col and date_col != 'date':
                            filtered_data['date'] = filtered_data[date_col]
                        all_data = pd.concat([all_data, filtered_data], ignore_index=True)
                        processed_count += 1

        except Exception as e:
            st.error(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {str(e)}")
        finally:
            # æ¸…ç†å†…å­˜
            gc.collect()

    return all_data, processed_count, mapping_warnings

def integrate_excel_files_streamlit(uploaded_files, target_month=None, channel_mapping=None, confirmed_mappings=None):
    """ä¼˜åŒ–æ€§èƒ½çš„æ–‡ä»¶æ•´åˆå‡½æ•°ï¼Œæ”¯æŒç”¨æˆ·ç¡®è®¤çš„æ™ºèƒ½æ˜ å°„"""
    if target_month is None:
        target_month = get_default_target_month()

    # ä½¿ç”¨ä¼ å…¥çš„æ¸ é“æ˜ å°„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤æ˜ å°„
    if channel_mapping is None:
        channel_mapping = DEFAULT_CHANNEL_MAPPING
    
    # å¦‚æœæ²¡æœ‰ç¡®è®¤æ˜ å°„ï¼Œåˆå§‹åŒ–ä¸ºç©ºå­—å…¸
    if confirmed_mappings is None:
        confirmed_mappings = {}

    # å‡†å¤‡ç¼“å­˜æ•°æ® - ä¼˜åŒ–å†…å­˜ä½¿ç”¨
    file_names = [f.name for f in uploaded_files]
    file_contents = []
    
    # åˆ†æ‰¹è¯»å–æ–‡ä»¶å†…å®¹ï¼Œé¿å…å†…å­˜è¿‡è½½
    for f in uploaded_files:
        file_contents.append(f.read())
    
    return integrate_excel_files_cached_with_mapping(file_names, file_contents, target_month, channel_mapping, confirmed_mappings)

# ==================== ç•™å­˜ç‡è®¡ç®—å‡½æ•° - ç¡®ä¿ä½¿ç”¨æ•°å­—åˆ—å ====================
def calculate_retention_rates_new_method(df):
    """OCPXæ ¼å¼ç•™å­˜ç‡è®¡ç®—ï¼šå„å¤©ç•™å­˜åˆ—ï¼ˆ1ã€2ã€3...ï¼‰å¹³å‡å€¼Ã·å›ä¼ æ–°å¢æ•°å¹³å‡å€¼"""
    retention_results = []
    data_sources = df['æ•°æ®æ¥æº'].unique()

    for source in data_sources:
        source_data = df[df['æ•°æ®æ¥æº'] == source].copy()
        
        # è®¡ç®—å¹³å‡æ–°å¢ç”¨æˆ·æ•°ä½œä¸ºåŸºæ•°
        new_users_values = []
        for _, row in source_data.iterrows():
            new_users = safe_convert_to_numeric(row.get('å›ä¼ æ–°å¢æ•°', 0))
            if new_users > 0:
                new_users_values.append(new_users)
        
        if not new_users_values:
            continue
        
        avg_new_users = np.mean(new_users_values)
        
        # è®¡ç®—1-30å¤©çš„å¹³å‡ç•™å­˜æ•°ï¼Œä½¿ç”¨æ•°å­—åˆ—å"1"ã€"2"ã€"3"...
        retention_data = {'data_source': source, 'avg_new_users': avg_new_users}
        days = []
        rates = []
        
        for day in range(1, 31):
            day_col = str(day)  # ç¡®ä¿ä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼çš„æ•°å­—åˆ—å
            day_retain_values = []
            
            # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦å­˜åœ¨
            if day_col not in source_data.columns:
                continue
            
            for _, row in source_data.iterrows():
                if day_col in row.index and not pd.isna(row[day_col]):
                    retain_count = safe_convert_to_numeric(row[day_col])
                    if retain_count >= 0:  # å…è®¸0å€¼
                        day_retain_values.append(retain_count)
            
            if day_retain_values:
                avg_retain = np.mean(day_retain_values)
                # OCPXè®¡ç®—æ–¹æ³•ï¼šç•™å­˜ç‡ = å¹³å‡ç•™å­˜æ•° / å¹³å‡æ–°å¢æ•°
                retention_rate = avg_retain / avg_new_users if avg_new_users > 0 else 0
                
                # ç•™å­˜ç‡èŒƒå›´ä¸º 0 â‰¤ ç•™å­˜ç‡ â‰¤ 1.0
                if 0 <= retention_rate <= 1.0:
                    days.append(day)
                    rates.append(retention_rate)
        
        if days:
            retention_results.append({
                'data_source': source,
                'days': np.array(days),
                'rates': np.array(rates),
                'avg_new_users': avg_new_users
            })

    return retention_results

# ==================== æ•°å­¦å»ºæ¨¡å‡½æ•° ====================
def power_function(x, a, b):
    """å¹‚å‡½æ•°ï¼šy = a * x^b"""
    return a * np.power(x, b)

def exponential_function(x, c, d):
    """æŒ‡æ•°å‡½æ•°ï¼šy = c * exp(d * x)"""
    return c * np.exp(d * x)

def calculate_cumulative_lt(days_array, rates_array, target_days):
    """è®¡ç®—æŒ‡å®šå¤©æ•°çš„ç´¯ç§¯LTå€¼"""
    result = {}
    for day in target_days:
        idx = np.searchsorted(days_array, day, side='right')
        if idx > 0:
            cumulative_lt = 1.0 + np.sum(rates_array[1:idx])
            result[day] = cumulative_lt
    return result

def calculate_lt_advanced(retention_result, channel_name, lt_years=5, return_curve_data=False, key_days=None):
    """æŒ‰æ¸ é“è§„åˆ™è®¡ç®— LT"""
    # æ¸ é“è§„åˆ™
    CHANNEL_RULES = {
        "åä¸º": {"stage_2": [30, 120], "stage_3_base": [120, 220]},
        "å°ç±³": {"stage_2": [30, 190], "stage_3_base": [190, 290]},
        "oppo": {"stage_2": [30, 160], "stage_3_base": [160, 260]},
        "vivo": {"stage_2": [30, 150], "stage_3_base": [150, 250]},
        "iphone": {"stage_2": [30, 150], "stage_3_base": [150, 250], "stage_2_func": "log"},
        "å…¶ä»–": {"stage_2": [30, 100], "stage_3_base": [100, 200]}
    }

    # æ¸ é“è§„åˆ™åŒ¹é…
    if re.search(r'åä¸º', channel_name):
        rules = CHANNEL_RULES["åä¸º"]
    elif re.search(r'å°ç±³', channel_name):
        rules = CHANNEL_RULES["å°ç±³"]
    elif re.search(r'[oO][pP][pP][oO]', channel_name):
        rules = CHANNEL_RULES["oppo"]
    elif re.search(r'vivo', channel_name):
        rules = CHANNEL_RULES["vivo"]
    elif re.search(r'[iI][pP]hone', channel_name):
        rules = CHANNEL_RULES["iphone"]
    else:
        rules = CHANNEL_RULES["å…¶ä»–"]
        
    stage_2_start, stage_2_end = rules["stage_2"]
    stage_3_base_start, stage_3_base_end = rules["stage_3_base"]

    max_days = lt_years * 365

    days = retention_result["days"]
    rates = retention_result["rates"]

    fit_params = {}

    # ç¬¬ä¸€é˜¶æ®µ - å¹‚å‡½æ•°æ‹Ÿåˆ
    try:
        popt_power, _ = curve_fit(power_function, days, rates)
        a, b = popt_power
        fit_params["power"] = {"a": a, "b": b}

        days_full = np.arange(1, 31)
        rates_full = power_function(days_full, a, b)
        lt1_to_30 = np.sum(rates_full)
    except Exception as e:
        lt1_to_30 = 0.0
        a, b = 1.0, -1.0

    # ç¬¬äºŒé˜¶æ®µ
    try:
        days_stage_2 = np.arange(stage_2_start, stage_2_end + 1)
        rates_stage_2 = power_function(days_stage_2, a, b)
        lt_stage_2 = np.sum(rates_stage_2)
    except Exception as e:
        lt_stage_2 = 0.0
        rates_stage_2 = np.array([])

    # ç¬¬ä¸‰é˜¶æ®µ - æŒ‡æ•°æ‹Ÿåˆ
    try:
        days_stage_3_base = np.arange(stage_3_base_start, stage_3_base_end + 1)
        rates_stage_3_base = power_function(days_stage_3_base, a, b)

        initial_c = rates_stage_3_base[0]
        initial_d = -0.001
        popt_exp, _ = curve_fit(
            exponential_function,
            days_stage_3_base,
            rates_stage_3_base,
            p0=[initial_c, initial_d],
            bounds=([0, -np.inf], [np.inf, 0])
        )
        c, d = popt_exp
        fit_params["exponential"] = {"c": c, "d": d}
        days_stage_3 = np.arange(stage_3_base_start, max_days + 1)
        rates_stage_3 = exponential_function(days_stage_3, c, d)
        lt_stage_3 = np.sum(rates_stage_3)
    except Exception as e:
        days_stage_3 = np.arange(stage_3_base_start, max_days + 1)
        rates_stage_3 = power_function(days_stage_3, a, b) if 'a' in locals() else np.zeros(len(days_stage_3))
        lt_stage_3 = np.sum(rates_stage_3)

    total_lt = 1.0 + lt1_to_30 + lt_stage_2 + lt_stage_3

    try:
        predicted_rates = power_function(days, a, b)
        r2_score = 1 - np.sum((rates - predicted_rates) ** 2) / np.sum((rates - np.mean(rates)) ** 2)
    except:
        r2_score = 0.0

    if return_curve_data:
        all_days = np.concatenate([days_full, days_stage_2, days_stage_3])
        
        if 'rates_stage_2' not in locals():
            rates_stage_2 = power_function(days_stage_2, a, b)
        
        all_rates = np.concatenate([rates_full, rates_stage_2, rates_stage_3])

        sort_idx = np.argsort(all_days)
        all_days = all_days[sort_idx]
        all_rates = all_rates[sort_idx]

        max_idx = np.searchsorted(all_days, lt_years * 365, side='right')
        all_days = all_days[:max_idx]
        all_rates = all_rates[:max_idx]

        key_days_lt = {}
        if key_days:
            key_days_lt = calculate_cumulative_lt(all_days, all_rates, key_days)

        return {
            'lt_value': total_lt,
            'fit_params': fit_params,
            'power_r2': max(0, min(1, r2_score)),
            'success': True,
            'model_used': 'power+exponential',
            'curve_days': all_days,
            'curve_rates': all_rates,
            'key_days_lt': key_days_lt
        }

    return total_lt

# ==================== å•æ¸ é“å›¾è¡¨ç”Ÿæˆå‡½æ•° - é¿å…ä¸­æ–‡æ ‡é¢˜ ====================
def create_individual_channel_chart(channel_name, curve_data, original_data, max_days=100, lt_2y=None, lt_5y=None):
    """åˆ›å»ºå•ä¸ªæ¸ é“çš„100å¤©LTæ‹Ÿåˆå›¾è¡¨ - é¿å…ä¸­æ–‡æ ‡é¢˜æ˜¾ç¤ºé—®é¢˜ï¼Œæ·»åŠ 2å¹´5å¹´LTæ˜¾ç¤º"""
    
    # ä½¿ç”¨è‹±æ–‡å­—ä½“è®¾ç½®
    plt.rcParams['font.family'] = ['Arial', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, ax = plt.subplots(figsize=(8, 6))
    
    # ç»˜åˆ¶å®é™…æ•°æ®ç‚¹
    if channel_name in original_data:
        ax.scatter(
            original_data[channel_name]["days"],
            original_data[channel_name]["rates"],
            color='#ef4444',
            s=60,
            alpha=0.8,
            label='Actual Data',
            zorder=3
        )
    
    # é™åˆ¶æ‹Ÿåˆæ›²çº¿åˆ°100å¤©
    curve_days = curve_data["days"]
    curve_rates = curve_data["rates"]
    
    # ç­›é€‰100å¤©å†…çš„æ•°æ®
    mask = curve_days <= max_days
    curve_days_filtered = curve_days[mask]
    curve_rates_filtered = curve_rates[mask]
    
    # ç»˜åˆ¶æ‹Ÿåˆæ›²çº¿
    ax.plot(
        curve_days_filtered,
        curve_rates_filtered,
        color='#3b82f6',
        linewidth=2.5,
        label='Fitted Curve',
        zorder=2
    )
    
    # è®¾ç½®å›¾è¡¨æ ·å¼ - ä½¿ç”¨è‹±æ–‡æ ‡é¢˜é¿å…ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
    ax.set_xlim(0, max_days)
    ax.set_ylim(0, 0.6)
    
    ax.set_xlabel('Retention Days', fontsize=12)
    ax.set_ylabel('Retention Rate', fontsize=12)
    # ä½¿ç”¨ç®€æ´çš„è‹±æ–‡æ ‡é¢˜ï¼Œé¿å…ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
    ax.set_title(f'{max_days}-Day LT Fitting Analysis', fontsize=14, fontweight='bold')
    
    ax.grid(True, linestyle='--', alpha=0.4)
    ax.legend(fontsize=10)
    
    # è®¾ç½®Yè½´åˆ»åº¦ä¸ºç™¾åˆ†æ¯”
    y_ticks = [0, 0.15, 0.3, 0.45, 0.6]
    y_labels = ['0%', '15%', '30%', '45%', '60%']
    ax.set_yticks(y_ticks)
    ax.set_yticklabels(y_labels)
    
    plt.tight_layout()
    return fig

# ==================== ã€ä¿®å¤ã€‘åŠ è½½5æœˆåARPUæ•°æ®å‡½æ•° ====================
@st.cache_data
def load_user_arpu_data_after_april(uploaded_file_content, builtin_df):
    """ã€ä¿®å¤ç‰ˆã€‘åŠ è½½ç”¨æˆ·ä¸Šä¼ çš„5æœˆåŠä¹‹åçš„ARPUæ•°æ®ï¼Œå¹¶ä¸å†…ç½®æ•°æ®åˆå¹¶"""
    try:
        # è¯»å–ç”¨æˆ·ä¸Šä¼ çš„Excelæ–‡ä»¶
        user_df = pd.read_excel(io.BytesIO(uploaded_file_content), engine='openpyxl')
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_cols = ['pid', 'instl_user_cnt', 'ad_all_rven_1d_m']
        missing_cols = [col for col in required_cols if col not in user_df.columns]
        
        if missing_cols:
            return None, f"æ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {', '.join(missing_cols)}"
        
        # æ•°æ®æ¸…ç†å’Œæ ¼å¼åŒ–
        user_df['pid'] = user_df['pid'].astype(str).str.replace('.0', '', regex=False)
        
        # ç­›é€‰5æœˆåŠä¹‹åçš„æ•°æ® - å¢å¼ºæ—¥æœŸå¤„ç†
        user_df_filtered = None
        
        if 'æœˆä»½' in user_df.columns:
            # ä½¿ç”¨æœˆä»½åˆ—ç­›é€‰
            try:
                user_df['æœˆä»½'] = user_df['æœˆä»½'].astype(str)
                # æ ‡å‡†åŒ–æœˆä»½æ ¼å¼
                user_df['month_standard'] = user_df['æœˆä»½'].apply(lambda x: x[:7] if len(str(x)) >= 7 else str(x))
                
                # ç­›é€‰2025å¹´5æœˆåŠä¹‹åçš„æ•°æ®
                user_df_filtered = user_df[user_df['month_standard'] >= '2025-05'].copy()
                
                if len(user_df_filtered) == 0:
                    return None, "æœªæ‰¾åˆ°2025å¹´5æœˆåŠä¹‹åçš„æ•°æ®ï¼Œè¯·æ£€æŸ¥æœˆä»½æ ¼å¼"
                    
            except Exception as e:
                return None, f"å¤„ç†æœˆä»½åˆ—æ—¶å‡ºé”™ï¼š{str(e)}"
                
        elif 'stat_date' in user_df.columns:
            # ä½¿ç”¨æ—¥æœŸåˆ—ç­›é€‰
            try:
                user_df['stat_date'] = pd.to_datetime(user_df['stat_date'], errors='coerce')
                # ç­›é€‰2025å¹´5æœˆåŠä¹‹åçš„æ•°æ®
                april_2025_end = pd.to_datetime('2025-04-30')
                user_df_filtered = user_df[user_df['stat_date'] > april_2025_end].copy()
                
                if len(user_df_filtered) == 0:
                    return None, "æœªæ‰¾åˆ°2025å¹´5æœˆåŠä¹‹åçš„æ•°æ®ï¼Œè¯·æ£€æŸ¥æ—¥æœŸæ ¼å¼"
                    
                # æ·»åŠ æœˆä»½åˆ—ä»¥ä¾¿åç»­å¤„ç†
                user_df_filtered['æœˆä»½'] = user_df_filtered['stat_date'].dt.strftime('%Y-%m')
                
            except Exception as e:
                return None, f"å¤„ç†stat_dateåˆ—æ—¶å‡ºé”™ï¼š{str(e)}"
        else:
            return None, "æœªæ‰¾åˆ°æœˆä»½æˆ–stat_dateåˆ—è¿›è¡Œç­›é€‰"
        
        if user_df_filtered is None or len(user_df_filtered) == 0:
            return None, "ç­›é€‰åæ— æœ‰æ•ˆæ•°æ®"
        
        # æ•°æ®éªŒè¯
        numeric_cols = ['instl_user_cnt', 'ad_all_rven_1d_m']
        for col in numeric_cols:
            user_df_filtered[col] = pd.to_numeric(user_df_filtered[col], errors='coerce')
            # ç§»é™¤æ— æ•ˆæ•°å€¼è¡Œ
            user_df_filtered = user_df_filtered.dropna(subset=[col])
        
        if len(user_df_filtered) == 0:
            return None, "æ•°æ®æ¸…ç†åæ— æœ‰æ•ˆè®°å½•"
        
        # åˆå¹¶å†…ç½®æ•°æ®å’Œç”¨æˆ·æ•°æ®
        # ç¡®ä¿åˆ—æ ¼å¼ä¸€è‡´
        if 'month_standard' in user_df_filtered.columns:
            user_df_filtered = user_df_filtered.drop('month_standard', axis=1)
        
        # ç¡®ä¿ä¸¤ä¸ªDataFrameæœ‰ç›¸åŒçš„åˆ—
        builtin_cols = set(builtin_df.columns)
        user_cols = set(user_df_filtered.columns)
        
        # åªä¿ç•™å…¬å…±åˆ—
        common_cols = list(builtin_cols & user_cols)
        if not common_cols:
            return None, "å†…ç½®æ•°æ®ä¸ç”¨æˆ·æ•°æ®æ— å…¬å…±åˆ—"
        
        builtin_subset = builtin_df[common_cols].copy()
        user_subset = user_df_filtered[common_cols].copy()
        
        # åˆå¹¶æ•°æ®
        combined_df = pd.concat([builtin_subset, user_subset], ignore_index=True)
        
        # å»é‡å¤„ç†ï¼ˆå¦‚æœæœ‰é‡å¤çš„æœˆä»½+pidç»„åˆï¼Œä¿ç•™ç”¨æˆ·æ•°æ®ï¼‰
        if 'æœˆä»½' in combined_df.columns and 'pid' in combined_df.columns:
            combined_df = combined_df.drop_duplicates(subset=['æœˆä»½', 'pid'], keep='last')
        
        return combined_df, f"æ•°æ®åˆå¹¶æˆåŠŸï¼Œæ–°å¢ {len(user_df_filtered)} æ¡è®°å½•"
        
    except Exception as e:
        return None, f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}"

# ==================== ARPUè®¡ç®—å‡½æ•°ä¼˜åŒ–ç‰ˆæœ¬ ====================
def calculate_arpu_optimized(filtered_arpu_df, channel_mapping, batch_size=1000):
    """ä¼˜åŒ–çš„ARPUè®¡ç®—å‡½æ•°ï¼Œåˆ†æ‰¹å¤„ç†å¤§æ•°æ®ï¼Œæ”¯æŒç‰¹æ®Šæ¸ é“è®¡ç®—"""
    try:
        # ç¡®ä¿pidä¸ºå­—ç¬¦ä¸²æ ¼å¼
        filtered_arpu_df['pid'] = filtered_arpu_df['pid'].astype(str).str.replace('.0', '', regex=False)
        
        # æ•°æ®æ¸…ç† - ç¡®ä¿æ•°å€¼åˆ—ä¸ºæ•°å€¼ç±»å‹
        numeric_cols = ['instl_user_cnt', 'ad_all_rven_1d_m']
        for col in numeric_cols:
            filtered_arpu_df[col] = pd.to_numeric(filtered_arpu_df[col], errors='coerce')
        
        # ç§»é™¤æ— æ•ˆæ•°æ®
        filtered_arpu_df = filtered_arpu_df.dropna(subset=numeric_cols)
        filtered_arpu_df = filtered_arpu_df[
            (filtered_arpu_df['instl_user_cnt'] > 0) & 
            (filtered_arpu_df['ad_all_rven_1d_m'] >= 0)
        ]
        
        if len(filtered_arpu_df) == 0:
            return None, "æ•°æ®æ¸…ç†åæ— æœ‰æ•ˆè®°å½•"
        
        # åˆ›å»ºåå‘æ¸ é“æ˜ å°„
        reverse_mapping = create_reverse_mapping(channel_mapping)
        
        # åˆ†æ‰¹å¤„ç†æ•°æ®ä»¥é¿å…å†…å­˜é—®é¢˜
        arpu_results = []
        
        # å…ˆè®¡ç®—æ‰€æœ‰æœ‰æ¸ é“å·çš„æ¸ é“
        for pid, group in filtered_arpu_df.groupby('pid'):
            if pid in reverse_mapping:
                channel_name = reverse_mapping[pid]
                
                # åˆ†æ‰¹å¤„ç†å¤§ç»„
                for i in range(0, len(group), batch_size):
                    batch = group.iloc[i:i+batch_size]
                    
                    total_users = batch['instl_user_cnt'].sum()
                    total_revenue = batch['ad_all_rven_1d_m'].sum()
                    
                    if total_users > 0:
                        arpu_results.append({
                            'data_source': channel_name,
                            'total_users': total_users,
                            'total_revenue': total_revenue,
                            'record_count': len(batch)
                        })
        
        if arpu_results:
            # æŒ‰æ¸ é“åˆå¹¶ç›¸åŒæ¸ é“çš„æ•°æ®
            final_arpu = {}
            for result in arpu_results:
                channel = result['data_source']
                if channel in final_arpu:
                    final_arpu[channel]['total_users'] += result['total_users']
                    final_arpu[channel]['total_revenue'] += result['total_revenue']
                    final_arpu[channel]['record_count'] += result['record_count']
                else:
                    final_arpu[channel] = result.copy()
            
            # è®¡ç®—æ€»ä½“æ•°æ®ï¼ˆæ‰€æœ‰æ¸ é“çš„æ€»å’Œï¼‰
            total_users_sum = sum(data['total_users'] for data in final_arpu.values())
            total_revenue_sum = sum(data['total_revenue'] for data in final_arpu.values())
            total_record_count = sum(data['record_count'] for data in final_arpu.values())
            
            if total_users_sum > 0:
                final_arpu['æ€»ä½“'] = {
                    'total_users': total_users_sum,
                    'total_revenue': total_revenue_sum,
                    'record_count': total_record_count
                }
            
            # è®¡ç®—å®‰å“æ•°æ®ï¼ˆæ€»ä½“å‡å»iPhoneï¼‰
            iphone_data = final_arpu.get('iPhone', {'total_users': 0, 'total_revenue': 0, 'record_count': 0})
            android_users = total_users_sum - iphone_data['total_users']
            android_revenue = total_revenue_sum - iphone_data['total_revenue']
            android_records = total_record_count - iphone_data['record_count']
            
            if android_users > 0:
                final_arpu['å®‰å“'] = {
                    'total_users': android_users,
                    'total_revenue': android_revenue,
                    'record_count': android_records
                }
            
            # é‡æ–°è®¡ç®—ARPU
            arpu_summary = []
            for channel, data in final_arpu.items():
                arpu_value = data['total_revenue'] / data['total_users'] if data['total_users'] > 0 else 0
                arpu_summary.append({
                    'data_source': channel,
                    'arpu_value': arpu_value,
                    'record_count': data['record_count'],
                    'total_users': data['total_users'],
                    'total_revenue': data['total_revenue']
                })
            
            arpu_summary_df = pd.DataFrame(arpu_summary)
            return arpu_summary_df, "ARPUè®¡ç®—å®Œæˆ"
        else:
            return None, "æœªæ‰¾åˆ°åŒ¹é…çš„æ¸ é“æ•°æ®ï¼Œè¯·æ£€æŸ¥æ¸ é“æ˜ å°„é…ç½®"
            
    except Exception as e:
        return None, f"ARPUè®¡ç®—å¤±è´¥ï¼š{str(e)}"

# ==================== ä¸»åº”ç”¨ç¨‹åº ====================

# ä¸»æ ‡é¢˜
st.markdown("""
<div class="main-header">
    <div class="main-title">ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†æç³»ç»Ÿ</div>
    <div class="main-subtitle">åŸºäºåˆ†é˜¶æ®µæ•°å­¦å»ºæ¨¡çš„LTVé¢„æµ‹</div>
</div>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–session state
session_keys = [
    'channel_mapping', 'merged_data', 'cleaned_data', 'retention_data',
    'lt_results_2y', 'lt_results_5y', 'arpu_data', 'ltv_results', 'current_step',
    'excluded_data', 'excluded_dates_info', 'show_exclusion', 'show_manual_arpu',
    'visualization_data_5y', 'original_data', 'show_custom_mapping',
    'file_channel_confirmations'
]
for key in session_keys:
    if key not in st.session_state:
        st.session_state[key] = None

# è®¾ç½®é»˜è®¤å€¼
if st.session_state.channel_mapping is None:
    st.session_state.channel_mapping = DEFAULT_CHANNEL_MAPPING
if st.session_state.current_step is None:
    st.session_state.current_step = 0
if st.session_state.show_exclusion is None:
    st.session_state.show_exclusion = False
if st.session_state.show_manual_arpu is None:
    st.session_state.show_manual_arpu = False
if st.session_state.show_custom_mapping is None:
    st.session_state.show_custom_mapping = False

# ==================== åˆ†ææ­¥éª¤å®šä¹‰ ====================
ANALYSIS_STEPS = [
    {
        "name": "LTæ¨¡å‹æ„å»º",
        "sub_steps": ["æ•°æ®ä¸Šä¼ æ±‡æ€»", "å¼‚å¸¸å‰”é™¤", "ç•™å­˜ç‡è®¡ç®—", "LTæ‹Ÿåˆåˆ†æ"]
    },
    {"name": "ARPUè®¡ç®—"},
    {"name": "LTVç»“æœæŠ¥å‘Š"}
]

# ==================== ä¾§è¾¹æ å¯¼èˆª ====================
with st.sidebar:
    st.markdown('<div class="nav-container">', unsafe_allow_html=True)
    st.markdown('<h4 style="text-align: center; margin-bottom: 1rem; color: white;">åˆ†ææµç¨‹</h4>',
                unsafe_allow_html=True)

    for i, step in enumerate(ANALYSIS_STEPS):
        button_text = f"{i + 1}. {step['name']}"
        if st.button(button_text, key=f"nav_{i}", use_container_width=True,
                     type="primary" if i == st.session_state.current_step else "secondary"):
            st.session_state.current_step = i
            st.rerun()
        
        # åªåœ¨LTæ¨¡å‹æ„å»ºæ—¶æ˜¾ç¤ºå­æ­¥éª¤
        if "sub_steps" in step and i == st.session_state.current_step and step['name'] == "LTæ¨¡å‹æ„å»º":
            sub_steps_text = " â€¢ ".join(step["sub_steps"])
            st.markdown(f'<div class="sub-steps">{sub_steps_text}</div>', unsafe_allow_html=True)

    st.markdown('</div>', unsafe_allow_html=True)

# ==================== é¡µé¢è·¯ç”± ====================
current_page = ANALYSIS_STEPS[st.session_state.current_step]["name"]

# ==================== é¡µé¢å†…å®¹ ====================

if current_page == "LTæ¨¡å‹æ„å»º":
    # åŸç†è§£é‡Š
    st.markdown("""
    <div class="principle-box">
        <div class="principle-title">LTæ¨¡å‹æ„å»ºåŸç†</div>
        <div class="principle-content">
        LTæ¨¡å‹æ„å»ºåŒ…å«å››ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼š<br>
        <strong>1. æ•°æ®ä¸Šä¼ æ±‡æ€»ï¼š</strong>æ•´åˆå¤šä¸ªExcelæ–‡ä»¶ï¼Œæ”¯æŒæ–°æ ¼å¼è¡¨å’Œä¼ ç»Ÿæ ¼å¼è¡¨<br>
        <strong>2. å¼‚å¸¸å‰”é™¤ï¼š</strong>æŒ‰éœ€æ¸…ç†å¼‚å¸¸æ•°æ®ï¼Œæé«˜æ¨¡å‹å‡†ç¡®æ€§<br>
        <strong>3. ç•™å­˜ç‡è®¡ç®—ï¼š</strong>OCPXæ ¼å¼è¡¨æŒ‰æ¸ é“è®¡ç®—ï¼Œå„å¤©ç•™å­˜åˆ—ï¼ˆ1ã€2ã€3...ï¼‰å¹³å‡å€¼Ã·å›ä¼ æ–°å¢æ•°å¹³å‡å€¼<br>
        <strong>4. LTæ‹Ÿåˆåˆ†æï¼š</strong>é‡‡ç”¨ä¸‰é˜¶æ®µåˆ†å±‚å»ºæ¨¡ï¼Œé¢„æµ‹ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸé•¿åº¦
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # æ­¥éª¤1ï¼šæ•°æ®ä¸Šä¼ ä¸æ±‡æ€» - å§‹ç»ˆæ˜¾ç¤º
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.subheader("1. æ•°æ®ä¸Šä¼ ä¸æ±‡æ€»")
    
    # é»˜è®¤æ¸ é“æ˜ å°„ - é»˜è®¤å±•å¼€
    st.subheader("æ¸ é“æ˜ å°„é…ç½®")
    with st.expander("é»˜è®¤æ¸ é“æ˜ å°„ï¼ˆè¯·æŒ‰æ¸ é“åç§°å‘½åæ–‡ä»¶ï¼‰", expanded=True):
        st.markdown("""
        <div class="step-tip">
            <div class="step-tip-title">é‡è¦æç¤º</div>
            <div class="step-tip-content">
            <strong>æ–‡ä»¶å‘½åè§„åˆ™ï¼š</strong>è¯·å°†Excelæ–‡ä»¶æŒ‰ç…§ä¸‹è¡¨ä¸­çš„<strong>æ¸ é“åç§°</strong>è¿›è¡Œå‘½å<br>
            ä¾‹å¦‚ï¼š<code>é¼ä¹-ç››ä¸–7.xlsx</code> <code>åä¸º.xlsx</code><br>
            <strong>ç”¨é€”ï¼š</strong>æ­£ç¡®å‘½åå¯è‡ªåŠ¨åŒ¹é…ARPUæ•°æ®å’Œæ¸ é“åˆ†æ
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        default_mapping_rows = []
        for channel_name, pids in DEFAULT_CHANNEL_MAPPING.items():
            for pid in pids:
                default_mapping_rows.append({'æ¸ é“åç§°': channel_name, 'æ¸ é“å·': pid})
        default_mapping_df = pd.DataFrame(default_mapping_rows)
        st.dataframe(default_mapping_df, use_container_width=True)

    # æ·»åŠ æ˜¯å¦éœ€è¦ä¸Šä¼ æ¸ é“æ˜ å°„çš„é€‰æ‹©
    if not st.session_state.show_custom_mapping:
        if st.button("éœ€è¦ä¸Šä¼ è‡ªå®šä¹‰æ¸ é“æ˜ å°„", use_container_width=True):
            st.session_state.show_custom_mapping = True
            st.rerun()
        st.info("å¦‚æ— éœ€è‡ªå®šä¹‰æ¸ é“æ˜ å°„ï¼Œå¯ç›´æ¥è¿›è¡Œæ•°æ®ä¸Šä¼ ")
    else:
        st.markdown("### ä¸Šä¼ è‡ªå®šä¹‰æ¸ é“æ˜ å°„æ–‡ä»¶")
        channel_mapping_file = st.file_uploader(
            "é€‰æ‹©æ¸ é“æ˜ å°„Excelæ–‡ä»¶",
            type=['xlsx', 'xls'],
            help="æ ¼å¼ï¼šç¬¬ä¸€åˆ—ä¸ºæ¸ é“åç§°ï¼Œåç»­åˆ—ä¸ºå¯¹åº”çš„æ¸ é“å·",
            key="custom_channel_mapping"
        )
        
        if channel_mapping_file:
            try:
                file_content = channel_mapping_file.read()
                custom_mapping = parse_channel_mapping_from_excel(file_content)
                if custom_mapping and isinstance(custom_mapping, dict) and len(custom_mapping) > 0:
                    st.session_state.channel_mapping = custom_mapping
                    st.success(f"è‡ªå®šä¹‰æ¸ é“æ˜ å°„åŠ è½½æˆåŠŸï¼å…±åŒ…å« {len(custom_mapping)} ä¸ªæ¸ é“")
                    
                    # è‡ªåŠ¨å±•å¼€æ˜ å°„è¯¦æƒ…
                    with st.expander("æŸ¥çœ‹è‡ªå®šä¹‰æ¸ é“æ˜ å°„è¯¦æƒ…", expanded=True):
                        mapping_rows = []
                        for channel_name, pids in custom_mapping.items():
                            for pid in pids:
                                mapping_rows.append({'æ¸ é“åç§°': channel_name, 'æ¸ é“å·': pid})
                        mapping_df = pd.DataFrame(mapping_rows)
                        st.dataframe(mapping_df, use_container_width=True)
                else:
                    st.error("æ¸ é“æ˜ å°„æ–‡ä»¶è§£æå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤æ˜ å°„")
            except Exception as e:
                st.error(f"è¯»å–æ¸ é“æ˜ å°„æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")
        else:
            st.info("è¯·ä¸Šä¼ è‡ªå®šä¹‰æ¸ é“æ˜ å°„æ–‡ä»¶")

    # æ˜¾ç¤ºå½“å‰æ¸ é“æ˜ å°„æ‘˜è¦
    with st.expander("æŸ¥çœ‹å½“å‰æ¸ é“æ˜ å°„æ‘˜è¦", expanded=False):
        current_channels = list(st.session_state.channel_mapping.keys())
        st.markdown(f"**å½“å‰å…±æœ‰ {len(current_channels)} ä¸ªæ¸ é“ï¼š**")
        channels_text = "ã€".join(current_channels)
        st.text(channels_text)

    # æ•°æ®ä¸Šä¼ ç•Œé¢ - é»˜è®¤å±•å¼€
    st.markdown("### æ•°æ®æ–‡ä»¶ä¸Šä¼ ")
    
    # æ•°æ®æ–‡ä»¶ä¸Šä¼ 
    uploaded_files = st.file_uploader(
        "é€‰æ‹©Excelæ•°æ®æ–‡ä»¶",
        type=['xlsx', 'xls'],
        accept_multiple_files=True,
        help="æ”¯æŒOCPXåˆ†ç¦»å¼æ ¼å¼ï¼ˆç›‘æµ‹æ¸ é“å›ä¼ é‡+ocpxç›‘æµ‹ç•™å­˜æ•°ï¼‰å’Œä¼ ç»Ÿæ ¼å¼"
    )
    
    # æ·»åŠ æ ¼å¼è¯´æ˜
    st.markdown("""
    <div class="step-tip">
        <div class="step-tip-title">æ”¯æŒçš„Excelæ ¼å¼</div>
        <div class="step-tip-content">
        <strong>OCPXåˆ†ç¦»å¼æ ¼å¼ï¼ˆæ¨èï¼‰ï¼š</strong><br>
        â€¢ Sheet1: "ç›‘æµ‹æ¸ é“å›ä¼ é‡" - åŒ…å«"æ—¥æœŸ"å’Œ"å›ä¼ æ–°å¢æ•°"åˆ—<br>
        â€¢ Sheet2: "ocpxç›‘æµ‹ç•™å­˜æ•°" - åŒ…å«"ç•™å­˜å¤©æ•°"åˆ—å’Œç•™å­˜æ•°æ®ï¼ˆåˆ—åï¼š1ã€2ã€3...ï¼‰<br><br>
        <strong>ä¼ ç»Ÿæ ¼å¼ï¼š</strong><br>
        â€¢ åˆ—åæ ¼å¼ï¼š<strong>stat_date</strong>ï¼ˆæ—¥æœŸï¼‰ã€<strong>new</strong>ï¼ˆæ–°å¢æ•°ï¼‰ã€<strong>new_retain_1ã€new_retain_2...</strong>ï¼ˆç•™å­˜æ•°ï¼‰<br>
        â€¢ ç³»ç»Ÿè‡ªåŠ¨å°†new_retain_Xè½¬æ¢ä¸ºæ ‡å‡†åˆ—å1ã€2ã€3...<br><br>
        <strong>å…¼å®¹æ ¼å¼ï¼š</strong><br>
        â€¢ ç³»ç»Ÿä¼šå°è¯•è‡ªåŠ¨è¯†åˆ«å…¶ä»–æ ¼å¼çš„Excelæ–‡ä»¶
        </div>
    </div>
    """, unsafe_allow_html=True)

    default_month = get_default_target_month()
    target_month = st.text_input("ç›®æ ‡æœˆä»½ (YYYY-MM)", value=default_month)

    if uploaded_files:
        st.info(f"å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸ é“åç§°ç¡®è®¤
        suggestions = get_file_channel_suggestions(uploaded_files, st.session_state.channel_mapping)
        
        # åˆå§‹åŒ–å˜é‡
        confirmed_mappings = {}
        process_button_key = "process_data_direct"
        
        if suggestions:
            st.markdown("### ğŸ“‹ æ¸ é“åç§°ç¡®è®¤")
            st.markdown("""
            <div class="step-tip">
                <div class="step-tip-title">æ™ºèƒ½åŒ¹é…ç»“æœ</div>
                <div class="step-tip-content">
                ç³»ç»Ÿæ£€æµ‹åˆ°ä»¥ä¸‹æ–‡ä»¶åæœªå®Œå…¨åŒ¹é…æ¸ é“åç§°ï¼Œä¸ºæ‚¨æ¨èäº†æœ€ç›¸ä¼¼çš„æ¸ é“ã€‚è¯·ç¡®è®¤æ˜¯å¦æ­£ç¡®ï¼š
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            # åˆå§‹åŒ–ç¡®è®¤çŠ¶æ€
            if 'file_channel_confirmations' not in st.session_state:
                st.session_state.file_channel_confirmations = {}
            
            all_confirmed = True
            
            for file_name, suggestion in suggestions.items():
                col1, col2, col3 = st.columns([2, 2, 1])
                
                with col1:
                    st.markdown(f"**æ–‡ä»¶ï¼š** `{file_name}.xlsx`")
                
                with col2:
                    suggested_channel = suggestion['suggested_channel']
                    similarity_score = suggestion['similarity_score']
                    st.markdown(f"**å»ºè®®æ¸ é“ï¼š** {suggested_channel}")
                    st.caption(f"ç›¸ä¼¼åº¦: {similarity_score:.2%}")
                
                with col3:
                    confirm_key = f"confirm_{file_name}"
                    if confirm_key not in st.session_state.file_channel_confirmations:
                        if st.button("âœ… ç¡®è®¤", key=f"btn_confirm_{file_name}", use_container_width=True):
                            st.session_state.file_channel_confirmations[confirm_key] = suggested_channel
                            st.rerun()
                        all_confirmed = False
                    else:
                        confirmed_channel = st.session_state.file_channel_confirmations[confirm_key]
                        st.success(f"âœ… å·²ç¡®è®¤ä¸º: {confirmed_channel}")
                        confirmed_mappings[file_name] = confirmed_channel
                
                st.markdown("---")
            
            if not all_confirmed:
                st.info("è¯·ç¡®è®¤æ‰€æœ‰æ–‡ä»¶çš„æ¸ é“åç§°åå†ç»§ç»­å¤„ç†æ•°æ®")
                return
            
            # æ‰€æœ‰æ–‡ä»¶éƒ½å·²ç¡®è®¤ï¼Œå¯ä»¥å¤„ç†æ•°æ®
            st.success("âœ… æ‰€æœ‰æ–‡ä»¶æ¸ é“åç§°å·²ç¡®è®¤ï¼Œå¯ä»¥å¼€å§‹å¤„ç†æ•°æ®")
            process_button_key = "process_data_with_confirmations"

        if st.button("å¼€å§‹å¤„ç†æ•°æ®", type="primary", use_container_width=True, key=process_button_key):
            with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®æ–‡ä»¶..."):
                try:
                    merged_data, processed_count, mapping_warnings = integrate_excel_files_streamlit(
                        uploaded_files, target_month, st.session_state.channel_mapping, confirmed_mappings
                    )

                    if merged_data is not None and not merged_data.empty:
                        st.session_state.merged_data = merged_data
                        # æ¸…é™¤ç¡®è®¤çŠ¶æ€ï¼Œä¸ºä¸‹æ¬¡ä½¿ç”¨åšå‡†å¤‡
                        if 'file_channel_confirmations' in st.session_state:
                            del st.session_state.file_channel_confirmations
                        
                        st.success(f"æ•°æ®å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {processed_count} ä¸ªæ–‡ä»¶")

                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("æ€»è®°å½•æ•°", f"{len(merged_data):,}")
                        with col2:
                            st.metric("æ•°æ®æ¥æºæ•°", merged_data['æ•°æ®æ¥æº'].nunique())
                        with col3:
                            if 'å›ä¼ æ–°å¢æ•°' in merged_data.columns:
                                total_users = merged_data['å›ä¼ æ–°å¢æ•°'].sum()
                                st.metric("æ€»æ–°å¢ç”¨æˆ·", f"{total_users:,.0f}")

                        # æ˜¾ç¤ºæ–‡ä»¶åŒ¹é…æƒ…å†µ
                        st.subheader("æ–‡ä»¶åŒ¹é…æƒ…å†µ")
                        unique_sources = merged_data['æ•°æ®æ¥æº'].unique()
                        match_info = []
                        for source in unique_sources:
                            # æ£€æŸ¥æ˜¯å¦åœ¨æ˜ å°„ä¸­
                            is_in_mapping = source in st.session_state.channel_mapping or source in confirmed_mappings.values()
                            match_status = 'å·²åŒ¹é…'
                            if source in confirmed_mappings.values():
                                match_status = 'æ™ºèƒ½åŒ¹é…'
                            elif not is_in_mapping:
                                match_status = 'æœªåŒ¹é…'
                                
                            match_info.append({
                                'æ–‡ä»¶/æ¸ é“åç§°': source,
                                'åŒ¹é…çŠ¶æ€': match_status,
                                'è®°å½•æ•°': len(merged_data[merged_data['æ•°æ®æ¥æº'] == source])
                            })
                        
                        match_df = pd.DataFrame(match_info)
                        st.dataframe(match_df, use_container_width=True)

                        if mapping_warnings:
                            st.warning("ä»¥ä¸‹æ–‡ä»¶æœªåœ¨æ¸ é“æ˜ å°„ä¸­æ‰¾åˆ°å¯¹åº”å…³ç³»ï¼š")
                            for warning in mapping_warnings:
                                st.text(f"â€¢ {warning}")

                        # ä¼˜åŒ–çš„æ•°æ®é¢„è§ˆ - æ¯ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸¤è¡Œ
                        st.subheader("æ•°æ®é¢„è§ˆ")
                        
                        for source in unique_sources:
                            source_data = merged_data[merged_data['æ•°æ®æ¥æº'] == source]
                            optimized_preview = optimize_dataframe_for_preview(source_data, max_rows=2)
                            
                            # ä½¿ç”¨æµ…è“è‰²æ ·å¼æ˜¾ç¤ºæ•°æ®æ¥æº
                            st.markdown(f"""
                            <div style="background: rgba(59, 130, 246, 0.1); 
                                       border: 1px solid rgba(59, 130, 246, 0.3); 
                                       border-radius: 6px; 
                                       padding: 0.5rem; 
                                       margin: 0.5rem 0; 
                                       color: #1e40af; 
                                       font-weight: 600;">
                                {source}
                            </div>
                            """, unsafe_allow_html=True)
                            st.dataframe(optimized_preview, use_container_width=True)
                            
                    else:
                        st.error("æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
                except Exception as e:
                    st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")
    else:
        st.info("è¯·é€‰æ‹©Excelæ–‡ä»¶å¼€å§‹æ•°æ®å¤„ç†")

    st.markdown('</div>', unsafe_allow_html=True)

    # æ­¥éª¤2ï¼šå¼‚å¸¸æ•°æ®å‰”é™¤ - é»˜è®¤å±•å¼€
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.subheader("2. å¼‚å¸¸æ•°æ®å‰”é™¤")
    
    if st.session_state.merged_data is not None:
        merged_data = st.session_state.merged_data
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("### æŒ‰æ•°æ®æ¥æºå‰”é™¤")
            all_sources = sorted(merged_data['æ•°æ®æ¥æº'].unique().tolist())
            excluded_sources = st.multiselect(
                "é€‰æ‹©è¦å‰”é™¤çš„æ•°æ®æ¥æº", 
                options=all_sources, 
                key="exclude_sources_multiselect",
                help="é€‰æ‹©éœ€è¦ä»åˆ†æä¸­æ’é™¤çš„æ•°æ®æ¥æº"
            )

        with col2:
            st.markdown("### æŒ‰æ—¥æœŸå‰”é™¤")
            if 'date' in merged_data.columns:
                all_dates = sorted(merged_data['date'].unique().tolist())
                excluded_dates = st.multiselect(
                    "é€‰æ‹©è¦å‰”é™¤çš„æ—¥æœŸ", 
                    options=all_dates, 
                    key="exclude_dates_multiselect",
                    help="é€‰æ‹©éœ€è¦ä»åˆ†æä¸­æ’é™¤çš„æ—¥æœŸ"
                )
            else:
                st.info("æ•°æ®ä¸­æ— æ—¥æœŸå­—æ®µ")
                excluded_dates = []

        # è®¡ç®—å‰”é™¤ç»“æœ
        try:
            exclusion_mask = pd.Series([True] * len(merged_data), index=merged_data.index)

            if excluded_sources:
                source_mask = merged_data['æ•°æ®æ¥æº'].isin(excluded_sources)
                exclusion_mask &= source_mask

            if 'date' in merged_data.columns and excluded_dates:
                date_mask = merged_data['date'].isin(excluded_dates)
                exclusion_mask &= date_mask

            if not excluded_sources and not excluded_dates:
                exclusion_mask = pd.Series([False] * len(merged_data), index=merged_data.index)

            to_exclude = merged_data[exclusion_mask]
            to_keep = merged_data[~exclusion_mask]

        except Exception as e:
            st.error(f"è®¡ç®—å‰”é™¤æ¡ä»¶æ—¶å‡ºé”™: {str(e)}")
            to_exclude = pd.DataFrame()
            to_keep = merged_data.copy()

        col1, col2 = st.columns(2)
        with col1:
            st.markdown(f"### å°†è¢«å‰”é™¤çš„æ•°æ® ({len(to_exclude)} æ¡)")
            if len(to_exclude) > 0:
                preview_exclude = optimize_dataframe_for_preview(to_exclude, max_rows=5)
                st.dataframe(preview_exclude, use_container_width=True)
            else:
                st.info("æ— æ•°æ®å°†è¢«å‰”é™¤")

        with col2:
            st.markdown(f"### ä¿ç•™çš„æ•°æ® ({len(to_keep)} æ¡)")
            if len(to_keep) > 0:
                preview_keep = optimize_dataframe_for_preview(to_keep, max_rows=5)
                st.dataframe(preview_keep, use_container_width=True)

        if len(to_exclude) > 0:
            if st.button("ç¡®è®¤å‰”é™¤å¼‚å¸¸æ•°æ®", type="primary", use_container_width=True, key="confirm_exclude_btn"):
                try:
                    excluded_dates_info = []
                    for _, row in to_exclude.iterrows():
                        source = row.get('æ•°æ®æ¥æº', 'Unknown')
                        date = row.get('date', 'Unknown')
                        excluded_dates_info.append(f"{source}-{date}")
                    
                    st.session_state.excluded_data = excluded_dates_info
                    st.session_state.excluded_dates_info = excluded_dates
                    st.session_state.cleaned_data = to_keep.copy()
                    st.success(f"æˆåŠŸå‰”é™¤ {len(to_exclude)} æ¡å¼‚å¸¸æ•°æ®")
                except Exception as e:
                    st.error(f"å‰”é™¤æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        else:
            st.info("å½“å‰ç­›é€‰æ¡ä»¶ä¸‹æ— éœ€å‰”é™¤æ•°æ®")
            # å¦‚æœæ²¡æœ‰è¦å‰”é™¤çš„æ•°æ®ï¼Œè‡ªåŠ¨è®¾ç½®æ¸…ç†åæ•°æ®
            if not excluded_sources and not excluded_dates:
                st.session_state.cleaned_data = merged_data.copy()
    else:
        st.info("è¯·å…ˆå®Œæˆæ•°æ®ä¸Šä¼ ä¸æ±‡æ€»")

    st.markdown('</div>', unsafe_allow_html=True)

    # æ­¥éª¤3ï¼šç•™å­˜ç‡è®¡ç®— - é»˜è®¤å±•å¼€
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.subheader("3. ç•™å­˜ç‡è®¡ç®—")
    
    st.markdown("""
    <div class="step-tip">
        <div class="step-tip-title">OCPXæ ¼å¼ç•™å­˜ç‡è®¡ç®—æ–¹æ³•</div>
        <div class="step-tip-content">
        <strong>æ”¯æŒä¸¤ç§OCPXè¡¨æ ¼å¼ï¼š</strong><br>
        <strong>æ–¹å¼1ï¼šåˆ†ç¦»å¼OCPXè¡¨æ ¼</strong><br>
        â€¢ <strong>"ç›‘æµ‹æ¸ é“å›ä¼ é‡"</strong> sheetï¼šåŒ…å«"æ—¥æœŸ"å’Œ"å›ä¼ æ–°å¢æ•°"åˆ—<br>
        â€¢ <strong>"ocpxç›‘æµ‹ç•™å­˜æ•°"</strong> sheetï¼šåŒ…å«"ç•™å­˜å¤©æ•°"åˆ—å’Œå„å¤©ç•™å­˜æ•°ï¼ˆåˆ—åä¸º1ã€2ã€3...30ï¼‰<br>
        â€¢ ç³»ç»Ÿè‡ªåŠ¨åˆå¹¶ä¸¤ä¸ªè¡¨çš„æ•°æ®è¿›è¡Œè®¡ç®—<br><br>
        <strong>æ–¹å¼2ï¼šä¼ ç»Ÿæ ¼å¼è¡¨æ ¼</strong><br>
        â€¢ åˆ—åæ ¼å¼ï¼š<strong>stat_date</strong>ï¼ˆæ—¥æœŸï¼‰ã€<strong>new</strong>ï¼ˆæ–°å¢æ•°ï¼‰ã€<strong>new_retain_1ã€new_retain_2...</strong>ï¼ˆç•™å­˜æ•°ï¼‰<br>
        â€¢ ç³»ç»Ÿè‡ªåŠ¨å°†new_retain_Xè½¬æ¢ä¸ºæ ‡å‡†åˆ—å1ã€2ã€3...<br><br>
        <strong>è®¡ç®—å…¬å¼ï¼š</strong>ç•™å­˜ç‡ = å„å¤©ç•™å­˜æ•°å¹³å‡å€¼ Ã· å›ä¼ æ–°å¢æ•°å¹³å‡å€¼
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    if st.session_state.merged_data is not None:
        # ç¡®å®šä½¿ç”¨çš„æ•°æ®
        if st.session_state.cleaned_data is not None:
            working_data = st.session_state.cleaned_data
            st.info("ä½¿ç”¨æ¸…ç†åçš„æ•°æ®è¿›è¡Œè®¡ç®—")
        else:
            working_data = st.session_state.merged_data
            st.info("ä½¿ç”¨åŸå§‹æ•°æ®è¿›è¡Œè®¡ç®—")

        data_sources = working_data['æ•°æ®æ¥æº'].unique()
        selected_sources = st.multiselect("é€‰æ‹©è¦åˆ†æçš„æ•°æ®æ¥æº", options=data_sources, default=data_sources, key="retention_sources")

        if st.button("è®¡ç®—ç•™å­˜ç‡", type="primary", use_container_width=True, key="calc_retention"):
            if selected_sources:
                with st.spinner("æ­£åœ¨è®¡ç®—ç•™å­˜ç‡..."):
                    filtered_data = working_data[working_data['æ•°æ®æ¥æº'].isin(selected_sources)]
                    retention_results = calculate_retention_rates_new_method(filtered_data)
                    st.session_state.retention_data = retention_results

                    st.success("ç•™å­˜ç‡è®¡ç®—å®Œæˆï¼")

                    # åˆ›å»ºç•™å­˜ç‡è¡¨æ ¼æ˜¾ç¤º
                    if retention_results:
                        st.subheader("ç•™å­˜ç‡è¯¦ç»†æ•°æ®")
                        
                        # åˆ›å»ºè¡¨æ ¼æ•°æ®
                        table_data = []
                        days_range = range(1, 31)
                        
                        for day in days_range:
                            row = {'å¤©æ•°': day}
                            for result in retention_results:
                                channel_name = result['data_source']
                                days = result['days']
                                rates = result['rates']
                                
                                # æŸ¥æ‰¾å¯¹åº”å¤©æ•°çš„ç•™å­˜ç‡
                                day_index = np.where(days == day)[0]
                                if len(day_index) > 0:
                                    rate = rates[day_index[0]]
                                    row[channel_name] = f"{rate:.4f}"
                                else:
                                    row[channel_name] = "-"
                            table_data.append(row)
                        
                        # åˆ›å»ºDataFrameå¹¶æ˜¾ç¤º
                        retention_table_df = pd.DataFrame(table_data)
                        
                        # ä½¿ç”¨expanderå±•å¼€è¡¨æ ¼ï¼Œé™åˆ¶é«˜åº¦å¹¶æ˜¾ç¤ºæ»šåŠ¨æ¡
                        with st.expander("ç•™å­˜ç‡æ•°æ®è¡¨ï¼ˆ1-30å¤©ï¼‰", expanded=True):
                            st.dataframe(
                                retention_table_df, 
                                use_container_width=True,
                                height=400  # è®¾ç½®å›ºå®šé«˜åº¦ï¼Œçº¦10è¡Œçš„é«˜åº¦
                            )
            else:
                st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ•°æ®æ¥æº")
    else:
        st.info("è¯·å…ˆå®Œæˆæ•°æ®ä¸Šä¼ ä¸æ±‡æ€»")

    st.markdown('</div>', unsafe_allow_html=True)

    # æ­¥éª¤4ï¼šLTæ‹Ÿåˆåˆ†æ - é»˜è®¤å±•å¼€
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.subheader("4. LTæ‹Ÿåˆåˆ†æ")
    
    # æ¸ é“è§„åˆ™è¯´æ˜
    st.markdown("""
    <div class="step-tip">
        <div class="step-tip-title">ä¸‰é˜¶æ®µæ‹Ÿåˆè§„åˆ™</div>
        <div class="step-tip-content">
        <strong>ç¬¬ä¸€é˜¶æ®µï¼š</strong>1-30å¤©ï¼Œå¹‚å‡½æ•°æ‹Ÿåˆå®é™…æ•°æ®<br>
        <strong>ç¬¬äºŒé˜¶æ®µï¼š</strong>31-Xå¤©ï¼Œå»¶ç»­å¹‚å‡½æ•°æ¨¡å‹<br>
        <strong>ç¬¬ä¸‰é˜¶æ®µï¼š</strong>Yå¤©åï¼ŒæŒ‡æ•°å‡½æ•°å»ºæ¨¡é•¿æœŸè¡°å‡<br>
        ä¸åŒæ¸ é“é‡‡ç”¨ä¸åŒçš„é˜¶æ®µåˆ’åˆ†ç‚¹
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    if st.session_state.retention_data is not None:
        retention_data = st.session_state.retention_data

        if st.button("å¼€å§‹LTæ‹Ÿåˆåˆ†æ", type="primary", use_container_width=True, key="start_lt_fitting"):
            with st.spinner("æ­£åœ¨è¿›è¡Œæ‹Ÿåˆè®¡ç®—..."):
                lt_results_2y = []
                lt_results_5y = []
                visualization_data_2y = {}
                visualization_data_5y = {}
                original_data = {}
                
                key_days = [1, 7, 30, 60, 90, 100, 150, 200, 300]

                for retention_result in retention_data:
                    channel_name = retention_result['data_source']
                    
                    # è®¡ç®—2å¹´LT
                    lt_result_2y = calculate_lt_advanced(retention_result, channel_name, 2, 
                                                       return_curve_data=True, key_days=key_days)
                    
                    # è®¡ç®—5å¹´LT
                    lt_result_5y = calculate_lt_advanced(retention_result, channel_name, 5, 
                                                       return_curve_data=True, key_days=key_days)

                    lt_results_2y.append({
                        'data_source': channel_name,
                        'lt_value': lt_result_2y['lt_value'],
                        'fit_success': lt_result_2y['success'],
                        'fit_params': lt_result_2y['fit_params'],
                        'power_r2': lt_result_2y['power_r2'],
                        'model_used': lt_result_2y['model_used']
                    })
                    
                    lt_results_5y.append({
                        'data_source': channel_name,
                        'lt_value': lt_result_5y['lt_value'],
                        'fit_success': lt_result_5y['success'],
                        'fit_params': lt_result_5y['fit_params'],
                        'power_r2': lt_result_5y['power_r2'],
                        'model_used': lt_result_5y['model_used']
                    })

                    # ä¿å­˜å¯è§†åŒ–æ•°æ®
                    visualization_data_2y[channel_name] = {
                        "days": lt_result_2y['curve_days'],
                        "rates": lt_result_2y['curve_rates'],
                        "lt": lt_result_2y['lt_value']
                    }
                    
                    visualization_data_5y[channel_name] = {
                        "days": lt_result_5y['curve_days'],
                        "rates": lt_result_5y['curve_rates'],
                        "lt": lt_result_5y['lt_value']
                    }

                    # ä¿å­˜åŸå§‹æ•°æ®
                    original_data[channel_name] = {
                        "days": retention_result['days'],
                        "rates": retention_result['rates']
                    }

                st.session_state.lt_results_2y = lt_results_2y
                st.session_state.lt_results_5y = lt_results_5y
                st.session_state.visualization_data_5y = visualization_data_5y
                st.session_state.original_data = original_data
                st.success("LTæ‹Ÿåˆåˆ†æå®Œæˆï¼")

                # æ˜¾ç¤ºLTå€¼è¡¨æ ¼
                if lt_results_5y:
                    st.subheader("LTåˆ†æç»“æœ")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("#### 2å¹´LTç»“æœ")
                        results_2y_df = pd.DataFrame([
                            {
                                'æ¸ é“åç§°': r['data_source'],
                                '2å¹´LT': round(r['lt_value'], 2),
                                'RÂ²å¾—åˆ†': round(r['power_r2'], 3)
                            }
                            for r in lt_results_2y
                        ])
                        st.dataframe(results_2y_df, use_container_width=True)
                    
                    with col2:
                        st.markdown("#### 5å¹´LTç»“æœ")
                        results_5y_df = pd.DataFrame([
                            {
                                'æ¸ é“åç§°': r['data_source'],
                                '5å¹´LT': round(r['lt_value'], 2),
                                'RÂ²å¾—åˆ†': round(r['power_r2'], 3)
                            }
                            for r in lt_results_5y
                        ])
                        st.dataframe(results_5y_df, use_container_width=True)

                # æ˜¾ç¤ºå•æ¸ é“å›¾è¡¨ - 100å¤©ç‰ˆæœ¬ï¼ŒåŒæ—¶æ˜¾ç¤º2å¹´5å¹´LT
                if visualization_data_5y and original_data:
                    st.subheader("å„æ¸ é“100å¤©LTæ‹Ÿåˆå›¾è¡¨")
                    
                    # æŒ‰5å¹´LTå€¼æ’åº
                    sorted_channels = sorted(visualization_data_5y.items(), key=lambda x: x[1]['lt'])
                    
                    # æ¯è¡Œæ˜¾ç¤º2ä¸ªå›¾è¡¨
                    for i
