import streamlit as st
import os
import pandas as pd
import numpy as np
from pathlib import Path
import warnings
import datetime
import tempfile
import zipfile
import io
import matplotlib.pyplot as plt
import matplotlib as mpl
import re
from matplotlib.font_manager import FontProperties
import seaborn as sns
from scipy.optimize import curve_fit

# ==================== åŸºç¡€é…ç½® ====================
# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl.styles.stylesheet')
warnings.filterwarnings('ignore', category=UserWarning,
                        message="Could not infer format, so each element will be parsed individually")

# è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ - å¢å¼ºç‰ˆæœ¬
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    try:
        import matplotlib.font_manager as fm
        
        # ç³»ç»Ÿä¸­æ–‡å­—ä½“åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        chinese_fonts = [
            'SimHei',           # é»‘ä½“
            'Microsoft YaHei',  # å¾®è½¯é›…é»‘
            'SimSun',          # å®‹ä½“
            'KaiTi',           # æ¥·ä½“
            'FangSong',        # ä»¿å®‹
            'STSong',          # åæ–‡å®‹ä½“
            'STHeiti',         # åæ–‡é»‘ä½“
            'Arial Unicode MS', # Arial Unicode MS
            'DejaVu Sans',     # å¤‡ç”¨å­—ä½“
        ]
        
        # è·å–ç³»ç»Ÿæ‰€æœ‰å¯ç”¨å­—ä½“
        available_fonts = [f.name for f in fm.fontManager.ttflist]
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
        selected_font = None
        for font in chinese_fonts:
            if font in available_fonts:
                selected_font = font
                break
        
        if selected_font:
            # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = [selected_font, 'Microsoft YaHei', 'SimSun', 'Arial Unicode MS', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
            plt.rcParams['font.size'] = 10
            # å¼ºåˆ¶æ¸…é™¤å­—ä½“ç¼“å­˜å¹¶é‡æ–°è®¾ç½®
            try:
                fm._rebuild()
                # é¢å¤–çš„å­—ä½“è®¾ç½®
                plt.rcParams['font.family'] = 'sans-serif'
                mpl.rcParams.update({'font.sans-serif': [selected_font, 'Microsoft YaHei', 'SimSun', 'Arial Unicode MS', 'DejaVu Sans']})
                # è®¾ç½®å…¨å±€å­—ä½“
                mpl.font_manager.fontManager.addfont(None)
            except:
                pass
            st.success(f"å·²è®¾ç½®ä¸­æ–‡å­—ä½“: {selected_font}")
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun', 'Arial Unicode MS', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            plt.rcParams['font.size'] = 10
            plt.rcParams['font.family'] = 'sans-serif'
            st.warning("ä½¿ç”¨é»˜è®¤ä¸­æ–‡å­—ä½“è®¾ç½®")
        
        return True
        
    except Exception as e:
        st.warning(f"å­—ä½“è®¾ç½®å¤±è´¥: {e}")
        # ä½¿ç”¨é»˜è®¤è®¾ç½®ä½œä¸ºå¤‡ç”¨
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        plt.rcParams['font.size'] = 10
        plt.rcParams['font.family'] = 'sans-serif'
        return False

# åˆå§‹åŒ–å­—ä½“è®¾ç½®
setup_chinese_font()

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="LTV Analytics Platform",
    layout="wide",
    initial_sidebar_state="expanded",
    page_icon="ğŸ“Š"
)

# ==================== CSS æ ·å¼å®šä¹‰ ====================
st.markdown("""
<style>
    /* å…¨å±€æ ·å¼ */
    .main {
        background: #f8fafc;
        min-height: 100vh;
    }

    .block-container {
        padding: 1rem 1rem 3rem 1rem;
        max-width: 100%;
        background: transparent;
    }

    /* ä¸»æ ‡é¢˜åŒºåŸŸ */
    .main-header {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
        padding: 1.5rem;
        border-radius: 12px;
        box-shadow: 0 4px 20px rgba(30, 64, 175, 0.3);
        margin-bottom: 1.5rem;
        text-align: center;
        color: white;
    }

    .main-title {
        font-size: 2rem;
        font-weight: 700;
        color: white;
        margin-bottom: 0.5rem;
        text-shadow: 0 2px 4px rgba(0,0,0,0.3);
    }

    .main-subtitle {
        color: rgba(255,255,255,0.9);
        font-size: 1.2rem;
        font-weight: 400;
    }

    /* å¡ç‰‡æ ·å¼ */
    .glass-card {
        background: rgba(255, 255, 255, 0.95);
        border-radius: 12px;
        padding: 1.5rem;
        box-shadow: 0 8px 32px rgba(30, 64, 175, 0.1);
        border: 1px solid rgba(59, 130, 246, 0.2);
        margin-bottom: 1.5rem;
        backdrop-filter: blur(10px);
    }

    /* å¯¼èˆªæ­¥éª¤æ ·å¼ */
    .nav-container {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
        border-radius: 12px;
        padding: 1.5rem;
        margin-bottom: 1rem;
        color: white;
        box-shadow: 0 4px 20px rgba(30, 64, 175, 0.3);
    }

    /* æŒ‰é’®æ ·å¼ - ä¿®æ”¹æ‚¬åœæ•ˆæœä¸ºé»„è‰² */
    .stButton > button {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.6rem 2rem;
        font-weight: 600;
        transition: all 0.3s ease;
        box-shadow: 0 4px 15px rgba(30, 64, 175, 0.3);
    }

    .stButton > button:hover {
        background: linear-gradient(135deg, #f59e0b 0%, #fbbf24 100%);
        transform: translateY(-2px);
        box-shadow: 0 6px 20px rgba(245, 158, 11, 0.4);
    }

    /* å­æ­¥éª¤æ ·å¼ */
    .sub-steps {
        font-size: 0.8rem;
        color: rgba(255,255,255,0.7);
        margin-top: 0.3rem;
        line-height: 1.2;
    }

    /* è­¦å‘Šæ–‡å­—é¢œè‰² */
    .warning-text {
        color: #f59e0b !important;
    }

    /* åŸç†è§£é‡Šæ¡†æ ·å¼ */
    .principle-box {
        background: rgba(59, 130, 246, 0.05);
        border: 1px solid rgba(59, 130, 246, 0.2);
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }

    .principle-title {
        color: #1e40af;
        font-weight: 600;
        font-size: 1rem;
        margin-bottom: 0.5rem;
    }

    .principle-content {
        color: #374151;
        font-size: 0.9rem;
        line-height: 1.5;
    }

    /* æç¤ºæ¡†æ ·å¼ */
    .step-tip {
        background: #eff6ff;
        border: 1px solid #bfdbfe;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
        border-left: 4px solid #3b82f6;
    }

    .step-tip-title {
        color: #1e40af;
        font-weight: 600;
        font-size: 0.95rem;
        margin-bottom: 0.5rem;
    }

    .step-tip-content {
        color: #374151;
        font-size: 0.85rem;
        line-height: 1.4;
    }

    /* éšè—é»˜è®¤çš„Streamlitå…ƒç´  */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# ==================== é»˜è®¤é…ç½®æ•°æ® - é‡æ„ä¸ºæ¸ é“åç§°->æ¸ é“å·åˆ—è¡¨ ====================
DEFAULT_CHANNEL_MAPPING = {
    'æ€»ä½“': ['9000'],
    'æ–°åª’ä½“': ['500345', '500346', '500447', '500449', '500450', '500531', '500542'],
    'åº”ç”¨å®': ['5007XS', '500349', '500350'],
    'é¼ä¹-ç››ä¸–6': ['500285'],
    'é¼ä¹-ç››ä¸–7': ['500286'],
    'é…·æ´¾': ['5108', '5528'],
    'æ–°ç¾-åŒ—äº¬1': ['500274'],
    'æ–°ç¾-åŒ—äº¬2': ['500275'],
    'A_æ·±åœ³è›‹ä¸2': ['500316'],
    'è£è€€': ['500297'],
    'åä¸º': ['5057'],
    'vivo': ['5237'],
    'å°ç±³': ['5599'],
    'OPPO': ['5115'],
    'ç½‘æ˜“': ['500471', '500480', '500481', '500482'],
    'åä¸ºéå•†åº—-å“ä¼—': ['500337', '500338', '500343', '500445', '500383', '500444', '500441'],
    'é­…æ—': ['5072'],
    'OPPOéå•†åº—': ['500287', '500288'],
    'vivoéå•†åº—': ['5187'],
    'ç™¾åº¦sem--ç™¾åº¦æ—¶ä»£å®‰å“': ['500398', '500400', '500404'],
    'ç™¾åº¦sem--ç™¾åº¦æ—¶ä»£ios': ['500402', '500403', '500405'],
    'ç™¾é’è—¤-å®‰å“': ['500377', '500379', '500435', '500436', '500490', '500491', '500434', '500492'],
    'ç™¾é’è—¤-ios': ['500437'],
    'å°ç±³éå•†åº—': ['500170'],
    'åä¸ºéå•†åº—-æ˜Ÿç«': ['500532', '500533', '500534', '500537', '500538', '500539', '500540', '500541'],
    'å¾®åš-èœœæ©˜': ['500504', '500505'],
    'å¾®åš-å¤®å¹¿': ['500367', '500368', '500369'],
    'å¹¿ç‚¹é€š': ['500498', '500497', '500500', '500501', '500496', '500499'],
    'ç½‘æ˜“æ˜“æ•ˆ': ['500514', '500515', '500516']
}

# åˆ›å»ºåå‘æ˜ å°„ï¼šæ¸ é“å·->æ¸ é“åç§°
def create_reverse_mapping(channel_mapping):
    reverse_mapping = {}
    for channel_name, pids in channel_mapping.items():
        for pid in pids:
            reverse_mapping[str(pid)] = channel_name
    return reverse_mapping

# ==================== æ—¥æœŸå¤„ç†å‡½æ•° ====================
def get_default_target_month():
    today = datetime.datetime.now()
    if today.month <= 2:
        target_year = today.year - 1
        target_month = today.month + 10
    else:
        target_year = today.year
        target_month = today.month - 2
    return f"{target_year}-{target_month:02d}"

# ==================== æ•°æ®ç±»å‹è½¬æ¢å‡½æ•° ====================
def safe_convert_to_numeric(value):
    """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•°å€¼ç±»å‹"""
    if pd.isna(value) or value == '' or value is None:
        return 0
    try:
        if isinstance(value, str):
            value = value.strip()
            if value == '' or value.lower() in ['nan', 'null', 'none']:
                return 0
        return pd.to_numeric(value, errors='coerce')
    except:
        return 0

# ==================== æ•°æ®é¢„è§ˆä¼˜åŒ–å‡½æ•° ====================
def optimize_dataframe_for_preview(df, max_rows=2):
    """ä¼˜åŒ–DataFrameé¢„è§ˆï¼šæœ‰å€¼çš„åˆ—æ”¾å‰é¢ï¼Œè·³è¿‡dateä¸º'æ—¥æœŸ'çš„è¡Œ"""
    preview_df = df.copy()
    
    # è·³è¿‡dateå€¼ä¸º"æ—¥æœŸ"çš„è¡Œ
    if 'date' in preview_df.columns:
        preview_df = preview_df[preview_df['date'] != 'æ—¥æœŸ']
    if 'æ—¥æœŸ' in preview_df.columns:
        preview_df = preview_df[preview_df['æ—¥æœŸ'] != 'æ—¥æœŸ']
    
    # å–å‰max_rowsè¡Œ
    preview_df = preview_df.head(max_rows)
    
    if preview_df.empty:
        return preview_df
    
    # è®¡ç®—æ¯åˆ—çš„éç©ºå€¼æ•°é‡
    non_null_counts = {}
    for col in preview_df.columns:
        non_null_count = preview_df[col].notna().sum()
        # æ’é™¤å…¨ä¸º0æˆ–ç©ºçš„æ•°å€¼åˆ—
        if preview_df[col].dtype in ['int64', 'float64']:
            non_zero_count = (preview_df[col] != 0).sum()
            non_null_counts[col] = non_null_count + non_zero_count
        else:
            non_null_counts[col] = non_null_count
    
    # æŒ‰éç©ºå€¼æ•°é‡æ’åºåˆ—
    sorted_columns = sorted(non_null_counts.keys(), key=lambda x: non_null_counts[x], reverse=True)
    
    # ç¡®ä¿'æ•°æ®æ¥æº'åˆ—åœ¨æœ€å‰é¢ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'æ•°æ®æ¥æº' in sorted_columns:
        sorted_columns.remove('æ•°æ®æ¥æº')
        sorted_columns.insert(0, 'æ•°æ®æ¥æº')
    
    return preview_df[sorted_columns]

# ==================== æ¸ é“æ˜ å°„å¤„ç†å‡½æ•° - ä¿®æ”¹ä¸ºæ”¯æŒæ–°æ ¼å¼ ====================
def parse_channel_mapping_from_excel(channel_file):
    """ä»ä¸Šä¼ çš„Excelæ–‡ä»¶è§£ææ¸ é“æ˜ å°„"""
    try:
        df = pd.read_excel(channel_file)
        channel_mapping = {}
        
        for _, row in df.iterrows():
            channel_name = str(row.iloc[0]).strip()
            if pd.isna(channel_name) or channel_name == '' or channel_name == 'nan':
                continue
                
            pids = []
            for col_idx in range(1, len(row)):
                pid = row.iloc[col_idx]
                if pd.isna(pid) or str(pid).strip() in ['', 'nan', 'ã€€', ' ']:
                    continue
                # ç¡®ä¿æ¸ é“å·ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œå»é™¤å°æ•°
                pid_str = str(int(float(pid))) if isinstance(pid, (int, float)) else str(pid).strip()
                if pid_str:
                    pids.append(pid_str)
            
            if pids:
                channel_mapping[channel_name] = pids
                    
        return channel_mapping
    except Exception as e:
        st.error(f"è§£ææ¸ é“æ˜ å°„æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
        return {}

# ==================== æ–‡ä»¶æ•´åˆæ ¸å¿ƒå‡½æ•° - ä¼˜åŒ–æ€§èƒ½ ====================
@st.cache_data
def integrate_excel_files_cached(file_names, file_contents, target_month, reverse_mapping):
    """ç¼“å­˜ç‰ˆæœ¬çš„æ–‡ä»¶æ•´åˆå‡½æ•°"""
    all_data = pd.DataFrame()
    processed_count = 0
    mapping_warnings = []

    for i, (file_name, file_content) in enumerate(zip(file_names, file_contents)):
        source_name = os.path.splitext(file_name)[0]
        
        # æ¸ é“æ˜ å°„å¤„ç†
        if source_name in reverse_mapping:
            mapped_source = reverse_mapping[source_name]
        else:
            mapped_source = source_name
            mapping_warnings.append(f"æ–‡ä»¶ '{source_name}' æœªåœ¨æ¸ é“æ˜ å°„è¡¨ä¸­æ‰¾åˆ°å¯¹åº”é¡¹")

        try:
            # ä»å†…å­˜ä¸­è¯»å–Excelæ–‡ä»¶
            xls = pd.ExcelFile(io.BytesIO(file_content))
            sheet_names = xls.sheet_names

            # æŸ¥æ‰¾ç›®æ ‡å·¥ä½œè¡¨
            ocpx_sheet = None
            for sheet in sheet_names:
                if "ocpxç›‘æµ‹ç•™å­˜æ•°" in sheet:
                    ocpx_sheet = sheet
                    break

            if ocpx_sheet:
                file_data = pd.read_excel(io.BytesIO(file_content), sheet_name=ocpx_sheet)
            else:
                file_data = pd.read_excel(io.BytesIO(file_content), sheet_name=0)

            if file_data is not None and not file_data.empty:
                # æ•°æ®å¤„ç†é€»è¾‘ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                file_data_copy = file_data.copy()
                
                # æ£€æµ‹å¹¶å¤„ç†æ•°æ®æ ¼å¼
                has_stat_date = 'stat_date' in file_data_copy.columns
                retain_columns = [f'new_retain_{i}' for i in range(1, 31)]
                has_retain_columns = any(col in file_data_copy.columns for col in retain_columns)

                if has_stat_date and has_retain_columns:
                    # æ–°æ ¼å¼è¡¨å¤„ç†
                    standardized_data = file_data_copy.copy()
                    if 'new' in standardized_data.columns:
                        standardized_data['å›ä¼ æ–°å¢æ•°'] = standardized_data['new'].apply(safe_convert_to_numeric)

                    for i in range(1, 31):
                        retain_col = f'new_retain_{i}'
                        if retain_col in standardized_data.columns:
                            standardized_data[str(i)] = standardized_data[retain_col].apply(safe_convert_to_numeric)

                    date_col = 'stat_date'
                    standardized_data[date_col] = pd.to_datetime(standardized_data[date_col], errors='coerce')
                    standardized_data[date_col] = standardized_data[date_col].dt.strftime('%Y-%m-%d')
                    standardized_data['æ—¥æœŸ'] = standardized_data[date_col]
                    standardized_data['month'] = standardized_data[date_col].str[:7]

                    filtered_data = standardized_data[standardized_data['month'] == target_month].copy()

                    if not filtered_data.empty:
                        filtered_data.insert(0, 'æ•°æ®æ¥æº', mapped_source)
                        if 'stat_date' in filtered_data.columns:
                            filtered_data['date'] = filtered_data['stat_date']
                        all_data = pd.concat([all_data, filtered_data], ignore_index=True)
                        processed_count += 1
                else:
                    # ä¼ ç»Ÿæ ¼å¼è¡¨å¤„ç†
                    retention_col = None
                    for col in file_data_copy.columns:
                        if 'ç•™å­˜å¤©æ•°' in str(col):
                            retention_col = col
                            break

                    report_users_col = None
                    for col in file_data_copy.columns:
                        if 'å›ä¼ æ–°å¢æ•°' in str(col):
                            report_users_col = col
                            break
                        if 'total_new_users' in str(col).lower():
                            report_users_col = col
                            break

                    column_b = file_data_copy.columns[1] if len(file_data_copy.columns) > 1 else None

                    if report_users_col and report_users_col != 'å›ä¼ æ–°å¢æ•°':
                        file_data_copy['å›ä¼ æ–°å¢æ•°'] = file_data_copy[report_users_col].apply(safe_convert_to_numeric)
                    elif not report_users_col and column_b:
                        file_data_copy['å›ä¼ æ–°å¢æ•°'] = file_data_copy[column_b].apply(safe_convert_to_numeric)

                    for i in range(1, 31):
                        col_name = str(i)
                        if col_name in file_data_copy.columns:
                            file_data_copy[col_name] = file_data_copy[col_name].apply(safe_convert_to_numeric)

                    # ç®€åŒ–æ—¥æœŸå¤„ç†ï¼Œç›´æ¥æŒ‰æœˆä»½ç­›é€‰
                    date_col = None
                    for col in file_data_copy.columns:
                        if 'æ—¥æœŸ' in str(col):
                            date_col = col
                            break

                    if date_col:
                        file_data_copy['month'] = file_data_copy[date_col].apply(
                            lambda x: str(x)[:7] if isinstance(x, str) else None
                        )
                        filtered_data = file_data_copy[file_data_copy['month'] == target_month].copy()
                    else:
                        filtered_data = file_data_copy.copy()

                    if not filtered_data.empty:
                        filtered_data.insert(0, 'æ•°æ®æ¥æº', mapped_source)
                        if retention_col is not None:
                            filtered_data.rename(columns={retention_col: 'date'}, inplace=True)
                        elif date_col and date_col != 'date':
                            filtered_data['date'] = filtered_data[date_col]
                        all_data = pd.concat([all_data, filtered_data], ignore_index=True)
                        processed_count += 1

        except Exception as e:
            st.error(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {str(e)}")

    return all_data, processed_count, mapping_warnings

def integrate_excel_files_streamlit(uploaded_files, target_month=None, channel_mapping=None):
    """ä¼˜åŒ–æ€§èƒ½çš„æ–‡ä»¶æ•´åˆå‡½æ•°"""
    if target_month is None:
        target_month = get_default_target_month()

    # åˆ›å»ºåå‘æ˜ å°„
    if channel_mapping:
        reverse_mapping = create_reverse_mapping(channel_mapping)
    else:
        reverse_mapping = create_reverse_mapping(DEFAULT_CHANNEL_MAPPING)

    # å‡†å¤‡ç¼“å­˜æ•°æ®
    file_names = [f.name for f in uploaded_files]
    file_contents = [f.read() for f in uploaded_files]
    
    return integrate_excel_files_cached(file_names, file_contents, target_month, reverse_mapping)

# ==================== ç•™å­˜ç‡è®¡ç®—å‡½æ•° - ä¿®æ”¹ä¸ºä»¥æ–°å¢ç”¨æˆ·å‡å€¼ä¸ºåŸºæ•° ====================
def calculate_retention_rates_new_method(df):
    """æ–°çš„ç•™å­˜ç‡è®¡ç®—æ–¹æ³•ï¼šæŒ‰æ¸ é“è®¡ç®—å¹³å‡æ–°å¢æ•°ä½œä¸ºåŸºæ•°ï¼Œå„å¤©ç•™å­˜æ•°/å¹³å‡æ–°å¢æ•°"""
    retention_results = []
    data_sources = df['æ•°æ®æ¥æº'].unique()

    for source in data_sources:
        source_data = df[df['æ•°æ®æ¥æº'] == source].copy()
        
        # è®¡ç®—å¹³å‡æ–°å¢ç”¨æˆ·æ•°ä½œä¸ºåŸºæ•°
        new_users_values = []
        for _, row in source_data.iterrows():
            new_users = safe_convert_to_numeric(row.get('å›ä¼ æ–°å¢æ•°', 0))
            if new_users > 0:
                new_users_values.append(new_users)
        
        if not new_users_values:
            continue
        
        avg_new_users = np.mean(new_users_values)
        
        # è®¡ç®—1-30å¤©çš„å¹³å‡ç•™å­˜æ•°ï¼Œå¹¶ç”¨å¹³å‡æ–°å¢æ•°ä½œä¸ºåŸºæ•°è®¡ç®—ç•™å­˜ç‡
        retention_data = {'data_source': source, 'avg_new_users': avg_new_users}
        days = []
        rates = []
        
        for day in range(1, 31):
            day_col = str(day)
            day_retain_values = []
            
            for _, row in source_data.iterrows():
                if day_col in row and not pd.isna(row[day_col]):
                    retain_count = safe_convert_to_numeric(row[day_col])
                    if retain_count >= 0:  # å…è®¸0å€¼
                        day_retain_values.append(retain_count)
            
            if day_retain_values:
                avg_retain = np.mean(day_retain_values)
                # ä¿®æ”¹è®¡ç®—æ–¹æ³•ï¼šç•™å­˜ç‡ = å¹³å‡ç•™å­˜æ•° / å¹³å‡æ–°å¢æ•°
                retention_rate = avg_retain / avg_new_users if avg_new_users > 0 else 0
                
                # ç•™å­˜ç‡èŒƒå›´ä¸º 0 â‰¤ ç•™å­˜ç‡ â‰¤ 1.0
                if 0 <= retention_rate <= 1.0:
                    days.append(day)
                    rates.append(retention_rate)
        
        if days:
            retention_results.append({
                'data_source': source,
                'days': np.array(days),
                'rates': np.array(rates),
                'avg_new_users': avg_new_users
            })

    return retention_results

# ==================== æ•°å­¦å»ºæ¨¡å‡½æ•° ====================
def power_function(x, a, b):
    """å¹‚å‡½æ•°ï¼šy = a * x^b"""
    return a * np.power(x, b)

def exponential_function(x, c, d):
    """æŒ‡æ•°å‡½æ•°ï¼šy = c * exp(d * x)"""
    return c * np.exp(d * x)

def calculate_cumulative_lt(days_array, rates_array, target_days):
    """è®¡ç®—æŒ‡å®šå¤©æ•°çš„ç´¯ç§¯LTå€¼"""
    result = {}
    for day in target_days:
        idx = np.searchsorted(days_array, day, side='right')
        if idx > 0:
            cumulative_lt = 1.0 + np.sum(rates_array[1:idx])
            result[day] = cumulative_lt
    return result

def calculate_lt_advanced(retention_result, channel_name, lt_years=5, return_curve_data=False, key_days=None):
    """æŒ‰æ¸ é“è§„åˆ™è®¡ç®— LT"""
    # æ¸ é“è§„åˆ™
    CHANNEL_RULES = {
        "åä¸º": {"stage_2": [30, 120], "stage_3_base": [120, 220]},
        "å°ç±³": {"stage_2": [30, 190], "stage_3_base": [190, 290]},
        "oppo": {"stage_2": [30, 160], "stage_3_base": [160, 260]},
        "vivo": {"stage_2": [30, 150], "stage_3_base": [150, 250]},
        "iphone": {"stage_2": [30, 150], "stage_3_base": [150, 250], "stage_2_func": "log"},
        "å…¶ä»–": {"stage_2": [30, 100], "stage_3_base": [100, 200]}
    }

    # æ¸ é“è§„åˆ™åŒ¹é…
    if re.search(r'åä¸º', channel_name):
        rules = CHANNEL_RULES["åä¸º"]
    elif re.search(r'å°ç±³', channel_name):
        rules = CHANNEL_RULES["å°ç±³"]
    elif re.search(r'[oO][pP][pP][oO]', channel_name):
        rules = CHANNEL_RULES["oppo"]
    elif re.search(r'vivo', channel_name):
        rules = CHANNEL_RULES["vivo"]
    elif re.search(r'[iI][pP]hone', channel_name):
        rules = CHANNEL_RULES["iphone"]
    else:
        rules = CHANNEL_RULES["å…¶ä»–"]
        
    stage_2_start, stage_2_end = rules["stage_2"]
    stage_3_base_start, stage_3_base_end = rules["stage_3_base"]

    max_days = lt_years * 365

    days = retention_result["days"]
    rates = retention_result["rates"]

    fit_params = {}

    # ç¬¬ä¸€é˜¶æ®µ - å¹‚å‡½æ•°æ‹Ÿåˆ
    try:
        popt_power, _ = curve_fit(power_function, days, rates)
        a, b = popt_power
        fit_params["power"] = {"a": a, "b": b}

        days_full = np.arange(1, 31)
        rates_full = power_function(days_full, a, b)
        lt1_to_30 = np.sum(rates_full)
    except Exception as e:
        lt1_to_30 = 0.0
        a, b = 1.0, -1.0

    # ç¬¬äºŒé˜¶æ®µ
    try:
        days_stage_2 = np.arange(stage_2_start, stage_2_end + 1)
        rates_stage_2 = power_function(days_stage_2, a, b)
        lt_stage_2 = np.sum(rates_stage_2)
    except Exception as e:
        lt_stage_2 = 0.0
        rates_stage_2 = np.array([])

    # ç¬¬ä¸‰é˜¶æ®µ - æŒ‡æ•°æ‹Ÿåˆ
    try:
        days_stage_3_base = np.arange(stage_3_base_start, stage_3_base_end + 1)
        rates_stage_3_base = power_function(days_stage_3_base, a, b)

        initial_c = rates_stage_3_base[0]
        initial_d = -0.001
        popt_exp, _ = curve_fit(
            exponential_function,
            days_stage_3_base,
            rates_stage_3_base,
            p0=[initial_c, initial_d],
            bounds=([0, -np.inf], [np.inf, 0])
        )
        c, d = popt_exp
        fit_params["exponential"] = {"c": c, "d": d}
        days_stage_3 = np.arange(stage_3_base_start, max_days + 1)
        rates_stage_3 = exponential_function(days_stage_3, c, d)
        lt_stage_3 = np.sum(rates_stage_3)
    except Exception as e:
        days_stage_3 = np.arange(stage_3_base_start, max_days + 1)
        rates_stage_3 = power_function(days_stage_3, a, b) if 'a' in locals() else np.zeros(len(days_stage_3))
        lt_stage_3 = np.sum(rates_stage_3)

    total_lt = 1.0 + lt1_to_30 + lt_stage_2 + lt_stage_3

    try:
        predicted_rates = power_function(days, a, b)
        r2_score = 1 - np.sum((rates - predicted_rates) ** 2) / np.sum((rates - np.mean(rates)) ** 2)
    except:
        r2_score = 0.0

    if return_curve_data:
        all_days = np.concatenate([days_full, days_stage_2, days_stage_3])
        
        if 'rates_stage_2' not in locals():
            rates_stage_2 = power_function(days_stage_2, a, b)
        
        all_rates = np.concatenate([rates_full, rates_stage_2, rates_stage_3])

        sort_idx = np.argsort(all_days)
        all_days = all_days[sort_idx]
        all_rates = all_rates[sort_idx]

        max_idx = np.searchsorted(all_days, lt_years * 365, side='right')
        all_days = all_days[:max_idx]
        all_rates = all_rates[:max_idx]

        key_days_lt = {}
        if key_days:
            key_days_lt = calculate_cumulative_lt(all_days, all_rates, key_days)

        return {
            'lt_value': total_lt,
            'fit_params': fit_params,
            'power_r2': max(0, min(1, r2_score)),
            'success': True,
            'model_used': 'power+exponential',
            'curve_days': all_days,
            'curve_rates': all_rates,
            'key_days_lt': key_days_lt
        }

    return total_lt

# ==================== å•æ¸ é“å›¾è¡¨ç”Ÿæˆå‡½æ•° - å¼ºåŒ–ä¸­æ–‡æ˜¾ç¤º ====================
def create_individual_channel_chart(channel_name, curve_data, original_data, max_days=100):
    """åˆ›å»ºå•ä¸ªæ¸ é“çš„100å¤©LTæ‹Ÿåˆå›¾è¡¨"""
    # å¼ºåˆ¶é‡æ–°è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans', 'SimSun', 'Arial Unicode MS']
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['font.size'] = 10
    
    fig, ax = plt.subplots(figsize=(8, 6))
    
    # ç»˜åˆ¶å®é™…æ•°æ®ç‚¹
    if channel_name in original_data:
        ax.scatter(
            original_data[channel_name]["days"],
            original_data[channel_name]["rates"],
            color='red',
            s=60,
            alpha=0.8,
            label='å®é™…æ•°æ®',
            zorder=3
        )
    
    # é™åˆ¶æ‹Ÿåˆæ›²çº¿åˆ°100å¤©
    curve_days = curve_data["days"]
    curve_rates = curve_data["rates"]
    
    # ç­›é€‰100å¤©å†…çš„æ•°æ®
    mask = curve_days <= max_days
    curve_days_filtered = curve_days[mask]
    curve_rates_filtered = curve_rates[mask]
    
    # ç»˜åˆ¶æ‹Ÿåˆæ›²çº¿
    ax.plot(
        curve_days_filtered,
        curve_rates_filtered,
        color='blue',
        linewidth=2.5,
        label='æ‹Ÿåˆæ›²çº¿',
        zorder=2
    )
    
    # è®¾ç½®å›¾è¡¨æ ·å¼ - å¼ºåˆ¶ä½¿ç”¨ä¸­æ–‡
    ax.set_xlim(0, max_days)
    ax.set_ylim(0, 0.6)
    ax.set_xlabel('ç•™å­˜å¤©æ•°', fontsize=12, fontfamily='SimHei')
    ax.set_ylabel('ç•™å­˜ç‡', fontsize=12, fontfamily='SimHei')
    ax.set_title(f'{channel_name} ({max_days}å¤©LTæ‹Ÿåˆ)', fontsize=14, fontweight='bold', fontfamily='SimHei')
    ax.grid(True, linestyle='--', alpha=0.4)
    
    # è®¾ç½®å›¾ä¾‹ - å¼ºåˆ¶ä¸­æ–‡å­—ä½“
    legend = ax.legend(fontsize=10)
    for text in legend.get_texts():
        text.set_fontfamily('SimHei')
    
    # è®¾ç½®Yè½´åˆ»åº¦ä¸ºç™¾åˆ†æ¯”
    y_ticks = [0, 0.15, 0.3, 0.45, 0.6]
    y_labels = ['0%', '15%', '30%', '45%', '60%']
    ax.set_yticks(y_ticks)
    ax.set_yticklabels(y_labels)
    
    # å¼ºåˆ¶è®¾ç½®åˆ»åº¦æ ‡ç­¾å­—ä½“
    for label in ax.get_xticklabels() + ax.get_yticklabels():
        label.set_fontfamily('SimHei')
    
    plt.tight_layout()
    return fig

# ==================== ä¸»åº”ç”¨ç¨‹åº ====================

# ä¸»æ ‡é¢˜
st.markdown("""
<div class="main-header">
    <div class="main-title">ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†æç³»ç»Ÿ</div>
    <div class="main-subtitle">åŸºäºåˆ†é˜¶æ®µæ•°å­¦å»ºæ¨¡çš„LTVé¢„æµ‹</div>
</div>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–session state
session_keys = [
    'channel_mapping', 'merged_data', 'cleaned_data', 'retention_data',
    'lt_results_2y', 'lt_results_5y', 'arpu_data', 'ltv_results', 'current_step',
    'excluded_data', 'excluded_dates_info', 'show_exclusion', 'show_manual_arpu',
    'visualization_data_5y', 'original_data', 'show_upload_interface'
]
for key in session_keys:
    if key not in st.session_state:
        st.session_state[key] = None

# è®¾ç½®é»˜è®¤å€¼
if st.session_state.channel_mapping is None:
    st.session_state.channel_mapping = DEFAULT_CHANNEL_MAPPING
if st.session_state.current_step is None:
    st.session_state.current_step = 0
if st.session_state.show_exclusion is None:
    st.session_state.show_exclusion = False
if st.session_state.show_manual_arpu is None:
    st.session_state.show_manual_arpu = False
if st.session_state.show_upload_interface is None:
    st.session_state.show_upload_interface = False

# ==================== åˆ†ææ­¥éª¤å®šä¹‰ - é‡æ„ä¸º3æ­¥ ====================
ANALYSIS_STEPS = [
    {
        "name": "LTæ¨¡å‹æ„å»º",
        "sub_steps": ["æ•°æ®ä¸Šä¼ æ±‡æ€»", "å¼‚å¸¸å‰”é™¤", "ç•™å­˜ç‡è®¡ç®—", "LTæ‹Ÿåˆåˆ†æ"]
    },
    {"name": "ARPUè®¡ç®—"},
    {"name": "LTVç»“æœæŠ¥å‘Š"}
]

# ==================== ä¾§è¾¹æ å¯¼èˆª ====================
with st.sidebar:
    st.markdown('<div class="nav-container">', unsafe_allow_html=True)
    st.markdown('<h4 style="text-align: center; margin-bottom: 1rem; color: white;">åˆ†ææµç¨‹</h4>',
                unsafe_allow_html=True)

    for i, step in enumerate(ANALYSIS_STEPS):
        button_text = f"{i + 1}. {step['name']}"
        if st.button(button_text, key=f"nav_{i}", use_container_width=True,
                     type="primary" if i == st.session_state.current_step else "secondary"):
            st.session_state.current_step = i
            st.rerun()
        
        # åªåœ¨LTæ¨¡å‹æ„å»ºæ—¶æ˜¾ç¤ºå­æ­¥éª¤
        if "sub_steps" in step and i == st.session_state.current_step and step['name'] == "LTæ¨¡å‹æ„å»º":
            sub_steps_text = " â€¢ ".join(step["sub_steps"])
            st.markdown(f'<div class="sub-steps">{sub_steps_text}</div>', unsafe_allow_html=True)

    st.markdown('</div>', unsafe_allow_html=True)

# ==================== é¡µé¢è·¯ç”± ====================
current_page = ANALYSIS_STEPS[st.session_state.current_step]["name"]

# ==================== é¡µé¢å†…å®¹ ====================

if current_page == "LTæ¨¡å‹æ„å»º":
    # åŸç†è§£é‡Š
    st.markdown("""
    <div class="principle-box">
        <div class="principle-title">ğŸ“š LTæ¨¡å‹æ„å»ºåŸç†</div>
        <div class="principle-content">
        LTæ¨¡å‹æ„å»ºåŒ…å«å››ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼š<br>
        <strong>1. æ•°æ®ä¸Šä¼ æ±‡æ€»ï¼š</strong>æ•´åˆå¤šä¸ªExcelæ–‡ä»¶ï¼Œæ”¯æŒæ–°æ ¼å¼è¡¨å’Œä¼ ç»Ÿæ ¼å¼è¡¨<br>
        <strong>2. å¼‚å¸¸å‰”é™¤ï¼š</strong>æŒ‰éœ€æ¸…ç†å¼‚å¸¸æ•°æ®ï¼Œæé«˜æ¨¡å‹å‡†ç¡®æ€§<br>
        <strong>3. ç•™å­˜ç‡è®¡ç®—ï¼š</strong>æŒ‰æ¸ é“è®¡ç®—å¹³å‡æ–°å¢ç”¨æˆ·æ•°ä½œä¸ºåŸºæ•°ï¼Œå„å¤©ç•™å­˜æ•°Ã·å¹³å‡æ–°å¢æ•°<br>
        <strong>4. LTæ‹Ÿåˆåˆ†æï¼š</strong>é‡‡ç”¨ä¸‰é˜¶æ®µåˆ†å±‚å»ºæ¨¡ï¼Œé¢„æµ‹ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸé•¿åº¦
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # æ­¥éª¤1ï¼šæ•°æ®ä¸Šä¼ ä¸æ±‡æ€»
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.subheader("1. æ•°æ®ä¸Šä¼ ä¸æ±‡æ€»")
    
    # é»˜è®¤æ¸ é“æ˜ å°„ - é»˜è®¤å±•å¼€
    with st.expander("æŸ¥çœ‹é»˜è®¤æ¸ é“æ˜ å°„ï¼ˆğŸ“‹ è¯·æŒ‰æ¸ é“åç§°å‘½åæ–‡ä»¶ï¼Œç”¨äºARPUè®¡ç®—ï¼‰", expanded=True):
        st.markdown("""
        <div class="step-tip">
            <div class="step-tip-title">ğŸ’¡ é‡è¦æç¤º</div>
            <div class="step-tip-content">
            è¯·å°†Excelæ–‡ä»¶æŒ‰ç…§ä¸‹è¡¨ä¸­çš„<strong>æ¸ é“åç§°</strong>è¿›è¡Œå‘½åï¼Œè¿™æ ·ç³»ç»Ÿå¯ä»¥è‡ªåŠ¨åŒ¹é…ARPUæ•°æ®ã€‚<br>
            ä¾‹å¦‚ï¼šåä¸º.xlsxã€å°ç±³.xlsxã€OPPO.xlsx ç­‰
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        default_mapping_rows = []
        for channel_name, pids in DEFAULT_CHANNEL_MAPPING.items():
            for pid in pids:
                default_mapping_rows.append({'æ¸ é“åç§°': channel_name, 'æ¸ é“å·': pid})
        default_mapping_df = pd.DataFrame(default_mapping_rows)
        st.dataframe(default_mapping_df, use_container_width=True)
    
    # æŒ‰é’®æ§åˆ¶æ˜¾ç¤ºä¸Šä¼ ç•Œé¢
    if not st.session_state.show_upload_interface:
        if st.button("å¼€å§‹æ•°æ®ä¸Šä¼ ", type="primary", use_container_width=True):
            st.session_state.show_upload_interface = True
            st.rerun()
    else:
        # æ¸ é“æ˜ å°„æ–‡ä»¶ä¸Šä¼ 
        channel_mapping_file = st.file_uploader(
            "ä¸Šä¼ æ¸ é“æ˜ å°„æ–‡ä»¶ (Excelæ ¼å¼ï¼Œå¯é€‰)",
            type=['xlsx', 'xls'],
            help="æ ¼å¼ï¼šç¬¬ä¸€åˆ—ä¸ºæ¸ é“åç§°ï¼Œåç»­åˆ—ä¸ºå¯¹åº”çš„æ¸ é“å·"
        )
        
        if channel_mapping_file:
            try:
                custom_mapping = parse_channel_mapping_from_excel(channel_mapping_file)
                if custom_mapping and isinstance(custom_mapping, dict) and len(custom_mapping) > 0:
                    st.session_state.channel_mapping = custom_mapping
                    st.success(f"æ¸ é“æ˜ å°„æ–‡ä»¶åŠ è½½æˆåŠŸï¼å…±åŒ…å« {len(custom_mapping)} ä¸ªæ¸ é“")
                    
                    # è‡ªåŠ¨å±•å¼€æ˜ å°„è¯¦æƒ…
                    with st.expander("æŸ¥çœ‹æ¸ é“æ˜ å°„è¯¦æƒ…", expanded=True):
                        mapping_rows = []
                        for channel_name, pids in custom_mapping.items():
                            for pid in pids:
                                mapping_rows.append({'æ¸ é“åç§°': channel_name, 'æ¸ é“å·': pid})
                        mapping_df = pd.DataFrame(mapping_rows)
                        st.dataframe(mapping_df, use_container_width=True)
                else:
                    st.error("æ¸ é“æ˜ å°„æ–‡ä»¶è§£æå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤æ˜ å°„")
            except Exception as e:
                st.error(f"è¯»å–æ¸ é“æ˜ å°„æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")
        else:
            st.info("æœªä¸Šä¼ æ¸ é“æ˜ å°„æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤æ˜ å°„å…³ç³»")

        # æ•°æ®æ–‡ä»¶ä¸Šä¼ 
        uploaded_files = st.file_uploader(
            "é€‰æ‹©Excelæ•°æ®æ–‡ä»¶",
            type=['xlsx', 'xls'],
            accept_multiple_files=True,
            help="æ”¯æŒä¸Šä¼ å¤šä¸ªExcelæ–‡ä»¶"
        )

        default_month = get_default_target_month()
        target_month = st.text_input("ç›®æ ‡æœˆä»½ (YYYY-MM)", value=default_month)

        if uploaded_files:
            st.info(f"å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")

            if st.button("å¼€å§‹å¤„ç†æ•°æ®", type="primary", use_container_width=True):
                with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®æ–‡ä»¶..."):
                    try:
                        merged_data, processed_count, mapping_warnings = integrate_excel_files_streamlit(
                            uploaded_files, target_month, st.session_state.channel_mapping
                        )

                        if merged_data is not None and not merged_data.empty:
                            st.session_state.merged_data = merged_data
                            st.success(f"æ•°æ®å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {processed_count} ä¸ªæ–‡ä»¶")

                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("æ€»è®°å½•æ•°", f"{len(merged_data):,}")
                            with col2:
                                st.metric("æ•°æ®æ¥æºæ•°", merged_data['æ•°æ®æ¥æº'].nunique())
                            with col3:
                                if 'å›ä¼ æ–°å¢æ•°' in merged_data.columns:
                                    total_users = merged_data['å›ä¼ æ–°å¢æ•°'].sum()
                                    st.metric("æ€»æ–°å¢ç”¨æˆ·", f"{total_users:,.0f}")

                            if mapping_warnings:
                                st.warning("ä»¥ä¸‹æ–‡ä»¶æœªåœ¨æ¸ é“æ˜ å°„ä¸­æ‰¾åˆ°å¯¹åº”å…³ç³»ï¼š")
                                for warning in mapping_warnings:
                                    st.text(f"â€¢ {warning}")

                            # ä¼˜åŒ–çš„æ•°æ®é¢„è§ˆ - æ¯ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸¤è¡Œ
                            st.subheader("æ•°æ®é¢„è§ˆ")
                            unique_sources = merged_data['æ•°æ®æ¥æº'].unique()
                            
                            for source in unique_sources:
                                source_data = merged_data[merged_data['æ•°æ®æ¥æº'] == source]
                                optimized_preview = optimize_dataframe_for_preview(source_data, max_rows=2)
                                st.markdown(f"**{source}ï¼š**")
                                st.dataframe(optimized_preview, use_container_width=True)
                                
                        else:
                            st.error("æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
                    except Exception as e:
                        st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")
        else:
            st.info("è¯·é€‰æ‹©Excelæ–‡ä»¶å¼€å§‹æ•°æ®å¤„ç†")

    st.markdown('</div>', unsafe_allow_html=True)

    # æ­¥éª¤2ï¼šå¼‚å¸¸æ•°æ®å‰”é™¤ï¼ˆæŒ‰éœ€æ˜¾ç¤ºï¼‰
    if st.session_state.merged_data is not None:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("2. å¼‚å¸¸æ•°æ®å‰”é™¤")
        
        if not st.session_state.show_exclusion:
            if st.button("éœ€è¦å¼‚å¸¸æ•°æ®å‰”é™¤", use_container_width=True):
                st.session_state.show_exclusion = True
                st.rerun()
            st.info("å¦‚æ— éœ€å‰”é™¤å¼‚å¸¸æ•°æ®ï¼Œå¯ç›´æ¥è¿›è¡Œä¸‹ä¸€æ­¥")
        else:
            merged_data = st.session_state.merged_data
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("### æŒ‰æ•°æ®æ¥æºå‰”é™¤")
                all_sources = merged_data['æ•°æ®æ¥æº'].unique().tolist()
                excluded_sources = st.multiselect("é€‰æ‹©è¦å‰”é™¤çš„æ•°æ®æ¥æº", options=all_sources)

            with col2:
                st.markdown("### æŒ‰æ—¥æœŸå‰”é™¤")
                if 'date' in merged_data.columns:
                    all_dates = sorted(merged_data['date'].unique().tolist())
                    excluded_dates = st.multiselect("é€‰æ‹©è¦å‰”é™¤çš„æ—¥æœŸ", options=all_dates)
                else:
                    st.info("æ•°æ®ä¸­æ— æ—¥æœŸå­—æ®µ")
                    excluded_dates = []

            # è®¡ç®—å‰”é™¤ç»“æœ
            try:
                exclusion_mask = pd.Series([True] * len(merged_data), index=merged_data.index)

                if excluded_sources:
                    source_mask = merged_data['æ•°æ®æ¥æº'].isin(excluded_sources)
                    exclusion_mask &= source_mask

                if 'date' in merged_data.columns and excluded_dates:
                    date_mask = merged_data['date'].isin(excluded_dates)
                    exclusion_mask &= date_mask

                if not excluded_sources and not excluded_dates:
                    exclusion_mask = pd.Series([False] * len(merged_data), index=merged_data.index)

                to_exclude = merged_data[exclusion_mask]
                to_keep = merged_data[~exclusion_mask]

            except Exception as e:
                st.error(f"è®¡ç®—å‰”é™¤æ¡ä»¶æ—¶å‡ºé”™: {str(e)}")
                to_exclude = pd.DataFrame()
                to_keep = merged_data.copy()

            col1, col2 = st.columns(2)
            with col1:
                st.markdown(f"### å°†è¢«å‰”é™¤çš„æ•°æ® ({len(to_exclude)} æ¡)")
                if len(to_exclude) > 0:
                    preview_exclude = optimize_dataframe_for_preview(to_exclude, max_rows=5)
                    st.dataframe(preview_exclude, use_container_width=True)

            with col2:
                st.markdown(f"### ä¿ç•™çš„æ•°æ® ({len(to_keep)} æ¡)")
                if len(to_keep) > 0:
                    preview_keep = optimize_dataframe_for_preview(to_keep, max_rows=5)
                    st.dataframe(preview_keep, use_container_width=True)

            if st.button("ç¡®è®¤å‰”é™¤å¼‚å¸¸æ•°æ®", type="primary", use_container_width=True):
                try:
                    if len(to_exclude) > 0:
                        excluded_dates_info = []
                        for _, row in to_exclude.iterrows():
                            source = row.get('æ•°æ®æ¥æº', 'Unknown')
                            date = row.get('date', 'Unknown')
                            excluded_dates_info.append(f"{source}-{date}")
                        
                        st.session_state.excluded_data = excluded_dates_info
                        st.session_state.excluded_dates_info = excluded_dates
                        st.session_state.cleaned_data = to_keep.copy()
                        st.success(f"æˆåŠŸå‰”é™¤ {len(to_exclude)} æ¡å¼‚å¸¸æ•°æ®")
                    else:
                        st.session_state.cleaned_data = merged_data.copy()
                        st.session_state.excluded_dates_info = []
                        st.info("æœªå‘ç°éœ€è¦å‰”é™¤çš„å¼‚å¸¸æ•°æ®")
                except Exception as e:
                    st.error(f"å‰”é™¤æ•°æ®æ—¶å‡ºé”™: {str(e)}")

        st.markdown('</div>', unsafe_allow_html=True)

    # æ­¥éª¤3ï¼šç•™å­˜ç‡è®¡ç®—
    if st.session_state.merged_data is not None:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("3. ç•™å­˜ç‡è®¡ç®—")
        
        # ä¿®æ”¹è®¡ç®—æ–¹æ³•è¯´æ˜
        st.markdown("""
        <div class="step-tip">
            <div class="step-tip-title">ğŸ“‹ ç•™å­˜ç‡è®¡ç®—æ–¹æ³•</div>
            <div class="step-tip-content">
            <strong>æ–°è®¡ç®—æ–¹æ³•ï¼š</strong>ä»¥å¹³å‡æ–°å¢ç”¨æˆ·æ•°ä½œä¸ºåŸºæ•°<br>
            â€¢ è®¡ç®—æ¯ä¸ªæ¸ é“çš„å¹³å‡æ–°å¢ç”¨æˆ·æ•°<br>
            â€¢ è®¡ç®—å„å¤©çš„å¹³å‡ç•™å­˜æ•°<br>
            â€¢ ç•™å­˜ç‡ = å„å¤©å¹³å‡ç•™å­˜æ•° Ã· å¹³å‡æ–°å¢ç”¨æˆ·æ•°
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        # ç¡®å®šä½¿ç”¨çš„æ•°æ®
        if st.session_state.cleaned_data is not None:
            working_data = st.session_state.cleaned_data
            st.info("ä½¿ç”¨æ¸…ç†åçš„æ•°æ®è¿›è¡Œè®¡ç®—")
        else:
            working_data = st.session_state.merged_data
            st.info("ä½¿ç”¨åŸå§‹æ•°æ®è¿›è¡Œè®¡ç®—")

        data_sources = working_data['æ•°æ®æ¥æº'].unique()
        selected_sources = st.multiselect("é€‰æ‹©è¦åˆ†æçš„æ•°æ®æ¥æº", options=data_sources, default=data_sources)

        if st.button("è®¡ç®—ç•™å­˜ç‡", type="primary", use_container_width=True):
            if selected_sources:
                with st.spinner("æ­£åœ¨è®¡ç®—ç•™å­˜ç‡..."):
                    filtered_data = working_data[working_data['æ•°æ®æ¥æº'].isin(selected_sources)]
                    retention_results = calculate_retention_rates_new_method(filtered_data)
                    st.session_state.retention_data = retention_results

                    st.success("ç•™å­˜ç‡è®¡ç®—å®Œæˆï¼")

                    # åˆ›å»ºç•™å­˜ç‡è¡¨æ ¼æ˜¾ç¤º
                    if retention_results:
                        st.subheader("ç•™å­˜ç‡è¯¦ç»†æ•°æ®")
                        
                        # åˆ›å»ºè¡¨æ ¼æ•°æ®
                        table_data = []
                        days_range = range(1, 31)
                        
                        for day in days_range:
                            row = {'å¤©æ•°': day}
                            for result in retention_results:
                                channel_name = result['data_source']
                                days = result['days']
                                rates = result['rates']
                                
                                # æŸ¥æ‰¾å¯¹åº”å¤©æ•°çš„ç•™å­˜ç‡
                                day_index = np.where(days == day)[0]
                                if len(day_index) > 0:
                                    rate = rates[day_index[0]]
                                    row[channel_name] = f"{rate:.4f}"
                                else:
                                    row[channel_name] = "-"
                            table_data.append(row)
                        
                        # åˆ›å»ºDataFrameå¹¶æ˜¾ç¤º
                        retention_table_df = pd.DataFrame(table_data)
                        
                        # ä½¿ç”¨expanderå±•å¼€è¡¨æ ¼ï¼Œé™åˆ¶é«˜åº¦å¹¶æ˜¾ç¤ºæ»šåŠ¨æ¡
                        with st.expander("ç•™å­˜ç‡æ•°æ®è¡¨ï¼ˆ1-30å¤©ï¼‰", expanded=True):
                            st.dataframe(
                                retention_table_df, 
                                use_container_width=True,
                                height=400  # è®¾ç½®å›ºå®šé«˜åº¦ï¼Œçº¦10è¡Œçš„é«˜åº¦
                            )
            else:
                st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ•°æ®æ¥æº")

        st.markdown('</div>', unsafe_allow_html=True)

    # æ­¥éª¤4ï¼šLTæ‹Ÿåˆåˆ†æ
    if st.session_state.retention_data is not None:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("4. LTæ‹Ÿåˆåˆ†æ")
        
        retention_data = st.session_state.retention_data

        # æ¸ é“è§„åˆ™è¯´æ˜
        st.markdown("""
        <div class="step-tip">
            <div class="step-tip-title">ğŸ“‹ ä¸‰é˜¶æ®µæ‹Ÿåˆè§„åˆ™</div>
            <div class="step-tip-content">
            <strong>ç¬¬ä¸€é˜¶æ®µï¼š</strong>1-30å¤©ï¼Œå¹‚å‡½æ•°æ‹Ÿåˆå®é™…æ•°æ®<br>
            <strong>ç¬¬äºŒé˜¶æ®µï¼š</strong>31-Xå¤©ï¼Œå»¶ç»­å¹‚å‡½æ•°æ¨¡å‹<br>
            <strong>ç¬¬ä¸‰é˜¶æ®µï¼š</strong>Yå¤©åï¼ŒæŒ‡æ•°å‡½æ•°å»ºæ¨¡é•¿æœŸè¡°å‡<br>
            ä¸åŒæ¸ é“é‡‡ç”¨ä¸åŒçš„é˜¶æ®µåˆ’åˆ†ç‚¹
            </div>
        </div>
        """, unsafe_allow_html=True)

        if st.button("å¼€å§‹LTæ‹Ÿåˆåˆ†æ", type="primary", use_container_width=True):
            with st.spinner("æ­£åœ¨è¿›è¡Œæ‹Ÿåˆè®¡ç®—..."):
                lt_results_2y = []
                lt_results_5y = []
                visualization_data_2y = {}
                visualization_data_5y = {}
                original_data = {}
                
                key_days = [1, 7, 30, 60, 90, 100, 150, 200, 300]

                for retention_result in retention_data:
                    channel_name = retention_result['data_source']
                    
                    # è®¡ç®—2å¹´LT
                    lt_result_2y = calculate_lt_advanced(retention_result, channel_name, 2, 
                                                       return_curve_data=True, key_days=key_days)
                    
                    # è®¡ç®—5å¹´LT
                    lt_result_5y = calculate_lt_advanced(retention_result, channel_name, 5, 
                                                       return_curve_data=True, key_days=key_days)

                    lt_results_2y.append({
                        'data_source': channel_name,
                        'lt_value': lt_result_2y['lt_value'],
                        'fit_success': lt_result_2y['success'],
                        'fit_params': lt_result_2y['fit_params'],
                        'power_r2': lt_result_2y['power_r2'],
                        'model_used': lt_result_2y['model_used']
                    })
                    
                    lt_results_5y.append({
                        'data_source': channel_name,
                        'lt_value': lt_result_5y['lt_value'],
                        'fit_success': lt_result_5y['success'],
                        'fit_params': lt_result_5y['fit_params'],
                        'power_r2': lt_result_5y['power_r2'],
                        'model_used': lt_result_5y['model_used']
                    })

                    # ä¿å­˜å¯è§†åŒ–æ•°æ®
                    visualization_data_2y[channel_name] = {
                        "days": lt_result_2y['curve_days'],
                        "rates": lt_result_2y['curve_rates'],
                        "lt": lt_result_2y['lt_value']
                    }
                    
                    visualization_data_5y[channel_name] = {
                        "days": lt_result_5y['curve_days'],
                        "rates": lt_result_5y['curve_rates'],
                        "lt": lt_result_5y['lt_value']
                    }

                    # ä¿å­˜åŸå§‹æ•°æ®
                    original_data[channel_name] = {
                        "days": retention_result['days'],
                        "rates": retention_result['rates']
                    }

                st.session_state.lt_results_2y = lt_results_2y
                st.session_state.lt_results_5y = lt_results_5y
                st.session_state.visualization_data_5y = visualization_data_5y
                st.session_state.original_data = original_data
                st.success("LTæ‹Ÿåˆåˆ†æå®Œæˆï¼")

                # æ˜¾ç¤ºLTå€¼è¡¨æ ¼
                if lt_results_5y:
                    st.subheader("LTåˆ†æç»“æœ")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("#### 2å¹´LTç»“æœ")
                        results_2y_df = pd.DataFrame([
                            {
                                'æ¸ é“åç§°': r['data_source'],
                                '2å¹´LT': round(r['lt_value'], 2),
                                'RÂ²å¾—åˆ†': round(r['power_r2'], 3)
                            }
                            for r in lt_results_2y
                        ])
                        st.dataframe(results_2y_df, use_container_width=True)
                    
                    with col2:
                        st.markdown("#### 5å¹´LTç»“æœ")
                        results_5y_df = pd.DataFrame([
                            {
                                'æ¸ é“åç§°': r['data_source'],
                                '5å¹´LT': round(r['lt_value'], 2),
                                'RÂ²å¾—åˆ†': round(r['power_r2'], 3)
                            }
                            for r in lt_results_5y
                        ])
                        st.dataframe(results_5y_df, use_container_width=True)

                # æ˜¾ç¤ºå•æ¸ é“å›¾è¡¨ - 100å¤©ç‰ˆæœ¬
                if visualization_data_5y and original_data:
                    st.subheader("å„æ¸ é“100å¤©LTæ‹Ÿåˆå›¾è¡¨")
                    
                    # æŒ‰LTå€¼æ’åº
                    sorted_channels = sorted(visualization_data_5y.items(), key=lambda x: x[1]['lt'])
                    
                    # æ¯è¡Œæ˜¾ç¤º2ä¸ªå›¾è¡¨
                    for i in range(0, len(sorted_channels), 2):
                        cols = st.columns(2)
                        for j, col in enumerate(cols):
                            if i + j < len(sorted_channels):
                                channel_name, curve_data = sorted_channels[i + j]
                                with col:
                                    fig = create_individual_channel_chart(
                                        channel_name, curve_data, original_data, max_days=100
                                    )
                                    st.pyplot(fig, use_container_width=True)
                                    plt.close(fig)

        st.markdown('</div>', unsafe_allow_html=True)

elif current_page == "ARPUè®¡ç®—":
    # åŸç†è§£é‡Š
    st.markdown("""
    <div class="principle-box">
        <div class="principle-title">ğŸ“š ARPUè®¡ç®—åŸç†</div>
        <div class="principle-content">
        ARPUï¼ˆAverage Revenue Per Userï¼‰è®¡ç®—åŸºäºç”¨æˆ·æ–°å¢æ•°å’Œæ”¶å…¥æ•°æ®ã€‚ç³»ç»Ÿæ”¯æŒExcelæ–‡ä»¶ä¸Šä¼ ï¼Œæ ¹æ®PIDè¿›è¡Œæ¸ é“åŒ¹é…ï¼Œ
        æŒ‰æœˆä»½ç­›é€‰æ•°æ®ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼šARPU = æ€»æ”¶å…¥ Ã· æ€»æ–°å¢ç”¨æˆ·æ•°ã€‚
        </div>
    </div>
    """, unsafe_allow_html=True)

    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.subheader("ARPUæ•°æ®å¤„ç†")

    # ARPUæ–‡ä»¶æ ¼å¼è¯´æ˜ - æ›´æ–°æ ¼å¼
    st.markdown("""
    <div class="step-tip">
        <div class="step-tip-title">ğŸ“‹ ARPUæ–‡ä»¶æ ¼å¼è¦æ±‚</div>
        <div class="step-tip-content">
        â€¢ Excelæ ¼å¼(.xlsx/.xls)<br>
        â€¢ å¿…é¡»åŒ…å«ä»¥ä¸‹åˆ—ï¼š<br>
        &nbsp;&nbsp;- <strong>æœˆä»½</strong>ï¼šæœˆä»½ä¿¡æ¯<br>
        &nbsp;&nbsp;- <strong>pid</strong>ï¼šæ¸ é“å·<br>
        &nbsp;&nbsp;- <strong>stat_date</strong>ï¼šç»Ÿè®¡æ—¥æœŸ<br>
        &nbsp;&nbsp;- <strong>instl_user_cnt</strong>ï¼šæ–°å¢ç”¨æˆ·æ•°<br>
        &nbsp;&nbsp;- <strong>ad_all_rven_1d_m</strong>ï¼šæ”¶å…¥æ•°æ®<br>
        â€¢ æ”¯æŒæŒ‰æœˆä»½ç­›é€‰æ•°æ®
        </div>
    </div>
    """, unsafe_allow_html=True)

    arpu_file = st.file_uploader("é€‰æ‹©ARPUæ•°æ®æ–‡ä»¶ (Excelæ ¼å¼)", type=['xlsx', 'xls'])

    if arpu_file:
        try:
            with st.spinner("æ­£åœ¨è¯»å–ARPUæ–‡ä»¶..."):
                arpu_df = pd.read_excel(arpu_file)
            st.success("ARPUæ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
            
            # æ£€æŸ¥å¿…éœ€åˆ—
            required_cols = ['pid', 'instl_user_cnt', 'ad_all_rven_1d_m']
            missing_cols = [col for col in required_cols if col not in arpu_df.columns]
            
            if missing_cols:
                st.error(f"æ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {', '.join(missing_cols)}")
                st.info("å¯ç”¨åˆ—: " + ", ".join(arpu_df.columns.tolist()))
            else:
                # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                preview_arpu = optimize_dataframe_for_preview(arpu_df, max_rows=10)
                st.dataframe(preview_arpu, use_container_width=True)
                
                # æœˆä»½ç­›é€‰ - ä¼˜å…ˆä½¿ç”¨æœˆä»½åˆ—ï¼Œå…¶æ¬¡ä½¿ç”¨stat_dateåˆ—
                st.subheader("æœˆä»½ç­›é€‰")
                
                if 'æœˆä»½' in arpu_df.columns:
                    # ä½¿ç”¨æœˆä»½åˆ—
                    available_months = sorted(arpu_df['æœˆä»½'].dropna().unique())
                    available_months = [str(m) for m in available_months]
                    
                    if available_months:
                        col1, col2 = st.columns(2)
                        with col1:
                            start_month = st.selectbox("å¼€å§‹æœˆä»½", options=available_months)
                        with col2:
                            end_month = st.selectbox("ç»“æŸæœˆä»½", options=available_months, 
                                                   index=len(available_months)-1)
                    else:
                        st.warning("æœˆä»½åˆ—æ— æœ‰æ•ˆæ•°æ®ï¼Œå°†ä½¿ç”¨æ‰€æœ‰æ•°æ®")
                        start_month = end_month = None
                        
                elif 'stat_date' in arpu_df.columns:
                    # ä½¿ç”¨stat_dateåˆ—
                    arpu_df['stat_date'] = pd.to_datetime(arpu_df['stat_date'], errors='coerce')
                    arpu_df['month'] = arpu_df['stat_date'].dt.to_period('M')
                    available_months = arpu_df['month'].dropna().unique()
                    available_months = sorted([str(m) for m in available_months])
                    
                    if available_months:
                        col1, col2 = st.columns(2)
                        with col1:
                            start_month = st.selectbox("å¼€å§‹æœˆä»½", options=available_months)
                        with col2:
                            end_month = st.selectbox("ç»“æŸæœˆä»½", options=available_months, 
                                                   index=len(available_months)-1)
                    else:
                        st.warning("æ— æ³•è§£æstat_dateæ•°æ®ï¼Œå°†ä½¿ç”¨æ‰€æœ‰æ•°æ®")
                        start_month = end_month = None
                else:
                    st.info("æœªæ‰¾åˆ°æœˆä»½æˆ–stat_dateåˆ—ï¼Œå°†ä½¿ç”¨æ‰€æœ‰æ•°æ®")
                    start_month = end_month = None

                if st.button("è®¡ç®—ARPU", type="primary", use_container_width=True):
                    with st.spinner("æ­£åœ¨è®¡ç®—ARPU..."):
                        try:
                            # æœˆä»½ç­›é€‰
                            if start_month and end_month:
                                if 'æœˆä»½' in arpu_df.columns:
                                    mask = (arpu_df['æœˆä»½'] >= start_month) & (arpu_df['æœˆä»½'] <= end_month)
                                else:
                                    mask = (arpu_df['month'] >= start_month) & (arpu_df['month'] <= end_month)
                                filtered_arpu_df = arpu_df[mask].copy()
                                st.info(f"ç­›é€‰æœˆä»½: {start_month} è‡³ {end_month}")
                            else:
                                filtered_arpu_df = arpu_df.copy()
                                st.info("ä½¿ç”¨å…¨éƒ¨æ•°æ®")

                            # ç¡®ä¿pidä¸ºå­—ç¬¦ä¸²æ ¼å¼
                            filtered_arpu_df['pid'] = filtered_arpu_df['pid'].astype(str).str.replace('.0', '', regex=False)
                            
                            # åˆ›å»ºåå‘æ¸ é“æ˜ å°„
                            reverse_mapping = create_reverse_mapping(st.session_state.channel_mapping)
                            
                            # æŒ‰æ¸ é“åŒ¹é…å’Œæ±‡æ€»
                            arpu_results = []
                            
                            for pid, group in filtered_arpu_df.groupby('pid'):
                                if pid in reverse_mapping:
                                    channel_name = reverse_mapping[pid]
                                    total_users = group['instl_user_cnt'].sum()
                                    total_revenue = group['ad_all_rven_1d_m'].sum()
                                    
                                    if total_users > 0:
                                        arpu_value = total_revenue / total_users
                                        arpu_results.append({
                                            'data_source': channel_name,
                                            'total_users': total_users,
                                            'total_revenue': total_revenue,
                                            'arpu_value': arpu_value,
                                            'record_count': len(group)
                                        })

                            if arpu_results:
                                # æŒ‰æ¸ é“åˆå¹¶ç›¸åŒæ¸ é“çš„æ•°æ®
                                final_arpu = {}
                                for result in arpu_results:
                                    channel = result['data_source']
                                    if channel in final_arpu:
                                        final_arpu[channel]['total_users'] += result['total_users']
                                        final_arpu[channel]['total_revenue'] += result['total_revenue']
                                        final_arpu[channel]['record_count'] += result['record_count']
                                    else:
                                        final_arpu[channel] = result.copy()
                                
                                # é‡æ–°è®¡ç®—ARPU
                                arpu_summary = []
                                for channel, data in final_arpu.items():
                                    arpu_value = data['total_revenue'] / data['total_users'] if data['total_users'] > 0 else 0
                                    arpu_summary.append({
                                        'data_source': channel,
                                        'arpu_value': arpu_value,
                                        'record_count': data['record_count']
                                    })
                                
                                arpu_summary_df = pd.DataFrame(arpu_summary)
                                st.session_state.arpu_data = arpu_summary_df
                                st.success("ARPUè®¡ç®—å®Œæˆï¼")
                                
                                # æ˜¾ç¤ºç»“æœ
                                display_arpu_df = arpu_summary_df.copy()
                                display_arpu_df['ARPU'] = display_arpu_df['arpu_value'].round(4)
                                display_arpu_df = display_arpu_df[['data_source', 'ARPU', 'record_count']]
                                display_arpu_df.columns = ['æ¸ é“åç§°', 'ARPUå€¼', 'è®°å½•æ•°']
                                st.dataframe(display_arpu_df, use_container_width=True)
                            else:
                                st.error("æœªæ‰¾åˆ°åŒ¹é…çš„æ¸ é“æ•°æ®")

                        except Exception as e:
                            st.error(f"ARPUè®¡ç®—å¤±è´¥ï¼š{str(e)}")

        except Exception as e:
            st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")

    else:
        st.info("è¯·ä¸Šä¼ ARPUæ•°æ®æ–‡ä»¶")

    # æ‰‹åŠ¨è®¾ç½®ARPUï¼ˆæŒ‰éœ€æ˜¾ç¤ºï¼‰
    if st.session_state.lt_results_5y:
        if not st.session_state.show_manual_arpu:
            if st.button("æ‰‹åŠ¨è®¾ç½®ARPUå€¼"):
                st.session_state.show_manual_arpu = True
                st.rerun()
        else:
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            st.subheader("æ‰‹åŠ¨è®¾ç½®ARPUå€¼")
            
            arpu_inputs = {}
            
            col1, col2 = st.columns(2)
            for i, result in enumerate(st.session_state.lt_results_5y):
                source = result['data_source']
                with col1 if i % 2 == 0 else col2:
                    arpu_value = st.number_input(
                        f"{source}", min_value=0.0, value=0.04, step=0.001,
                        format="%.4f", key=f"manual_arpu_{source}"
                    )
                    arpu_inputs[source] = arpu_value

            if st.button("ä¿å­˜æ‰‹åŠ¨ARPUè®¾ç½®", type="primary", use_container_width=True):
                arpu_df = pd.DataFrame([
                    {'data_source': source, 'arpu_value': value, 'record_count': 1}
                    for source, value in arpu_inputs.items()
                ])
                st.session_state.arpu_data = arpu_df
                st.success("ARPUè®¾ç½®å·²ä¿å­˜ï¼")
                st.dataframe(arpu_df[['data_source', 'arpu_value']], use_container_width=True)
            
            st.markdown('</div>', unsafe_allow_html=True)

    st.markdown('</div>', unsafe_allow_html=True)

elif current_page == "LTVç»“æœæŠ¥å‘Š":
    # åŸç†è§£é‡Š
    st.markdown("""
    <div class="principle-box">
        <div class="principle-title">ğŸ“š LTVç»“æœæŠ¥å‘Š</div>
        <div class="principle-content">
        LTVç»“æœæŠ¥å‘Šæ•´åˆäº†LTæ‹Ÿåˆåˆ†æå’ŒARPUè®¡ç®—çš„ç»“æœï¼Œé€šè¿‡LTV = LT Ã— ARPUå…¬å¼è®¡ç®—æœ€ç»ˆçš„ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼ã€‚
        æŠ¥å‘Šæä¾›2å¹´å’Œ5å¹´åŒæ®µå¯¹æ¯”ï¼ŒåŒ…å«è¯¦ç»†çš„æ‹Ÿåˆæ¨¡å‹ä¿¡æ¯å’Œè´¨é‡è¯„ä¼°ã€‚
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    if st.session_state.lt_results_5y is not None and st.session_state.arpu_data is not None:
        lt_results_2y = st.session_state.lt_results_2y
        lt_results_5y = st.session_state.lt_results_5y
        arpu_data = st.session_state.arpu_data

        # è®¡ç®—LTVç»“æœ
        ltv_results = []
        
        for lt_result_5y in lt_results_5y:
            source = lt_result_5y['data_source']
            
            # æŸ¥æ‰¾å¯¹åº”çš„2å¹´LTæ•°æ®
            lt_result_2y = next((r for r in lt_results_2y if r['data_source'] == source), None)
            
            # æŸ¥æ‰¾ARPUæ•°æ®
            arpu_row = arpu_data[arpu_data['data_source'] == source]
            if not arpu_row.empty:
                arpu_value = arpu_row.iloc[0]['arpu_value']
            else:
                arpu_value = 0
                st.warning(f"æ¸ é“ '{source}' æœªæ‰¾åˆ°ARPUæ•°æ®")

            ltv_5y = lt_result_5y['lt_value'] * arpu_value
            ltv_2y = lt_result_2y['lt_value'] * arpu_value if lt_result_2y else 0

            ltv_results.append({
                'data_source': source,
                'lt_2y': lt_result_2y['lt_value'] if lt_result_2y else 0,
                'lt_5y': lt_result_5y['lt_value'],
                'arpu_value': arpu_value,
                'ltv_2y': ltv_2y,
                'ltv_5y': ltv_5y,
                'fit_success': lt_result_5y['fit_success'],
                'model_used': lt_result_5y.get('model_used', 'unknown'),
                'power_params': lt_result_5y.get('fit_params', {}).get('power', {}),
                'exp_params': lt_result_5y.get('fit_params', {}).get('exponential', {})
            })

        st.session_state.ltv_results = ltv_results

        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("LTVç»¼åˆè®¡ç®—ç»“æœ")

        # åˆ›å»ºå®Œæ•´çš„ç»“æœè¡¨æ ¼
        display_data = []
        for result in ltv_results:
            # è·å–æ‹Ÿåˆå‚æ•°ä¿¡æ¯ - ä¿®æ”¹å¤‡æ³¨æ¢è¡Œ
            power_params = result['power_params']
            exp_params = result['exp_params']
            
            å¤‡æ³¨_parts = []
            if power_params:
                power_func = f"å¹‚å‡½æ•°: y = {power_params.get('a', 0):.4f} * x^{power_params.get('b', 0):.4f}"
                å¤‡æ³¨_parts.append(power_func)
            
            if exp_params:
                exp_func = f"æŒ‡æ•°å‡½æ•°: y = {exp_params.get('c', 0):.4f} * exp({exp_params.get('d', 0):.4f} * x)"
                å¤‡æ³¨_parts.append(exp_func)
            
            å¤‡æ³¨ = "\n".join(å¤‡æ³¨_parts) if å¤‡æ³¨_parts else "æœªçŸ¥"
            
            display_data.append({
                'æ¸ é“åç§°': result['data_source'],
                '5å¹´LT': round(result['lt_5y'], 2),
                '5å¹´ARPU': round(result['arpu_value'], 4),
                '5å¹´LTV': round(result['ltv_5y'], 2),
                '2å¹´LT': round(result['lt_2y'], 2),
                '2å¹´ARPU': round(result['arpu_value'], 4),
                '2å¹´LTV': round(result['ltv_2y'], 2),
                'å¤‡æ³¨': å¤‡æ³¨
            })

        results_df = pd.DataFrame(display_data)
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        column_order = ['æ¸ é“åç§°', '5å¹´LT', '5å¹´ARPU', '5å¹´LTV', '2å¹´LT', '2å¹´ARPU', '2å¹´LTV', 'å¤‡æ³¨']
        results_df = results_df[column_order]
        
        st.dataframe(results_df, use_container_width=True, height=600)
        st.markdown('</div>', unsafe_allow_html=True)

        # æ˜¾ç¤ºæ‰€æœ‰æ‹Ÿåˆæ›²çº¿ - ä¸€è¡Œå››ä¸ª
        if st.session_state.visualization_data_5y and st.session_state.original_data:
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            st.subheader("æ‰€æœ‰æ¸ é“æ‹Ÿåˆæ›²çº¿ï¼ˆ100å¤©ï¼‰")
            
            visualization_data_5y = st.session_state.visualization_data_5y
            original_data = st.session_state.original_data
            
            # æŒ‰LTå€¼æ’åº
            sorted_channels = sorted(visualization_data_5y.items(), key=lambda x: x[1]['lt'])
            
            # æ¯è¡Œæ˜¾ç¤º4ä¸ªå›¾è¡¨
            for i in range(0, len(sorted_channels), 4):
                cols = st.columns(4)
                for j, col in enumerate(cols):
                    if i + j < len(sorted_channels):
                        channel_name, curve_data = sorted_channels[i + j]
                        with col:
                            fig = create_individual_channel_chart(
                                channel_name, curve_data, original_data, max_days=100
                            )
                            st.pyplot(fig, use_container_width=True)
                            plt.close(fig)
            
            st.markdown('</div>', unsafe_allow_html=True)

        # æ•°æ®å¯¼å‡º
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("åˆ†æç»“æœå¯¼å‡º")

        col1, col2 = st.columns(2)

        with col1:
            # CSVå¯¼å‡º
            csv_data = results_df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ä¸‹è½½LTVåˆ†æç»“æœ (CSV)",
                data=csv_data.encode('utf-8-sig'),
                file_name=f"LTV_Analysis_Results_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                mime="text/csv",
                use_container_width=True
            )

        with col2:
            # è¯¦ç»†æŠ¥å‘Š
            data_source_desc = ""
            if st.session_state.excluded_dates_info and len(st.session_state.excluded_dates_info) > 0:
                excluded_dates_str = ", ".join(st.session_state.excluded_dates_info)
                data_source_desc = f"å·²å‰”é™¤ä»¥ä¸‹æ—¥æœŸæ•°æ®ï¼š{excluded_dates_str}"
            elif st.session_state.cleaned_data is not None:
                data_source_desc = "ä½¿ç”¨æ¸…ç†åæ•°æ®"
            else:
                data_source_desc = "ä½¿ç”¨åŸå§‹æ•°æ®"
            
            report_text = f"""
LTVç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†ææŠ¥å‘Š
===========================================
ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

æ‰§è¡Œæ‘˜è¦
-----------
æœ¬æŠ¥å‘ŠåŸºäºä¸‰é˜¶æ®µåˆ†å±‚æ•°å­¦å»ºæ¨¡æ–¹æ³•ï¼Œå¯¹ {len(results_df)} ä¸ªæ¸ é“è¿›è¡Œäº†ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†æã€‚

æ ¸å¿ƒæŒ‡æ ‡æ±‡æ€»
-----------
â€¢ å‚ä¸åˆ†æçš„æ¸ é“æ•°é‡: {len(results_df)}
â€¢ 5å¹´å¹³å‡LTV: {results_df['5å¹´LTV'].mean():.2f}
â€¢ 5å¹´æœ€é«˜LTV: {results_df['5å¹´LTV'].max():.2f}
â€¢ 5å¹´æœ€ä½LTV: {results_df['5å¹´LTV'].min():.2f}
â€¢ 2å¹´å¹³å‡LTV: {results_df['2å¹´LTV'].mean():.2f}
â€¢ å¹³å‡5å¹´LTå€¼: {results_df['5å¹´LT'].mean():.2f} å¤©
â€¢ å¹³å‡2å¹´LTå€¼: {results_df['2å¹´LT'].mean():.2f} å¤©
â€¢ å¹³å‡ARPU: {results_df['5å¹´ARPU'].mean():.4f}

è¯¦ç»†ç»“æœ
-----------
{results_df[['æ¸ é“åç§°', '5å¹´LT', '5å¹´ARPU', '5å¹´LTV', '2å¹´LT', '2å¹´ARPU', '2å¹´LTV']].to_string(index=False)}

æ•°æ®æ¥æºè¯´æ˜
-----------
{data_source_desc}

è®¡ç®—æ–¹æ³•
-----------
â€¢ LTæ‹Ÿåˆ: ä¸‰é˜¶æ®µåˆ†å±‚å»ºæ¨¡ï¼ˆ1-30å¤©å¹‚å‡½æ•° + 31-Xå¤©å¹‚å‡½æ•°å»¶ç»­ + Yå¤©åæŒ‡æ•°å‡½æ•°ï¼‰
â€¢ LTVå…¬å¼: LTV = LT Ã— ARPU
â€¢ æ¸ é“è§„åˆ™: æŒ‰åä¸ºã€å°ç±³ã€OPPOã€vivoã€iPhoneåˆ†ç±»è®¾å®šä¸åŒæ‹Ÿåˆå‚æ•°
â€¢ ç•™å­˜ç‡è®¡ç®—: æŒ‰æ¸ é“å¹³å‡æ–°å¢ç”¨æˆ·æ•°ä½œä¸ºåŸºæ•°ï¼Œå„å¤©ç•™å­˜æ•°Ã·å¹³å‡æ–°å¢æ•°

æŠ¥å‘Šç”Ÿæˆ: LTVæ™ºèƒ½åˆ†æå¹³å° v3.2
"""

            st.download_button(
                label="ä¸‹è½½è¯¦ç»†åˆ†ææŠ¥å‘Š (TXT)",
                data=report_text.encode('utf-8'),
                file_name=f"LTV_Detailed_Report_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.txt",
                mime="text/plain",
                use_container_width=True
            )

        st.markdown('</div>', unsafe_allow_html=True)
    else:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        missing_components = []
        if st.session_state.lt_results_5y is None:
            missing_components.append("LTæ‹Ÿåˆåˆ†æ")
        if st.session_state.arpu_data is None:
            missing_components.append("ARPUè®¡ç®—")
        
        st.info(f"è¯·å…ˆå®Œæˆï¼š{', '.join(missing_components)}")
        st.markdown('</div>', unsafe_allow_html=True)

# ==================== åº•éƒ¨ä¿¡æ¯ ====================
with st.sidebar:
    st.markdown("---")
    st.markdown("""
    <div class="nav-container">
        <h4 style="text-align: center; color: white;">ä½¿ç”¨æŒ‡å—</h4>
        <p style="font-size: 0.9rem; color: rgba(255,255,255,0.9); text-align: center;">
        æŒ‰æ­¥éª¤å®Œæˆåˆ†ææµç¨‹ï¼Œæ¯æ­¥éƒ½æœ‰è¯¦ç»†æŒ‡å¯¼ã€‚
        </p>
        <p style="font-size: 0.8rem; color: rgba(255,255,255,0.7); text-align: center;">
        LTVæ™ºèƒ½åˆ†æå¹³å° v3.2<br>
        åŸºäºä¸‰é˜¶æ®µæ•°å­¦å»ºæ¨¡
        </p>
    </div>
    """, unsafe_allow_html=True)
