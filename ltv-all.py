import streamlit as st
import os
import pandas as pd
import numpy as np
from pathlib import Path
import warnings
import datetime
import tempfile
import zipfile
import io
import matplotlib.pyplot as plt
import matplotlib as mpl
import re
from matplotlib.font_manager import FontProperties
import seaborn as sns
from scipy.optimize import curve_fit

# ==================== åŸºç¡€é…ç½® ====================
# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl.styles.stylesheet')
warnings.filterwarnings('ignore', category=UserWarning,
                        message="Could not infer format, so each element will be parsed individually")

# è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ - ç®€åŒ–ç‰ˆæœ¬
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“ - ç®€åŒ–ç‰ˆæœ¬é¿å…é”™è¯¯"""
    try:
        # ä½¿ç”¨ç®€å•çš„å­—ä½“è®¾ç½®
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        plt.rcParams['font.size'] = 10
        return True
    except Exception as e:
        st.warning(f"å­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“: {e}")
        return False

# åˆå§‹åŒ–å­—ä½“è®¾ç½®
setup_chinese_font()

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="LTV Analytics Platform",
    layout="wide",
    initial_sidebar_state="expanded",
    page_icon="ğŸ“Š"
)

# ==================== CSS æ ·å¼å®šä¹‰ ====================
st.markdown("""
<style>
    /* å…¨å±€æ ·å¼ */
    .main {
        background: #f8fafc;
        min-height: 100vh;
    }

    .block-container {
        padding: 1rem 1rem 3rem 1rem;
        max-width: 100%;
        background: transparent;
    }

    /* ä¸»æ ‡é¢˜åŒºåŸŸ */
    .main-header {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
        padding: 1.5rem;
        border-radius: 12px;
        box-shadow: 0 4px 20px rgba(30, 64, 175, 0.3);
        margin-bottom: 1.5rem;
        text-align: center;
        color: white;
    }

    .main-title {
        font-size: 2rem;
        font-weight: 700;
        color: white;
        margin-bottom: 0.5rem;
        text-shadow: 0 2px 4px rgba(0,0,0,0.3);
    }

    .main-subtitle {
        color: rgba(255,255,255,0.9);
        font-size: 1.2rem;
        font-weight: 400;
    }

    /* å¡ç‰‡æ ·å¼ */
    .glass-card {
        background: rgba(255, 255, 255, 0.95);
        border-radius: 12px;
        padding: 1.5rem;
        box-shadow: 0 8px 32px rgba(30, 64, 175, 0.1);
        border: 1px solid rgba(59, 130, 246, 0.2);
        margin-bottom: 1.5rem;
        backdrop-filter: blur(10px);
    }

    /* å¯¼èˆªæ­¥éª¤æ ·å¼ */
    .nav-container {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
        border-radius: 12px;
        padding: 1.5rem;
        margin-bottom: 1rem;
        color: white;
        box-shadow: 0 4px 20px rgba(30, 64, 175, 0.3);
    }

    /* æŒ‰é’®æ ·å¼ - ä¿®æ”¹æ‚¬åœæ•ˆæœä¸ºæ·±è“è‰² */
    .stButton > button {
        background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.6rem 2rem;
        font-weight: 600;
        transition: all 0.3s ease;
        box-shadow: 0 4px 15px rgba(30, 64, 175, 0.3);
    }

    .stButton > button:hover {
        background: linear-gradient(135deg, #1d4ed8 0%, #2563eb 100%);
        transform: translateY(-2px);
        box-shadow: 0 6px 20px rgba(29, 78, 216, 0.4);
    }

    /* å­æ­¥éª¤æ ·å¼ */
    .sub-steps {
        font-size: 0.8rem;
        color: rgba(255,255,255,0.7);
        margin-top: 0.3rem;
        line-height: 1.2;
    }

    /* è­¦å‘Šæ–‡å­—é¢œè‰² */
    .warning-text {
        color: #f59e0b !important;
    }

    /* åŸç†è§£é‡Šæ¡†æ ·å¼ */
    .principle-box {
        background: rgba(59, 130, 246, 0.05);
        border: 1px solid rgba(59, 130, 246, 0.2);
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }

    .principle-title {
        color: #1e40af;
        font-weight: 600;
        font-size: 1rem;
        margin-bottom: 0.5rem;
    }

    .principle-content {
        color: #374151;
        font-size: 0.9rem;
        line-height: 1.5;
    }

    /* æç¤ºæ¡†æ ·å¼ */
    .step-tip {
        background: #eff6ff;
        border: 1px solid #bfdbfe;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
        border-left: 4px solid #3b82f6;
    }

    .step-tip-title {
        color: #1e40af;
        font-weight: 600;
        font-size: 0.95rem;
        margin-bottom: 0.5rem;
    }

    .step-tip-content {
        color: #374151;
        font-size: 0.85rem;
        line-height: 1.4;
    }

    /* éšè—é»˜è®¤çš„Streamlitå…ƒç´  */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    
    /* ä¿®æ”¹æ•£ç‚¹å›¾æ ‡ç­¾é¢œè‰²ä¸ºæµ…è“è‰² */
    .element-container .stScatter {
        color: #3b82f6 !important;
    }
    
    /* ä¿®æ”¹metricæ ‡ç­¾é¢œè‰² */
    [data-testid="metric-container"] > div > div > div > div {
        color: #3b82f6 !important;
    }
</style>
""", unsafe_allow_html=True)

# ==================== é»˜è®¤é…ç½®æ•°æ® - é‡æ„ä¸ºæ¸ é“åç§°->æ¸ é“å·åˆ—è¡¨ ====================
DEFAULT_CHANNEL_MAPPING = {
    'æ€»ä½“': ['9000'],
    'æ–°åª’ä½“': ['500345', '500346', '500447', '500449', '500450', '500531', '500542'],
    'åº”ç”¨å®': ['5007XS', '500349', '500350'],
    'é¼ä¹-ç››ä¸–6': ['500285'],
    'é¼ä¹-ç››ä¸–7': ['500286'],
    'é…·æ´¾': ['5108', '5528'],
    'æ–°ç¾-åŒ—äº¬1': ['500274'],
    'æ–°ç¾-åŒ—äº¬2': ['500275'],
    'A_æ·±åœ³è›‹ä¸2': ['500316'],
    'è£è€€': ['500297'],
    'åä¸º': ['5057'],
    'vivo': ['5237'],
    'å°ç±³': ['5599'],
    'OPPO': ['5115'],
    'ç½‘æ˜“': ['500471', '500480', '500481', '500482'],
    'åä¸ºéå•†åº—-å“ä¼—': ['500337', '500338', '500343', '500445', '500383', '500444', '500441'],
    'åä¸ºéå•†åº—-å¾®åˆ›': ['500543', '500544', '500545', '500546'],
    'é­…æ—': ['5072'],
    'OPPOéå•†åº—': ['500287', '500288'],
    'vivoéå•†åº—': ['5187'],
    'ç™¾åº¦sem--ç™¾åº¦æ—¶ä»£å®‰å“': ['500398', '500400', '500404'],
    'ç™¾åº¦sem--ç™¾åº¦æ—¶ä»£ios': ['500402', '500403', '500405'],
    'ç™¾é’è—¤-å®‰å“': ['500377', '500379', '500435', '500436', '500490', '500491', '500434', '500492'],
    'ç™¾é’è—¤-ios': ['500437'],
    'å°ç±³éå•†åº—': ['500170'],
    'åä¸ºéå•†åº—-æ˜Ÿç«': ['500532', '500533', '500534', '500537', '500538', '500539', '500540', '500541'],
    'å¾®åš-èœœæ©˜': ['500504', '500505'],
    'å¾®åš-å¤®å¹¿': ['500367', '500368', '500369'],
    'å¹¿ç‚¹é€š': ['500498', '500497', '500500', '500501', '500496', '500499'],
    'ç½‘æ˜“æ˜“æ•ˆ': ['500514', '500515', '500516']
}

# åˆ›å»ºåå‘æ˜ å°„ï¼šæ¸ é“å·->æ¸ é“åç§°
def create_reverse_mapping(channel_mapping):
    reverse_mapping = {}
    for channel_name, pids in channel_mapping.items():
        for pid in pids:
            reverse_mapping[str(pid)] = channel_name
    return reverse_mapping

# ==================== æ—¥æœŸå¤„ç†å‡½æ•° ====================
def get_default_target_month():
    today = datetime.datetime.now()
    if today.month <= 2:
        target_year = today.year - 1
        target_month = today.month + 10
    else:
        target_year = today.year
        target_month = today.month - 2
    return f"{target_year}-{target_month:02d}"

# ==================== æ•°æ®ç±»å‹è½¬æ¢å‡½æ•° ====================
def safe_convert_to_numeric(value):
    """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•°å€¼ç±»å‹"""
    if pd.isna(value) or value == '' or value is None:
        return 0
    try:
        if isinstance(value, str):
            value = value.strip()
            if value == '' or value.lower() in ['nan', 'null', 'none']:
                return 0
        return pd.to_numeric(value, errors='coerce')
    except:
        return 0

# ==================== æ•°æ®é¢„è§ˆä¼˜åŒ–å‡½æ•° ====================
def optimize_dataframe_for_preview(df, max_rows=2):
    """ä¼˜åŒ–DataFrameé¢„è§ˆï¼šæœ‰å€¼çš„åˆ—æ”¾å‰é¢ï¼Œè·³è¿‡dateä¸º'æ—¥æœŸ'çš„è¡Œ"""
    preview_df = df.copy()
    
    # è·³è¿‡dateå€¼ä¸º"æ—¥æœŸ"çš„è¡Œ
    if 'date' in preview_df.columns:
        preview_df = preview_df[preview_df['date'] != 'æ—¥æœŸ']
    if 'æ—¥æœŸ' in preview_df.columns:
        preview_df = preview_df[preview_df['æ—¥æœŸ'] != 'æ—¥æœŸ']
    
    # å–å‰max_rowsè¡Œ
    preview_df = preview_df.head(max_rows)
    
    if preview_df.empty:
        return preview_df
    
    # è®¡ç®—æ¯åˆ—çš„éç©ºå€¼æ•°é‡
    non_null_counts = {}
    for col in preview_df.columns:
        non_null_count = preview_df[col].notna().sum()
        # æ’é™¤å…¨ä¸º0æˆ–ç©ºçš„æ•°å€¼åˆ—
        if preview_df[col].dtype in ['int64', 'float64']:
            non_zero_count = (preview_df[col] != 0).sum()
            non_null_counts[col] = non_null_count + non_zero_count
        else:
            non_null_counts[col] = non_null_count
    
    # æŒ‰éç©ºå€¼æ•°é‡æ’åºåˆ—
    sorted_columns = sorted(non_null_counts.keys(), key=lambda x: non_null_counts[x], reverse=True)
    
    # ç¡®ä¿'æ•°æ®æ¥æº'åˆ—åœ¨æœ€å‰é¢ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'æ•°æ®æ¥æº' in sorted_columns:
        sorted_columns.remove('æ•°æ®æ¥æº')
        sorted_columns.insert(0, 'æ•°æ®æ¥æº')
    
    return preview_df[sorted_columns]

# ==================== æ¸ é“æ˜ å°„å¤„ç†å‡½æ•° - ä¿®æ”¹ä¸ºæ”¯æŒæ–°æ ¼å¼ ====================
def parse_channel_mapping_from_excel(channel_file):
    """ä»ä¸Šä¼ çš„Excelæ–‡ä»¶è§£ææ¸ é“æ˜ å°„"""
    try:
        df = pd.read_excel(channel_file)
        channel_mapping = {}
        
        for _, row in df.iterrows():
            channel_name = str(row.iloc[0]).strip()
            if pd.isna(channel_name) or channel_name == '' or channel_name == 'nan':
                continue
                
            pids = []
            for col_idx in range(1, len(row)):
                pid = row.iloc[col_idx]
                if pd.isna(pid) or str(pid).strip() in ['', 'nan', 'ã€€', ' ']:
                    continue
                # ç¡®ä¿æ¸ é“å·ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œå»é™¤å°æ•°
                pid_str = str(int(float(pid))) if isinstance(pid, (int, float)) else str(pid).strip()
                if pid_str:
                    pids.append(pid_str)
            
            if pids:
                channel_mapping[channel_name] = pids
                    
        return channel_mapping
    except Exception as e:
        st.error(f"è§£ææ¸ é“æ˜ å°„æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
        return {}

# ==================== å†…ç½®ARPUæ•°æ® ====================
def get_builtin_arpu_data():
    """è·å–å†…ç½®çš„2024.1-2025.4çš„ARPUåŸºç¡€æ•°æ®"""
    # ç¤ºä¾‹æ•°æ®ç»“æ„ - ç”¨æˆ·å¯ä»¥æ›¿æ¢ä¸ºå®é™…çš„10000å¤šè¡Œæ•°æ®
    builtin_data = []
    
    # 2024å¹´1æœˆåˆ°2025å¹´4æœˆçš„æ•°æ®
    months = []
    for year in [2024, 2025]:
        start_month = 1 if year == 2024 else 1
        end_month = 12 if year == 2024 else 4
        for month in range(start_month, end_month + 1):
            months.append(f"{year}-{month:02d}")
    
    # ä¸ºæ¯ä¸ªæ¸ é“å’Œæœˆä»½ç”Ÿæˆç¤ºä¾‹æ•°æ®
    for channel_name, pids in DEFAULT_CHANNEL_MAPPING.items():
        for pid in pids:
            for month in months:
                # ç”Ÿæˆç¤ºä¾‹æ•°æ® - ç”¨æˆ·éœ€è¦æ›¿æ¢ä¸ºå®é™…æ•°æ®
                builtin_data.append({
                    'æœˆä»½': month,
                    'pid': pid,
                    'stat_date': f"{month}-15",  # ç¤ºä¾‹æ—¥æœŸ
                    'instl_user_cnt': np.random.randint(100, 5000),  # ç¤ºä¾‹æ–°å¢ç”¨æˆ·æ•°
                    'ad_all_rven_1d_m': np.random.uniform(10, 500)  # ç¤ºä¾‹æ”¶å…¥
                })
    
    return pd.DataFrame(builtin_data)

def load_user_arpu_data_after_april(uploaded_file, builtin_df):
    """åŠ è½½ç”¨æˆ·ä¸Šä¼ çš„5æœˆåŠä¹‹åçš„ARPUæ•°æ®ï¼Œå¹¶ä¸å†…ç½®æ•°æ®åˆå¹¶"""
    try:
        user_df = pd.read_excel(uploaded_file)
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_cols = ['pid', 'instl_user_cnt', 'ad_all_rven_1d_m']
        missing_cols = [col for col in required_cols if col not in user_df.columns]
        
        if missing_cols:
            return None, f"æ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {', '.join(missing_cols)}"
        
        # ç­›é€‰5æœˆåŠä¹‹åçš„æ•°æ®
        if 'æœˆä»½' in user_df.columns:
            # ä½¿ç”¨æœˆä»½åˆ—ç­›é€‰
            user_df['month_num'] = pd.to_datetime(user_df['æœˆä»½'], format='%Y-%m', errors='coerce')
            april_2025 = pd.to_datetime('2025-04', format='%Y-%m')
            user_df = user_df[user_df['month_num'] > april_2025]
        elif 'stat_date' in user_df.columns:
            # ä½¿ç”¨æ—¥æœŸåˆ—ç­›é€‰
            user_df['stat_date'] = pd.to_datetime(user_df['stat_date'], errors='coerce')
            user_df['month'] = user_df['stat_date'].dt.to_period('M')
            april_2025 = pd.Period('2025-04', freq='M')
            user_df = user_df[user_df['month'] > april_2025]
        else:
            return None, "æœªæ‰¾åˆ°æœˆä»½æˆ–æ—¥æœŸåˆ—è¿›è¡Œç­›é€‰"
        
        # åˆå¹¶å†…ç½®æ•°æ®å’Œç”¨æˆ·æ•°æ®
        combined_df = pd.concat([builtin_df, user_df], ignore_index=True)
        
        return combined_df, "æ•°æ®åˆå¹¶æˆåŠŸ"
        
    except Exception as e:
        return None, f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}"
def parse_channel_mapping_from_excel(channel_file):
    """ä»ä¸Šä¼ çš„Excelæ–‡ä»¶è§£ææ¸ é“æ˜ å°„"""
    try:
        df = pd.read_excel(channel_file)
        channel_mapping = {}
        
        for _, row in df.iterrows():
            channel_name = str(row.iloc[0]).strip()
            if pd.isna(channel_name) or channel_name == '' or channel_name == 'nan':
                continue
                
            pids = []
            for col_idx in range(1, len(row)):
                pid = row.iloc[col_idx]
                if pd.isna(pid) or str(pid).strip() in ['', 'nan', 'ã€€', ' ']:
                    continue
                # ç¡®ä¿æ¸ é“å·ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œå»é™¤å°æ•°
                pid_str = str(int(float(pid))) if isinstance(pid, (int, float)) else str(pid).strip()
                if pid_str:
                    pids.append(pid_str)
            
            if pids:
                channel_mapping[channel_name] = pids
                    
        return channel_mapping
    except Exception as e:
        st.error(f"è§£ææ¸ é“æ˜ å°„æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
        return {}

# ==================== æ–‡ä»¶æ•´åˆæ ¸å¿ƒå‡½æ•° - ä¼˜åŒ–æ€§èƒ½ ====================
@st.cache_data
def integrate_excel_files_cached(file_names, file_contents, target_month, channel_mapping):
    """ç¼“å­˜ç‰ˆæœ¬çš„æ–‡ä»¶æ•´åˆå‡½æ•°"""
    all_data = pd.DataFrame()
    processed_count = 0
    mapping_warnings = []

    for i, (file_name, file_content) in enumerate(zip(file_names, file_contents)):
        source_name = os.path.splitext(file_name)[0]
        
        # æ¸ é“æ˜ å°„å¤„ç† - ä¿®å¤é€»è¾‘
        if source_name in channel_mapping:
            # å¦‚æœæ–‡ä»¶åç›´æ¥æ˜¯æ¸ é“åï¼Œç›´æ¥ä½¿ç”¨
            mapped_source = source_name
        else:
            # å¦‚æœæ–‡ä»¶åä¸æ˜¯æ¸ é“åï¼Œå°è¯•ä½œä¸ºæ¸ é“å·æŸ¥æ‰¾
            reverse_mapping = create_reverse_mapping(channel_mapping)
            if source_name in reverse_mapping:
                mapped_source = reverse_mapping[source_name]
            else:
                mapped_source = source_name
                mapping_warnings.append(f"æ–‡ä»¶ '{source_name}' æœªåœ¨æ¸ é“æ˜ å°„è¡¨ä¸­æ‰¾åˆ°å¯¹åº”é¡¹")

        try:
            # ä»å†…å­˜ä¸­è¯»å–Excelæ–‡ä»¶
            xls = pd.ExcelFile(io.BytesIO(file_content))
            sheet_names = xls.sheet_names

            # æŸ¥æ‰¾ç›®æ ‡å·¥ä½œè¡¨
            ocpx_sheet = None
            for sheet in sheet_names:
                if "ocpxç›‘æµ‹ç•™å­˜æ•°" in sheet:
                    ocpx_sheet = sheet
                    break

            if ocpx_sheet:
                file_data = pd.read_excel(io.BytesIO(file_content), sheet_name=ocpx_sheet)
            else:
                file_data = pd.read_excel(io.BytesIO(file_content), sheet_name=0)

            if file_data is not None and not file_data.empty:
                # æ•°æ®å¤„ç†é€»è¾‘ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                file_data_copy = file_data.copy()
                
                # æ£€æµ‹å¹¶å¤„ç†æ•°æ®æ ¼å¼
                has_stat_date = 'stat_date' in file_data_copy.columns
                retain_columns = [f'new_retain_{i}' for i in range(1, 31)]
                has_retain_columns = any(col in file_data_copy.columns for col in retain_columns)

                if has_stat_date and has_retain_columns:
                    # æ–°æ ¼å¼è¡¨å¤„ç†
                    standardized_data = file_data_copy.copy()
                    if 'new' in standardized_data.columns:
                        standardized_data['å›ä¼ æ–°å¢æ•°'] = standardized_data['new'].apply(safe_convert_to_numeric)

                    for i in range(1, 31):
                        retain_col = f'new_retain_{i}'
                        if retain_col in standardized_data.columns:
                            standardized_data[str(i)] = standardized_data[retain_col].apply(safe_convert_to_numeric)

                    date_col = 'stat_date'
                    standardized_data[date_col] = pd.to_datetime(standardized_data[date_col], errors='coerce')
                    standardized_data[date_col] = standardized_data[date_col].dt.strftime('%Y-%m-%d')
                    standardized_data['æ—¥æœŸ'] = standardized_data[date_col]
                    standardized_data['month'] = standardized_data[date_col].str[:7]

                    filtered_data = standardized_data[standardized_data['month'] == target_month].copy()

                    if not filtered_data.empty:
                        filtered_data.insert(0, 'æ•°æ®æ¥æº', mapped_source)
                        if 'stat_date' in filtered_data.columns:
                            filtered_data['date'] = filtered_data['stat_date']
                        all_data = pd.concat([all_data, filtered_data], ignore_index=True)
                        processed_count += 1
                else:
                    # ä¼ ç»Ÿæ ¼å¼è¡¨å¤„ç†
                    retention_col = None
                    for col in file_data_copy.columns:
                        if 'ç•™å­˜å¤©æ•°' in str(col):
                            retention_col = col
                            break

                    report_users_col = None
                    for col in file_data_copy.columns:
                        if 'å›ä¼ æ–°å¢æ•°' in str(col):
                            report_users_col = col
                            break
                        if 'total_new_users' in str(col).lower():
                            report_users_col = col
                            break

                    column_b = file_data_copy.columns[1] if len(file_data_copy.columns) > 1 else None

                    if report_users_col and report_users_col != 'å›ä¼ æ–°å¢æ•°':
                        file_data_copy['å›ä¼ æ–°å¢æ•°'] = file_data_copy[report_users_col].apply(safe_convert_to_numeric)
                    elif not report_users_col and column_b:
                        file_data_copy['å›ä¼ æ–°å¢æ•°'] = file_data_copy[column_b].apply(safe_convert_to_numeric)

                    for i in range(1, 31):
                        col_name = str(i)
                        if col_name in file_data_copy.columns:
                            file_data_copy[col_name] = file_data_copy[col_name].apply(safe_convert_to_numeric)

                    # ç®€åŒ–æ—¥æœŸå¤„ç†ï¼Œç›´æ¥æŒ‰æœˆä»½ç­›é€‰
                    date_col = None
                    for col in file_data_copy.columns:
                        if 'æ—¥æœŸ' in str(col):
                            date_col = col
                            break

                    if date_col:
                        file_data_copy['month'] = file_data_copy[date_col].apply(
                            lambda x: str(x)[:7] if isinstance(x, str) else None
                        )
                        filtered_data = file_data_copy[file_data_copy['month'] == target_month].copy()
                    else:
                        filtered_data = file_data_copy.copy()

                    if not filtered_data.empty:
                        filtered_data.insert(0, 'æ•°æ®æ¥æº', mapped_source)
                        if retention_col is not None:
                            filtered_data.rename(columns={retention_col: 'date'}, inplace=True)
                        elif date_col and date_col != 'date':
                            filtered_data['date'] = filtered_data[date_col]
                        all_data = pd.concat([all_data, filtered_data], ignore_index=True)
                        processed_count += 1

        except Exception as e:
            st.error(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {str(e)}")

    return all_data, processed_count, mapping_warnings

def integrate_excel_files_streamlit(uploaded_files, target_month=None, channel_mapping=None):
    """ä¼˜åŒ–æ€§èƒ½çš„æ–‡ä»¶æ•´åˆå‡½æ•°"""
    if target_month is None:
        target_month = get_default_target_month()

    # ä½¿ç”¨ä¼ å…¥çš„æ¸ é“æ˜ å°„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤æ˜ å°„
    if channel_mapping is None:
        channel_mapping = DEFAULT_CHANNEL_MAPPING

    # å‡†å¤‡ç¼“å­˜æ•°æ®
    file_names = [f.name for f in uploaded_files]
    file_contents = [f.read() for f in uploaded_files]
    
    return integrate_excel_files_cached(file_names, file_contents, target_month, channel_mapping)

# ==================== ç•™å­˜ç‡è®¡ç®—å‡½æ•° - ä¿®æ”¹ä¸ºä»¥æ–°å¢ç”¨æˆ·å‡å€¼ä¸ºåŸºæ•° ====================
def calculate_retention_rates_new_method(df):
    """æ–°çš„ç•™å­˜ç‡è®¡ç®—æ–¹æ³•ï¼šæŒ‰æ¸ é“è®¡ç®—å¹³å‡æ–°å¢æ•°ä½œä¸ºåŸºæ•°ï¼Œå„å¤©ç•™å­˜æ•°/å¹³å‡æ–°å¢æ•°"""
    retention_results = []
    data_sources = df['æ•°æ®æ¥æº'].unique()

    for source in data_sources:
        source_data = df[df['æ•°æ®æ¥æº'] == source].copy()
        
        # è®¡ç®—å¹³å‡æ–°å¢ç”¨æˆ·æ•°ä½œä¸ºåŸºæ•°
        new_users_values = []
        for _, row in source_data.iterrows():
            new_users = safe_convert_to_numeric(row.get('å›ä¼ æ–°å¢æ•°', 0))
            if new_users > 0:
                new_users_values.append(new_users)
        
        if not new_users_values:
            continue
        
        avg_new_users = np.mean(new_users_values)
        
        # è®¡ç®—1-30å¤©çš„å¹³å‡ç•™å­˜æ•°ï¼Œå¹¶ç”¨å¹³å‡æ–°å¢æ•°ä½œä¸ºåŸºæ•°è®¡ç®—ç•™å­˜ç‡
        retention_data = {'data_source': source, 'avg_new_users': avg_new_users}
        days = []
        rates = []
        
        for day in range(1, 31):
            day_col = str(day)
            day_retain_values = []
            
            for _, row in source_data.iterrows():
                if day_col in row and not pd.isna(row[day_col]):
                    retain_count = safe_convert_to_numeric(row[day_col])
                    if retain_count >= 0:  # å…è®¸0å€¼
                        day_retain_values.append(retain_count)
            
            if day_retain_values:
                avg_retain = np.mean(day_retain_values)
                # ä¿®æ”¹è®¡ç®—æ–¹æ³•ï¼šç•™å­˜ç‡ = å¹³å‡ç•™å­˜æ•° / å¹³å‡æ–°å¢æ•°
                retention_rate = avg_retain / avg_new_users if avg_new_users > 0 else 0
                
                # ç•™å­˜ç‡èŒƒå›´ä¸º 0 â‰¤ ç•™å­˜ç‡ â‰¤ 1.0
                if 0 <= retention_rate <= 1.0:
                    days.append(day)
                    rates.append(retention_rate)
        
        if days:
            retention_results.append({
                'data_source': source,
                'days': np.array(days),
                'rates': np.array(rates),
                'avg_new_users': avg_new_users
            })

    return retention_results

# ==================== æ•°å­¦å»ºæ¨¡å‡½æ•° ====================
def power_function(x, a, b):
    """å¹‚å‡½æ•°ï¼šy = a * x^b"""
    return a * np.power(x, b)

def exponential_function(x, c, d):
    """æŒ‡æ•°å‡½æ•°ï¼šy = c * exp(d * x)"""
    return c * np.exp(d * x)

def calculate_cumulative_lt(days_array, rates_array, target_days):
    """è®¡ç®—æŒ‡å®šå¤©æ•°çš„ç´¯ç§¯LTå€¼"""
    result = {}
    for day in target_days:
        idx = np.searchsorted(days_array, day, side='right')
        if idx > 0:
            cumulative_lt = 1.0 + np.sum(rates_array[1:idx])
            result[day] = cumulative_lt
    return result

def calculate_lt_advanced(retention_result, channel_name, lt_years=5, return_curve_data=False, key_days=None):
    """æŒ‰æ¸ é“è§„åˆ™è®¡ç®— LT"""
    # æ¸ é“è§„åˆ™
    CHANNEL_RULES = {
        "åä¸º": {"stage_2": [30, 120], "stage_3_base": [120, 220]},
        "å°ç±³": {"stage_2": [30, 190], "stage_3_base": [190, 290]},
        "oppo": {"stage_2": [30, 160], "stage_3_base": [160, 260]},
        "vivo": {"stage_2": [30, 150], "stage_3_base": [150, 250]},
        "iphone": {"stage_2": [30, 150], "stage_3_base": [150, 250], "stage_2_func": "log"},
        "å…¶ä»–": {"stage_2": [30, 100], "stage_3_base": [100, 200]}
    }

    # æ¸ é“è§„åˆ™åŒ¹é…
    if re.search(r'åä¸º', channel_name):
        rules = CHANNEL_RULES["åä¸º"]
    elif re.search(r'å°ç±³', channel_name):
        rules = CHANNEL_RULES["å°ç±³"]
    elif re.search(r'[oO][pP][pP][oO]', channel_name):
        rules = CHANNEL_RULES["oppo"]
    elif re.search(r'vivo', channel_name):
        rules = CHANNEL_RULES["vivo"]
    elif re.search(r'[iI][pP]hone', channel_name):
        rules = CHANNEL_RULES["iphone"]
    else:
        rules = CHANNEL_RULES["å…¶ä»–"]
        
    stage_2_start, stage_2_end = rules["stage_2"]
    stage_3_base_start, stage_3_base_end = rules["stage_3_base"]

    max_days = lt_years * 365

    days = retention_result["days"]
    rates = retention_result["rates"]

    fit_params = {}

    # ç¬¬ä¸€é˜¶æ®µ - å¹‚å‡½æ•°æ‹Ÿåˆ
    try:
        popt_power, _ = curve_fit(power_function, days, rates)
        a, b = popt_power
        fit_params["power"] = {"a": a, "b": b}

        days_full = np.arange(1, 31)
        rates_full = power_function(days_full, a, b)
        lt1_to_30 = np.sum(rates_full)
    except Exception as e:
        lt1_to_30 = 0.0
        a, b = 1.0, -1.0

    # ç¬¬äºŒé˜¶æ®µ
    try:
        days_stage_2 = np.arange(stage_2_start, stage_2_end + 1)
        rates_stage_2 = power_function(days_stage_2, a, b)
        lt_stage_2 = np.sum(rates_stage_2)
    except Exception as e:
        lt_stage_2 = 0.0
        rates_stage_2 = np.array([])

    # ç¬¬ä¸‰é˜¶æ®µ - æŒ‡æ•°æ‹Ÿåˆ
    try:
        days_stage_3_base = np.arange(stage_3_base_start, stage_3_base_end + 1)
        rates_stage_3_base = power_function(days_stage_3_base, a, b)

        initial_c = rates_stage_3_base[0]
        initial_d = -0.001
        popt_exp, _ = curve_fit(
            exponential_function,
            days_stage_3_base,
            rates_stage_3_base,
            p0=[initial_c, initial_d],
            bounds=([0, -np.inf], [np.inf, 0])
        )
        c, d = popt_exp
        fit_params["exponential"] = {"c": c, "d": d}
        days_stage_3 = np.arange(stage_3_base_start, max_days + 1)
        rates_stage_3 = exponential_function(days_stage_3, c, d)
        lt_stage_3 = np.sum(rates_stage_3)
    except Exception as e:
        days_stage_3 = np.arange(stage_3_base_start, max_days + 1)
        rates_stage_3 = power_function(days_stage_3, a, b) if 'a' in locals() else np.zeros(len(days_stage_3))
        lt_stage_3 = np.sum(rates_stage_3)

    total_lt = 1.0 + lt1_to_30 + lt_stage_2 + lt_stage_3

    try:
        predicted_rates = power_function(days, a, b)
        r2_score = 1 - np.sum((rates - predicted_rates) ** 2) / np.sum((rates - np.mean(rates)) ** 2)
    except:
        r2_score = 0.0

    if return_curve_data:
        all_days = np.concatenate([days_full, days_stage_2, days_stage_3])
        
        if 'rates_stage_2' not in locals():
            rates_stage_2 = power_function(days_stage_2, a, b)
        
        all_rates = np.concatenate([rates_full, rates_stage_2, rates_stage_3])

        sort_idx = np.argsort(all_days)
        all_days = all_days[sort_idx]
        all_rates = all_rates[sort_idx]

        max_idx = np.searchsorted(all_days, lt_years * 365, side='right')
        all_days = all_days[:max_idx]
        all_rates = all_rates[:max_idx]

        key_days_lt = {}
        if key_days:
            key_days_lt = calculate_cumulative_lt(all_days, all_rates, key_days)

        return {
            'lt_value': total_lt,
            'fit_params': fit_params,
            'power_r2': max(0, min(1, r2_score)),
            'success': True,
            'model_used': 'power+exponential',
            'curve_days': all_days,
            'curve_rates': all_rates,
            'key_days_lt': key_days_lt
        }

    return total_lt

# ==================== å•æ¸ é“å›¾è¡¨ç”Ÿæˆå‡½æ•° - å½»åº•è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ ====================
def create_individual_channel_chart(channel_name, curve_data, original_data, max_days=100):
    """åˆ›å»ºå•ä¸ªæ¸ é“çš„100å¤©LTæ‹Ÿåˆå›¾è¡¨ - é¿å…ä¸­æ–‡å­—ä½“é—®é¢˜"""
    
    fig, ax = plt.subplots(figsize=(8, 6))
    
    # ç»˜åˆ¶å®é™…æ•°æ®ç‚¹
    if channel_name in original_data:
        ax.scatter(
            original_data[channel_name]["days"],
            original_data[channel_name]["rates"],
            color='#ef4444',  # æµ…çº¢è‰²æ”¹ä¸ºçº¢è‰²
            s=60,
            alpha=0.8,
            label='Actual Data',  # ä½¿ç”¨è‹±æ–‡é¿å…å­—ä½“é—®é¢˜
            zorder=3
        )
    
    # é™åˆ¶æ‹Ÿåˆæ›²çº¿åˆ°100å¤©
    curve_days = curve_data["days"]
    curve_rates = curve_data["rates"]
    
    # ç­›é€‰100å¤©å†…çš„æ•°æ®
    mask = curve_days <= max_days
    curve_days_filtered = curve_days[mask]
    curve_rates_filtered = curve_rates[mask]
    
    # ç»˜åˆ¶æ‹Ÿåˆæ›²çº¿
    ax.plot(
        curve_days_filtered,
        curve_rates_filtered,
        color='#3b82f6',  # è“è‰²
        linewidth=2.5,
        label='Fitted Curve',  # ä½¿ç”¨è‹±æ–‡é¿å…å­—ä½“é—®é¢˜
        zorder=2
    )
    
    # è®¾ç½®å›¾è¡¨æ ·å¼ - ä½¿ç”¨è‹±æ–‡é¿å…å­—ä½“é—®é¢˜
    ax.set_xlim(0, max_days)
    ax.set_ylim(0, 0.6)
    ax.set_xlabel('Retention Days', fontsize=12)
    ax.set_ylabel('Retention Rate', fontsize=12)
    ax.set_title(f'{channel_name} ({max_days}d LT Fitting)', fontsize=14, fontweight='bold')
    ax.grid(True, linestyle='--', alpha=0.4)
    
    # è®¾ç½®å›¾ä¾‹
    ax.legend(fontsize=10)
    
    # è®¾ç½®Yè½´åˆ»åº¦ä¸ºç™¾åˆ†æ¯”
    y_ticks = [0, 0.15, 0.3, 0.45, 0.6]
    y_labels = ['0%', '15%', '30%', '45%', '60%']
    ax.set_yticks(y_ticks)
    ax.set_yticklabels(y_labels)
    
    plt.tight_layout()
    return fig

# ==================== ä¸»åº”ç”¨ç¨‹åº ====================

# ä¸»æ ‡é¢˜
st.markdown("""
<div class="main-header">
    <div class="main-title">ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†æç³»ç»Ÿ</div>
    <div class="main-subtitle">åŸºäºåˆ†é˜¶æ®µæ•°å­¦å»ºæ¨¡çš„LTVé¢„æµ‹</div>
</div>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–session state
session_keys = [
    'channel_mapping', 'merged_data', 'cleaned_data', 'retention_data',
    'lt_results_2y', 'lt_results_5y', 'arpu_data', 'ltv_results', 'current_step',
    'excluded_data', 'excluded_dates_info', 'show_exclusion', 'show_manual_arpu',
    'visualization_data_5y', 'original_data', 'show_upload_interface'
]
for key in session_keys:
    if key not in st.session_state:
        st.session_state[key] = None

# è®¾ç½®é»˜è®¤å€¼
if st.session_state.channel_mapping is None:
    st.session_state.channel_mapping = DEFAULT_CHANNEL_MAPPING
if st.session_state.current_step is None:
    st.session_state.current_step = 0
if st.session_state.show_exclusion is None:
    st.session_state.show_exclusion = False
if st.session_state.show_manual_arpu is None:
    st.session_state.show_manual_arpu = False
if st.session_state.show_upload_interface is None:
    st.session_state.show_upload_interface = False

# ==================== åˆ†ææ­¥éª¤å®šä¹‰ - é‡æ„ä¸º3æ­¥ ====================
ANALYSIS_STEPS = [
    {
        "name": "LTæ¨¡å‹æ„å»º",
        "sub_steps": ["æ•°æ®ä¸Šä¼ æ±‡æ€»", "å¼‚å¸¸å‰”é™¤", "ç•™å­˜ç‡è®¡ç®—", "LTæ‹Ÿåˆåˆ†æ"]
    },
    {"name": "ARPUè®¡ç®—"},
    {"name": "LTVç»“æœæŠ¥å‘Š"}
]

# ==================== ä¾§è¾¹æ å¯¼èˆª ====================
with st.sidebar:
    st.markdown('<div class="nav-container">', unsafe_allow_html=True)
    st.markdown('<h4 style="text-align: center; margin-bottom: 1rem; color: white;">åˆ†ææµç¨‹</h4>',
                unsafe_allow_html=True)

    for i, step in enumerate(ANALYSIS_STEPS):
        button_text = f"{i + 1}. {step['name']}"
        if st.button(button_text, key=f"nav_{i}", use_container_width=True,
                     type="primary" if i == st.session_state.current_step else "secondary"):
            st.session_state.current_step = i
            st.rerun()
        
        # åªåœ¨LTæ¨¡å‹æ„å»ºæ—¶æ˜¾ç¤ºå­æ­¥éª¤
        if "sub_steps" in step and i == st.session_state.current_step and step['name'] == "LTæ¨¡å‹æ„å»º":
            sub_steps_text = " â€¢ ".join(step["sub_steps"])
            st.markdown(f'<div class="sub-steps">{sub_steps_text}</div>', unsafe_allow_html=True)

    st.markdown('</div>', unsafe_allow_html=True)

# ==================== é¡µé¢è·¯ç”± ====================
current_page = ANALYSIS_STEPS[st.session_state.current_step]["name"]

# ==================== é¡µé¢å†…å®¹ ====================

if current_page == "LTæ¨¡å‹æ„å»º":
    # åŸç†è§£é‡Š
    st.markdown("""
    <div class="principle-box">
        <div class="principle-title">ğŸ“š LTæ¨¡å‹æ„å»ºåŸç†</div>
        <div class="principle-content">
        LTæ¨¡å‹æ„å»ºåŒ…å«å››ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼š<br>
        <strong>1. æ•°æ®ä¸Šä¼ æ±‡æ€»ï¼š</strong>æ•´åˆå¤šä¸ªExcelæ–‡ä»¶ï¼Œæ”¯æŒæ–°æ ¼å¼è¡¨å’Œä¼ ç»Ÿæ ¼å¼è¡¨<br>
        <strong>2. å¼‚å¸¸å‰”é™¤ï¼š</strong>æŒ‰éœ€æ¸…ç†å¼‚å¸¸æ•°æ®ï¼Œæé«˜æ¨¡å‹å‡†ç¡®æ€§<br>
        <strong>3. ç•™å­˜ç‡è®¡ç®—ï¼š</strong>OCPXæ ¼å¼è¡¨æŒ‰æ¸ é“è®¡ç®—ï¼Œå„å¤©ç•™å­˜åˆ—ï¼ˆ1ã€2ã€3...ï¼‰å¹³å‡å€¼Ã·å›ä¼ æ–°å¢æ•°å¹³å‡å€¼<br>
        <strong>4. LTæ‹Ÿåˆåˆ†æï¼š</strong>é‡‡ç”¨ä¸‰é˜¶æ®µåˆ†å±‚å»ºæ¨¡ï¼Œé¢„æµ‹ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸé•¿åº¦
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # æ­¥éª¤1ï¼šæ•°æ®ä¸Šä¼ ä¸æ±‡æ€»
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.subheader("1. æ•°æ®ä¸Šä¼ ä¸æ±‡æ€»")
    
    # é»˜è®¤æ¸ é“æ˜ å°„ - é»˜è®¤å±•å¼€ï¼Œæ·»åŠ ä¸Šä¼ é€‰æ‹©
    with st.expander("æŸ¥çœ‹é»˜è®¤æ¸ é“æ˜ å°„ï¼ˆğŸ“‹ è¯·æŒ‰æ¸ é“åç§°å‘½åæ–‡ä»¶ï¼Œç”¨äºARPUè®¡ç®—ï¼‰", expanded=True):
        st.markdown("""
        <div class="step-tip">
            <div class="step-tip-title">ğŸ’¡ é‡è¦æç¤º</div>
            <div class="step-tip-content">
            è¯·å°†Excelæ–‡ä»¶æŒ‰ç…§ä¸‹è¡¨ä¸­çš„<strong>æ¸ é“åç§°</strong>è¿›è¡Œå‘½åï¼Œè¿™æ ·ç³»ç»Ÿå¯ä»¥è‡ªåŠ¨åŒ¹é…ARPUæ•°æ®ã€‚<br>
            ä¾‹å¦‚ï¼šåä¸º.xlsxã€å°ç±³.xlsxã€OPPO.xlsx ç­‰
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        default_mapping_rows = []
        for channel_name, pids in DEFAULT_CHANNEL_MAPPING.items():
            for pid in pids:
                default_mapping_rows.append({'æ¸ é“åç§°': channel_name, 'æ¸ é“å·': pid})
        default_mapping_df = pd.DataFrame(default_mapping_rows)
        st.dataframe(default_mapping_df, use_container_width=True)
        
        # æ·»åŠ æ˜¯å¦éœ€è¦ä¸Šä¼ æ¸ é“æ˜ å°„çš„é€‰æ‹©
        st.subheader("æ¸ é“æ˜ å°„è®¾ç½®")
        need_custom_mapping = st.radio(
            "æ˜¯å¦éœ€è¦ä¸Šä¼ è‡ªå®šä¹‰æ¸ é“æ˜ å°„ï¼Ÿ",
            options=["ä½¿ç”¨é»˜è®¤æ˜ å°„", "ä¸Šä¼ è‡ªå®šä¹‰æ˜ å°„"],
            index=0,
            help="å¦‚æœæ‚¨çš„æ¸ é“ä¸é»˜è®¤æ˜ å°„ä¸åŒï¼Œè¯·é€‰æ‹©ä¸Šä¼ è‡ªå®šä¹‰æ˜ å°„"
        )
    
    # æŒ‰é’®æ§åˆ¶æ˜¾ç¤ºä¸Šä¼ ç•Œé¢
    if not st.session_state.show_upload_interface:
        if st.button("å¼€å§‹æ•°æ®ä¸Šä¼ ", type="primary", use_container_width=True):
            st.session_state.show_upload_interface = True
            st.rerun()
    else:
        # æ¸ é“æ˜ å°„æ–‡ä»¶ä¸Šä¼  - åªåœ¨é€‰æ‹©è‡ªå®šä¹‰æ˜ å°„æ—¶æ˜¾ç¤º
        if need_custom_mapping == "ä¸Šä¼ è‡ªå®šä¹‰æ˜ å°„":
            channel_mapping_file = st.file_uploader(
                "ä¸Šä¼ æ¸ é“æ˜ å°„æ–‡ä»¶ (Excelæ ¼å¼)",
                type=['xlsx', 'xls'],
                help="æ ¼å¼ï¼šç¬¬ä¸€åˆ—ä¸ºæ¸ é“åç§°ï¼Œåç»­åˆ—ä¸ºå¯¹åº”çš„æ¸ é“å·"
            )
            
            if channel_mapping_file:
                try:
                    custom_mapping = parse_channel_mapping_from_excel(channel_mapping_file)
                    if custom_mapping and isinstance(custom_mapping, dict) and len(custom_mapping) > 0:
                        st.session_state.channel_mapping = custom_mapping
                        st.success(f"æ¸ é“æ˜ å°„æ–‡ä»¶åŠ è½½æˆåŠŸï¼å…±åŒ…å« {len(custom_mapping)} ä¸ªæ¸ é“")
                        
                        # è‡ªåŠ¨å±•å¼€æ˜ å°„è¯¦æƒ…
                        with st.expander("æŸ¥çœ‹æ¸ é“æ˜ å°„è¯¦æƒ…", expanded=True):
                            mapping_rows = []
                            for channel_name, pids in custom_mapping.items():
                                for pid in pids:
                                    mapping_rows.append({'æ¸ é“åç§°': channel_name, 'æ¸ é“å·': pid})
                            mapping_df = pd.DataFrame(mapping_rows)
                            st.dataframe(mapping_df, use_container_width=True)
                    else:
                        st.error("æ¸ é“æ˜ å°„æ–‡ä»¶è§£æå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤æ˜ å°„")
                except Exception as e:
                    st.error(f"è¯»å–æ¸ é“æ˜ å°„æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")
        else:
            st.info("æ­£åœ¨ä½¿ç”¨é»˜è®¤æ¸ é“æ˜ å°„")

        # æ•°æ®æ–‡ä»¶ä¸Šä¼ 
        uploaded_files = st.file_uploader(
            "é€‰æ‹©Excelæ•°æ®æ–‡ä»¶",
            type=['xlsx', 'xls'],
            accept_multiple_files=True,
            help="æ”¯æŒä¸Šä¼ å¤šä¸ªExcelæ–‡ä»¶"
        )

        default_month = get_default_target_month()
        target_month = st.text_input("ç›®æ ‡æœˆä»½ (YYYY-MM)", value=default_month)

        if uploaded_files:
            st.info(f"å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")

            if st.button("å¼€å§‹å¤„ç†æ•°æ®", type="primary", use_container_width=True):
                with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®æ–‡ä»¶..."):
                    try:
                        merged_data, processed_count, mapping_warnings = integrate_excel_files_streamlit(
                            uploaded_files, target_month, st.session_state.channel_mapping
                        )

                        if merged_data is not None and not merged_data.empty:
                            st.session_state.merged_data = merged_data
                            st.success(f"æ•°æ®å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {processed_count} ä¸ªæ–‡ä»¶")

                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("æ€»è®°å½•æ•°", f"{len(merged_data):,}")
                            with col2:
                                st.metric("æ•°æ®æ¥æºæ•°", merged_data['æ•°æ®æ¥æº'].nunique())
                            with col3:
                                if 'å›ä¼ æ–°å¢æ•°' in merged_data.columns:
                                    total_users = merged_data['å›ä¼ æ–°å¢æ•°'].sum()
                                    st.metric("æ€»æ–°å¢ç”¨æˆ·", f"{total_users:,.0f}")

                            if mapping_warnings:
                                st.warning("ä»¥ä¸‹æ–‡ä»¶æœªåœ¨æ¸ é“æ˜ å°„ä¸­æ‰¾åˆ°å¯¹åº”å…³ç³»ï¼š")
                                for warning in mapping_warnings:
                                    st.text(f"â€¢ {warning}")

                            # ä¼˜åŒ–çš„æ•°æ®é¢„è§ˆ - æ¯ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸¤è¡Œ
                            st.subheader("æ•°æ®é¢„è§ˆ")
                            unique_sources = merged_data['æ•°æ®æ¥æº'].unique()
                            
                            for source in unique_sources:
                                source_data = merged_data[merged_data['æ•°æ®æ¥æº'] == source]
                                optimized_preview = optimize_dataframe_for_preview(source_data, max_rows=2)
                                st.markdown(f"**{source}ï¼š**")
                                st.dataframe(optimized_preview, use_container_width=True)
                                
                        else:
                            st.error("æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
                    except Exception as e:
                        st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")
        else:
            st.info("è¯·é€‰æ‹©Excelæ–‡ä»¶å¼€å§‹æ•°æ®å¤„ç†")

    st.markdown('</div>', unsafe_allow_html=True)

    # æ­¥éª¤2ï¼šå¼‚å¸¸æ•°æ®å‰”é™¤ï¼ˆæŒ‰éœ€æ˜¾ç¤ºï¼‰
    if st.session_state.merged_data is not None:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("2. å¼‚å¸¸æ•°æ®å‰”é™¤")
        
        if not st.session_state.show_exclusion:
            if st.button("éœ€è¦å¼‚å¸¸æ•°æ®å‰”é™¤", use_container_width=True):
                st.session_state.show_exclusion = True
                st.rerun()
            st.info("å¦‚æ— éœ€å‰”é™¤å¼‚å¸¸æ•°æ®ï¼Œå¯ç›´æ¥è¿›è¡Œä¸‹ä¸€æ­¥")
        else:
            merged_data = st.session_state.merged_data
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("### æŒ‰æ•°æ®æ¥æºå‰”é™¤")
                all_sources = merged_data['æ•°æ®æ¥æº'].unique().tolist()
                excluded_sources = st.multiselect("é€‰æ‹©è¦å‰”é™¤çš„æ•°æ®æ¥æº", options=all_sources)

            with col2:
                st.markdown("### æŒ‰æ—¥æœŸå‰”é™¤")
                if 'date' in merged_data.columns:
                    all_dates = sorted(merged_data['date'].unique().tolist())
                    excluded_dates = st.multiselect("é€‰æ‹©è¦å‰”é™¤çš„æ—¥æœŸ", options=all_dates)
                else:
                    st.info("æ•°æ®ä¸­æ— æ—¥æœŸå­—æ®µ")
                    excluded_dates = []

            # è®¡ç®—å‰”é™¤ç»“æœ
            try:
                exclusion_mask = pd.Series([True] * len(merged_data), index=merged_data.index)

                if excluded_sources:
                    source_mask = merged_data['æ•°æ®æ¥æº'].isin(excluded_sources)
                    exclusion_mask &= source_mask

                if 'date' in merged_data.columns and excluded_dates:
                    date_mask = merged_data['date'].isin(excluded_dates)
                    exclusion_mask &= date_mask

                if not excluded_sources and not excluded_dates:
                    exclusion_mask = pd.Series([False] * len(merged_data), index=merged_data.index)

                to_exclude = merged_data[exclusion_mask]
                to_keep = merged_data[~exclusion_mask]

            except Exception as e:
                st.error(f"è®¡ç®—å‰”é™¤æ¡ä»¶æ—¶å‡ºé”™: {str(e)}")
                to_exclude = pd.DataFrame()
                to_keep = merged_data.copy()

            col1, col2 = st.columns(2)
            with col1:
                st.markdown(f"### å°†è¢«å‰”é™¤çš„æ•°æ® ({len(to_exclude)} æ¡)")
                if len(to_exclude) > 0:
                    preview_exclude = optimize_dataframe_for_preview(to_exclude, max_rows=5)
                    st.dataframe(preview_exclude, use_container_width=True)

            with col2:
                st.markdown(f"### ä¿ç•™çš„æ•°æ® ({len(to_keep)} æ¡)")
                if len(to_keep) > 0:
                    preview_keep = optimize_dataframe_for_preview(to_keep, max_rows=5)
                    st.dataframe(preview_keep, use_container_width=True)

            if st.button("ç¡®è®¤å‰”é™¤å¼‚å¸¸æ•°æ®", type="primary", use_container_width=True):
                try:
                    if len(to_exclude) > 0:
                        excluded_dates_info = []
                        for _, row in to_exclude.iterrows():
                            source = row.get('æ•°æ®æ¥æº', 'Unknown')
                            date = row.get('date', 'Unknown')
                            excluded_dates_info.append(f"{source}-{date}")
                        
                        st.session_state.excluded_data = excluded_dates_info
                        st.session_state.excluded_dates_info = excluded_dates
                        st.session_state.cleaned_data = to_keep.copy()
                        st.success(f"æˆåŠŸå‰”é™¤ {len(to_exclude)} æ¡å¼‚å¸¸æ•°æ®")
                    else:
                        st.session_state.cleaned_data = merged_data.copy()
                        st.session_state.excluded_dates_info = []
                        st.info("æœªå‘ç°éœ€è¦å‰”é™¤çš„å¼‚å¸¸æ•°æ®")
                except Exception as e:
                    st.error(f"å‰”é™¤æ•°æ®æ—¶å‡ºé”™: {str(e)}")

        st.markdown('</div>', unsafe_allow_html=True)

    # æ­¥éª¤3ï¼šç•™å­˜ç‡è®¡ç®—
    if st.session_state.merged_data is not None:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("3. ç•™å­˜ç‡è®¡ç®—")
        
        # ä¿®æ”¹è®¡ç®—æ–¹æ³•è¯´æ˜
        st.markdown("""
        <div class="step-tip">
            <div class="step-tip-title">ğŸ“‹ OCPXæ ¼å¼ç•™å­˜ç‡è®¡ç®—æ–¹æ³•</div>
            <div class="step-tip-content">
            <strong>OCPXè¡¨è®¡ç®—æ–¹æ³•ï¼š</strong>ä»¥å¹³å‡æ–°å¢ç”¨æˆ·æ•°ä½œä¸ºåŸºæ•°<br>
            â€¢ è®¡ç®—æ¯ä¸ªæ¸ é“çš„<strong>å›ä¼ æ–°å¢æ•°</strong>çš„å¹³å‡å€¼ä½œä¸ºåŸºæ•°<br>
            â€¢ è®¡ç®—å„å¤©ç•™å­˜åˆ—ï¼ˆ1ã€2ã€3...30ï¼‰çš„å¹³å‡å€¼<br>
            â€¢ ç•™å­˜ç‡ = å„å¤©ç•™å­˜æ•°å¹³å‡å€¼ Ã· å›ä¼ æ–°å¢æ•°å¹³å‡å€¼<br>
            â€¢ é€‚ç”¨äºåŒ…å«1ã€2ã€3...30ç­‰æ•°å­—åˆ—åçš„OCPXç›‘æµ‹è¡¨æ ¼
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        # ç¡®å®šä½¿ç”¨çš„æ•°æ®
        if st.session_state.cleaned_data is not None:
            working_data = st.session_state.cleaned_data
            st.info("ä½¿ç”¨æ¸…ç†åçš„æ•°æ®è¿›è¡Œè®¡ç®—")
        else:
            working_data = st.session_state.merged_data
            st.info("ä½¿ç”¨åŸå§‹æ•°æ®è¿›è¡Œè®¡ç®—")

        data_sources = working_data['æ•°æ®æ¥æº'].unique()
        selected_sources = st.multiselect("é€‰æ‹©è¦åˆ†æçš„æ•°æ®æ¥æº", options=data_sources, default=data_sources)

        if st.button("è®¡ç®—ç•™å­˜ç‡", type="primary", use_container_width=True):
            if selected_sources:
                with st.spinner("æ­£åœ¨è®¡ç®—ç•™å­˜ç‡..."):
                    filtered_data = working_data[working_data['æ•°æ®æ¥æº'].isin(selected_sources)]
                    retention_results = calculate_retention_rates_new_method(filtered_data)
                    st.session_state.retention_data = retention_results

                    st.success("ç•™å­˜ç‡è®¡ç®—å®Œæˆï¼")

                    # åˆ›å»ºç•™å­˜ç‡è¡¨æ ¼æ˜¾ç¤º
                    if retention_results:
                        st.subheader("ç•™å­˜ç‡è¯¦ç»†æ•°æ®")
                        
                        # åˆ›å»ºè¡¨æ ¼æ•°æ®
                        table_data = []
                        days_range = range(1, 31)
                        
                        for day in days_range:
                            row = {'å¤©æ•°': day}
                            for result in retention_results:
                                channel_name = result['data_source']
                                days = result['days']
                                rates = result['rates']
                                
                                # æŸ¥æ‰¾å¯¹åº”å¤©æ•°çš„ç•™å­˜ç‡
                                day_index = np.where(days == day)[0]
                                if len(day_index) > 0:
                                    rate = rates[day_index[0]]
                                    row[channel_name] = f"{rate:.4f}"
                                else:
                                    row[channel_name] = "-"
                            table_data.append(row)
                        
                        # åˆ›å»ºDataFrameå¹¶æ˜¾ç¤º
                        retention_table_df = pd.DataFrame(table_data)
                        
                        # ä½¿ç”¨expanderå±•å¼€è¡¨æ ¼ï¼Œé™åˆ¶é«˜åº¦å¹¶æ˜¾ç¤ºæ»šåŠ¨æ¡
                        with st.expander("ç•™å­˜ç‡æ•°æ®è¡¨ï¼ˆ1-30å¤©ï¼‰", expanded=True):
                            st.dataframe(
                                retention_table_df, 
                                use_container_width=True,
                                height=400  # è®¾ç½®å›ºå®šé«˜åº¦ï¼Œçº¦10è¡Œçš„é«˜åº¦
                            )
            else:
                st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ•°æ®æ¥æº")

        st.markdown('</div>', unsafe_allow_html=True)

    # æ­¥éª¤4ï¼šLTæ‹Ÿåˆåˆ†æ
    if st.session_state.retention_data is not None:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("4. LTæ‹Ÿåˆåˆ†æ")
        
        retention_data = st.session_state.retention_data

        # æ¸ é“è§„åˆ™è¯´æ˜
        st.markdown("""
        <div class="step-tip">
            <div class="step-tip-title">ğŸ“‹ ä¸‰é˜¶æ®µæ‹Ÿåˆè§„åˆ™</div>
            <div class="step-tip-content">
            <strong>ç¬¬ä¸€é˜¶æ®µï¼š</strong>1-30å¤©ï¼Œå¹‚å‡½æ•°æ‹Ÿåˆå®é™…æ•°æ®<br>
            <strong>ç¬¬äºŒé˜¶æ®µï¼š</strong>31-Xå¤©ï¼Œå»¶ç»­å¹‚å‡½æ•°æ¨¡å‹<br>
            <strong>ç¬¬ä¸‰é˜¶æ®µï¼š</strong>Yå¤©åï¼ŒæŒ‡æ•°å‡½æ•°å»ºæ¨¡é•¿æœŸè¡°å‡<br>
            ä¸åŒæ¸ é“é‡‡ç”¨ä¸åŒçš„é˜¶æ®µåˆ’åˆ†ç‚¹
            </div>
        </div>
        """, unsafe_allow_html=True)

        if st.button("å¼€å§‹LTæ‹Ÿåˆåˆ†æ", type="primary", use_container_width=True):
            with st.spinner("æ­£åœ¨è¿›è¡Œæ‹Ÿåˆè®¡ç®—..."):
                lt_results_2y = []
                lt_results_5y = []
                visualization_data_2y = {}
                visualization_data_5y = {}
                original_data = {}
                
                key_days = [1, 7, 30, 60, 90, 100, 150, 200, 300]

                for retention_result in retention_data:
                    channel_name = retention_result['data_source']
                    
                    # è®¡ç®—2å¹´LT
                    lt_result_2y = calculate_lt_advanced(retention_result, channel_name, 2, 
                                                       return_curve_data=True, key_days=key_days)
                    
                    # è®¡ç®—5å¹´LT
                    lt_result_5y = calculate_lt_advanced(retention_result, channel_name, 5, 
                                                       return_curve_data=True, key_days=key_days)

                    lt_results_2y.append({
                        'data_source': channel_name,
                        'lt_value': lt_result_2y['lt_value'],
                        'fit_success': lt_result_2y['success'],
                        'fit_params': lt_result_2y['fit_params'],
                        'power_r2': lt_result_2y['power_r2'],
                        'model_used': lt_result_2y['model_used']
                    })
                    
                    lt_results_5y.append({
                        'data_source': channel_name,
                        'lt_value': lt_result_5y['lt_value'],
                        'fit_success': lt_result_5y['success'],
                        'fit_params': lt_result_5y['fit_params'],
                        'power_r2': lt_result_5y['power_r2'],
                        'model_used': lt_result_5y['model_used']
                    })

                    # ä¿å­˜å¯è§†åŒ–æ•°æ®
                    visualization_data_2y[channel_name] = {
                        "days": lt_result_2y['curve_days'],
                        "rates": lt_result_2y['curve_rates'],
                        "lt": lt_result_2y['lt_value']
                    }
                    
                    visualization_data_5y[channel_name] = {
                        "days": lt_result_5y['curve_days'],
                        "rates": lt_result_5y['curve_rates'],
                        "lt": lt_result_5y['lt_value']
                    }

                    # ä¿å­˜åŸå§‹æ•°æ®
                    original_data[channel_name] = {
                        "days": retention_result['days'],
                        "rates": retention_result['rates']
                    }

                st.session_state.lt_results_2y = lt_results_2y
                st.session_state.lt_results_5y = lt_results_5y
                st.session_state.visualization_data_5y = visualization_data_5y
                st.session_state.original_data = original_data
                st.success("LTæ‹Ÿåˆåˆ†æå®Œæˆï¼")

                # æ˜¾ç¤ºLTå€¼è¡¨æ ¼
                if lt_results_5y:
                    st.subheader("LTåˆ†æç»“æœ")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("#### 2å¹´LTç»“æœ")
                        results_2y_df = pd.DataFrame([
                            {
                                'æ¸ é“åç§°': r['data_source'],
                                '2å¹´LT': round(r['lt_value'], 2),
                                'RÂ²å¾—åˆ†': round(r['power_r2'], 3)
                            }
                            for r in lt_results_2y
                        ])
                        st.dataframe(results_2y_df, use_container_width=True)
                    
                    with col2:
                        st.markdown("#### 5å¹´LTç»“æœ")
                        results_5y_df = pd.DataFrame([
                            {
                                'æ¸ é“åç§°': r['data_source'],
                                '5å¹´LT': round(r['lt_value'], 2),
                                'RÂ²å¾—åˆ†': round(r['power_r2'], 3)
                            }
                            for r in lt_results_5y
                        ])
                        st.dataframe(results_5y_df, use_container_width=True)

                # æ˜¾ç¤ºå•æ¸ é“å›¾è¡¨ - 100å¤©ç‰ˆæœ¬
                if visualization_data_5y and original_data:
                    st.subheader("å„æ¸ é“100å¤©LTæ‹Ÿåˆå›¾è¡¨")
                    
                    # æŒ‰LTå€¼æ’åº
                    sorted_channels = sorted(visualization_data_5y.items(), key=lambda x: x[1]['lt'])
                    
                    # æ¯è¡Œæ˜¾ç¤º2ä¸ªå›¾è¡¨
                    for i in range(0, len(sorted_channels), 2):
                        cols = st.columns(2)
                        for j, col in enumerate(cols):
                            if i + j < len(sorted_channels):
                                channel_name, curve_data = sorted_channels[i + j]
                                with col:
                                    fig = create_individual_channel_chart(
                                        channel_name, curve_data, original_data, max_days=100
                                    )
                                    st.pyplot(fig, use_container_width=True)
                                    plt.close(fig)

        st.markdown('</div>', unsafe_allow_html=True)

elif current_page == "ARPUè®¡ç®—":
    # åŸç†è§£é‡Š
    st.markdown("""
    <div class="principle-box">
        <div class="principle-title">ğŸ“š ARPUè®¡ç®—åŸç†</div>
        <div class="principle-content">
        ARPUï¼ˆAverage Revenue Per Userï¼‰è®¡ç®—åŸºäºç”¨æˆ·æ–°å¢æ•°å’Œæ”¶å…¥æ•°æ®ã€‚ç³»ç»Ÿæ”¯æŒExcelæ–‡ä»¶ä¸Šä¼ ï¼Œæ ¹æ®PIDè¿›è¡Œæ¸ é“åŒ¹é…ï¼Œ
        æŒ‰æœˆä»½ç­›é€‰æ•°æ®ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼šARPU = æ€»æ”¶å…¥ Ã· æ€»æ–°å¢ç”¨æˆ·æ•°ã€‚
        </div>
    </div>
    """, unsafe_allow_html=True)

    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.subheader("ARPUæ•°æ®å¤„ç†")

    # æ·»åŠ æ•°æ®æºé€‰æ‹©
    st.markdown("### æ•°æ®æºé€‰æ‹©")
    data_source_option = st.radio(
        "é€‰æ‹©ARPUæ•°æ®æ¥æºï¼š",
        options=[
            "ä½¿ç”¨å†…ç½®æ•°æ®(2024.1-2025.4) + ä¸Šä¼ æ–°æ•°æ®(2025.5+)", 
            "å®Œå…¨ä¸Šä¼ è‡ªå®šä¹‰æ•°æ®"
        ],
        index=0,
        help="å†…ç½®æ•°æ®åŒ…å«2024å¹´1æœˆåˆ°2025å¹´4æœˆçš„åŸºç¡€ARPUå€¼ï¼Œæ‚¨åªéœ€ä¸Šä¼ 5æœˆåŠä¹‹åçš„æ•°æ®"
    )

    if data_source_option == "ä½¿ç”¨å†…ç½®æ•°æ®(2024.1-2025.4) + ä¸Šä¼ æ–°æ•°æ®(2025.5+)":
        # ä½¿ç”¨å†…ç½®æ•°æ® + æ–°æ•°æ®
        st.markdown("""
        <div class="step-tip">
            <div class="step-tip-title">ğŸ“‹ å†…ç½®æ•°æ® + æ–°æ•°æ®æ¨¡å¼</div>
            <div class="step-tip-content">
            â€¢ ç³»ç»Ÿå†…ç½®2024å¹´1æœˆè‡³2025å¹´4æœˆçš„ARPUåŸºç¡€æ•°æ®<br>
            â€¢ æ‚¨åªéœ€ä¸Šä¼ 2025å¹´5æœˆåŠä¹‹åçš„Excelæ•°æ®<br>
            â€¢ å¿…é¡»åŒ…å«åˆ—ï¼š<strong>æœˆä»½ã€pidã€stat_dateã€instl_user_cntã€ad_all_rven_1d_m</strong><br>
            â€¢ ç³»ç»Ÿè‡ªåŠ¨åˆå¹¶å†…ç½®æ•°æ®å’Œæ–°æ•°æ®è¿›è¡Œè®¡ç®—
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        # æ˜¾ç¤ºå†…ç½®æ•°æ®ä¿¡æ¯
        builtin_df = get_builtin_arpu_data()
        st.info(f"å†…ç½®æ•°æ®åŒ…å« {len(builtin_df)} æ¡è®°å½•ï¼Œè¦†ç›– {builtin_df['æœˆä»½'].nunique()} ä¸ªæœˆä»½")
        
        # ä¸Šä¼ æ–°æ•°æ®æ–‡ä»¶
        new_arpu_file = st.file_uploader(
            "ä¸Šä¼ 2025å¹´5æœˆåŠä¹‹åçš„ARPUæ•°æ® (Excelæ ¼å¼)", 
            type=['xlsx', 'xls'],
            key="new_arpu_file"
        )
        
        if new_arpu_file:
            try:
                combined_df, message = load_user_arpu_data_after_april(new_arpu_file, builtin_df)
                if combined_df is not None:
                    st.success(message)
                    st.info(f"åˆå¹¶åæ•°æ®åŒ…å« {len(combined_df)} æ¡è®°å½•")
                    
                    # æ˜¾ç¤ºåˆå¹¶åæ•°æ®é¢„è§ˆ
                    preview_combined = optimize_dataframe_for_preview(combined_df, max_rows=10)
                    st.dataframe(preview_combined, use_container_width=True)
                    
                    # ä½¿ç”¨åˆå¹¶åçš„æ•°æ®è¿›è¡ŒARPUè®¡ç®—
                    arpu_df = combined_df
                    process_arpu_calculation = True
                else:
                    st.error(message)
                    process_arpu_calculation = False
            except Exception as e:
                st.error(f"å¤„ç†æ–°æ•°æ®æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
                process_arpu_calculation = False
        else:
            # åªä½¿ç”¨å†…ç½®æ•°æ®
            st.info("æœªä¸Šä¼ æ–°æ•°æ®ï¼Œå°†ä»…ä½¿ç”¨å†…ç½®æ•°æ®è®¡ç®—ARPU")
            arpu_df = builtin_df
            process_arpu_calculation = True
    
    else:
        # å®Œå…¨è‡ªå®šä¹‰æ•°æ®

    else:
        # å®Œå…¨è‡ªå®šä¹‰æ•°æ®
        st.markdown("""
        <div class="step-tip">
            <div class="step-tip-title">ğŸ“‹ å®Œå…¨è‡ªå®šä¹‰æ•°æ®æ¨¡å¼</div>
            <div class="step-tip-content">
            â€¢ Excelæ ¼å¼(.xlsx/.xls)<br>
            â€¢ å¿…é¡»åŒ…å«ä»¥ä¸‹åˆ—ï¼š<br>
            &nbsp;&nbsp;- <strong>æœˆä»½</strong>ï¼šæœˆä»½ä¿¡æ¯<br>
            &nbsp;&nbsp;- <strong>pid</strong>ï¼šæ¸ é“å·<br>
            &nbsp;&nbsp;- <strong>stat_date</strong>ï¼šç»Ÿè®¡æ—¥æœŸ<br>
            &nbsp;&nbsp;- <strong>instl_user_cnt</strong>ï¼šæ–°å¢ç”¨æˆ·æ•°<br>
            &nbsp;&nbsp;- <strong>ad_all_rven_1d_m</strong>ï¼šæ”¶å…¥æ•°æ®<br>
            â€¢ æ”¯æŒæŒ‰æœˆä»½ç­›é€‰æ•°æ®
            </div>
        </div>
        """, unsafe_allow_html=True)

        arpu_file = st.file_uploader("é€‰æ‹©ARPUæ•°æ®æ–‡ä»¶ (Excelæ ¼å¼)", type=['xlsx', 'xls'])

        if arpu_file:
            try:
                with st.spinner("æ­£åœ¨è¯»å–ARPUæ–‡ä»¶..."):
                    arpu_df = pd.read_excel(arpu_file)
                st.success("ARPUæ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
                
                # æ£€æŸ¥å¿…éœ€åˆ—
                required_cols = ['pid', 'instl_user_cnt', 'ad_all_rven_1d_m']
                missing_cols = [col for col in required_cols if col not in arpu_df.columns]
                
                if missing_cols:
                    st.error(f"æ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {', '.join(missing_cols)}")
                    st.info("å¯ç”¨åˆ—: " + ", ".join(arpu_df.columns.tolist()))
                    process_arpu_calculation = False
                else:
                    # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                    preview_arpu = optimize_dataframe_for_preview(arpu_df, max_rows=10)
                    st.dataframe(preview_arpu, use_container_width=True)
                    process_arpu_calculation = True
                    
            except Exception as e:
                st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")
                process_arpu_calculation = False
        else:
            st.info("è¯·ä¸Šä¼ ARPUæ•°æ®æ–‡ä»¶")
            process_arpu_calculation = False

    # ç»Ÿä¸€çš„ARPUè®¡ç®—å¤„ç†
    if process_arpu_calculation and 'arpu_df' in locals():
                
    # ç»Ÿä¸€çš„ARPUè®¡ç®—å¤„ç†
    if process_arpu_calculation and 'arpu_df' in locals():
        # æœˆä»½ç­›é€‰ - ä¼˜å…ˆä½¿ç”¨æœˆä»½åˆ—ï¼Œå…¶æ¬¡ä½¿ç”¨stat_dateåˆ—
        st.subheader("æœˆä»½ç­›é€‰")
        
        if 'æœˆä»½' in arpu_df.columns:
            # ä½¿ç”¨æœˆä»½åˆ—
            available_months = sorted(arpu_df['æœˆä»½'].dropna().unique())
            available_months = [str(m) for m in available_months]
            
            if available_months:
                col1, col2 = st.columns(2)
                with col1:
                    start_month = st.selectbox("å¼€å§‹æœˆä»½", options=available_months)
                with col2:
                    end_month = st.selectbox("ç»“æŸæœˆä»½", options=available_months, 
                                           index=len(available_months)-1)
            else:
                st.warning("æœˆä»½åˆ—æ— æœ‰æ•ˆæ•°æ®ï¼Œå°†ä½¿ç”¨æ‰€æœ‰æ•°æ®")
                start_month = end_month = None
                
        elif 'stat_date' in arpu_df.columns:
            # ä½¿ç”¨stat_dateåˆ—
            arpu_df['stat_date'] = pd.to_datetime(arpu_df['stat_date'], errors='coerce')
            arpu_df['month'] = arpu_df['stat_date'].dt.to_period('M')
            available_months = arpu_df['month'].dropna().unique()
            available_months = sorted([str(m) for m in available_months])
            
            if available_months:
                col1, col2 = st.columns(2)
                with col1:
                    start_month = st.selectbox("å¼€å§‹æœˆä»½", options=available_months)
                with col2:
                    end_month = st.selectbox("ç»“æŸæœˆä»½", options=available_months, 
                                           index=len(available_months)-1)
            else:
                st.warning("æ— æ³•è§£æstat_dateæ•°æ®ï¼Œå°†ä½¿ç”¨æ‰€æœ‰æ•°æ®")
                start_month = end_month = None
        else:
            st.info("æœªæ‰¾åˆ°æœˆä»½æˆ–stat_dateåˆ—ï¼Œå°†ä½¿ç”¨æ‰€æœ‰æ•°æ®")
            start_month = end_month = None

        if st.button("è®¡ç®—ARPU", type="primary", use_container_width=True):
            with st.spinner("æ­£åœ¨è®¡ç®—ARPU..."):
                try:
                    # æœˆä»½ç­›é€‰
                    if start_month and end_month:
                        if 'æœˆä»½' in arpu_df.columns:
                            mask = (arpu_df['æœˆä»½'] >= start_month) & (arpu_df['æœˆä»½'] <= end_month)
                        else:
                            mask = (arpu_df['month'] >= start_month) & (arpu_df['month'] <= end_month)
                        filtered_arpu_df = arpu_df[mask].copy()
                        st.info(f"ç­›é€‰æœˆä»½: {start_month} è‡³ {end_month}")
                    else:
                        filtered_arpu_df = arpu_df.copy()
                        st.info("ä½¿ç”¨å…¨éƒ¨æ•°æ®")

                    # ç¡®ä¿pidä¸ºå­—ç¬¦ä¸²æ ¼å¼
                    filtered_arpu_df['pid'] = filtered_arpu_df['pid'].astype(str).str.replace('.0', '', regex=False)
                    
                    # åˆ›å»ºåå‘æ¸ é“æ˜ å°„
                    reverse_mapping = create_reverse_mapping(st.session_state.channel_mapping)
                    
                    # æŒ‰æ¸ é“åŒ¹é…å’Œæ±‡æ€»
                    arpu_results = []
                    
                    for pid, group in filtered_arpu_df.groupby('pid'):
                        if pid in reverse_mapping:
                            channel_name = reverse_mapping[pid]
                            total_users = group['instl_user_cnt'].sum()
                            total_revenue = group['ad_all_rven_1d_m'].sum()
                            
                            if total_users > 0:
                                arpu_value = total_revenue / total_users
                                arpu_results.append({
                                    'data_source': channel_name,
                                    'total_users': total_users,
                                    'total_revenue': total_revenue,
                                    'arpu_value': arpu_value,
                                    'record_count': len(group)
                                })

                    if arpu_results:
                        # æŒ‰æ¸ é“åˆå¹¶ç›¸åŒæ¸ é“çš„æ•°æ®
                        final_arpu = {}
                        for result in arpu_results:
                            channel = result['data_source']
                            if channel in final_arpu:
                                final_arpu[channel]['total_users'] += result['total_users']
                                final_arpu[channel]['total_revenue'] += result['total_revenue']
                                final_arpu[channel]['record_count'] += result['record_count']
                            else:
                                final_arpu[channel] = result.copy()
                        
                        # é‡æ–°è®¡ç®—ARPU
                        arpu_summary = []
                        for channel, data in final_arpu.items():
                            arpu_value = data['total_revenue'] / data['total_users'] if data['total_users'] > 0 else 0
                            arpu_summary.append({
                                'data_source': channel,
                                'arpu_value': arpu_value,
                                'record_count': data['record_count']
                            })
                        
                        arpu_summary_df = pd.DataFrame(arpu_summary)
                        st.session_state.arpu_data = arpu_summary_df
                        st.success("ARPUè®¡ç®—å®Œæˆï¼")
                        
                        # æ˜¾ç¤ºç»“æœ
                        display_arpu_df = arpu_summary_df.copy()
                        display_arpu_df['ARPU'] = display_arpu_df['arpu_value'].round(4)
                        display_arpu_df = display_arpu_df[['data_source', 'ARPU', 'record_count']]
                        display_arpu_df.columns = ['æ¸ é“åç§°', 'ARPUå€¼', 'è®°å½•æ•°']
                        st.dataframe(display_arpu_df, use_container_width=True)
                    else:
                        st.error("æœªæ‰¾åˆ°åŒ¹é…çš„æ¸ é“æ•°æ®")

                except Exception as e:
                    st.error(f"ARPUè®¡ç®—å¤±è´¥ï¼š{str(e)}")

    st.markdown('</div>', unsafe_allow_html=True)

    # æ‰‹åŠ¨è®¾ç½®ARPUï¼ˆæŒ‰éœ€æ˜¾ç¤ºï¼‰
    if st.session_state.lt_results_5y:
        if not st.session_state.show_manual_arpu:
            if st.button("æ‰‹åŠ¨è®¾ç½®ARPUå€¼"):
                st.session_state.show_manual_arpu = True
                st.rerun()
        else:
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            st.subheader("æ‰‹åŠ¨è®¾ç½®ARPUå€¼")
            
            arpu_inputs = {}
            
            col1, col2 = st.columns(2)
            for i, result in enumerate(st.session_state.lt_results_5y):
                source = result['data_source']
                with col1 if i % 2 == 0 else col2:
                    arpu_value = st.number_input(
                        f"{source}", min_value=0.0, value=0.04, step=0.001,
                        format="%.4f", key=f"manual_arpu_{source}"
                    )
                    arpu_inputs[source] = arpu_value

            if st.button("ä¿å­˜æ‰‹åŠ¨ARPUè®¾ç½®", type="primary", use_container_width=True):
                arpu_df = pd.DataFrame([
                    {'data_source': source, 'arpu_value': value, 'record_count': 1}
                    for source, value in arpu_inputs.items()
                ])
                st.session_state.arpu_data = arpu_df
                st.success("ARPUè®¾ç½®å·²ä¿å­˜ï¼")
                st.dataframe(arpu_df[['data_source', 'arpu_value']], use_container_width=True)
            
            st.markdown('</div>', unsafe_allow_html=True)

    st.markdown('</div>', unsafe_allow_html=True)

elif current_page == "LTVç»“æœæŠ¥å‘Š":
    # åŸç†è§£é‡Š
    st.markdown("""
    <div class="principle-box">
        <div class="principle-title">ğŸ“š LTVç»“æœæŠ¥å‘Š</div>
        <div class="principle-content">
        LTVç»“æœæŠ¥å‘Šæ•´åˆäº†LTæ‹Ÿåˆåˆ†æå’ŒARPUè®¡ç®—çš„ç»“æœï¼Œé€šè¿‡LTV = LT Ã— ARPUå…¬å¼è®¡ç®—æœ€ç»ˆçš„ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼ã€‚
        æŠ¥å‘Šæä¾›2å¹´å’Œ5å¹´åŒæ®µå¯¹æ¯”ï¼ŒåŒ…å«è¯¦ç»†çš„æ‹Ÿåˆæ¨¡å‹ä¿¡æ¯å’Œè´¨é‡è¯„ä¼°ã€‚
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    if st.session_state.lt_results_5y is not None and st.session_state.arpu_data is not None:
        lt_results_2y = st.session_state.lt_results_2y
        lt_results_5y = st.session_state.lt_results_5y
        arpu_data = st.session_state.arpu_data

        # è®¡ç®—LTVç»“æœ
        ltv_results = []
        
        for lt_result_5y in lt_results_5y:
            source = lt_result_5y['data_source']
            
            # æŸ¥æ‰¾å¯¹åº”çš„2å¹´LTæ•°æ®
            lt_result_2y = next((r for r in lt_results_2y if r['data_source'] == source), None)
            
            # æŸ¥æ‰¾ARPUæ•°æ®
            arpu_row = arpu_data[arpu_data['data_source'] == source]
            if not arpu_row.empty:
                arpu_value = arpu_row.iloc[0]['arpu_value']
            else:
                arpu_value = 0
                st.warning(f"æ¸ é“ '{source}' æœªæ‰¾åˆ°ARPUæ•°æ®")

            ltv_5y = lt_result_5y['lt_value'] * arpu_value
            ltv_2y = lt_result_2y['lt_value'] * arpu_value if lt_result_2y else 0

            ltv_results.append({
                'data_source': source,
                'lt_2y': lt_result_2y['lt_value'] if lt_result_2y else 0,
                'lt_5y': lt_result_5y['lt_value'],
                'arpu_value': arpu_value,
                'ltv_2y': ltv_2y,
                'ltv_5y': ltv_5y,
                'fit_success': lt_result_5y['fit_success'],
                'model_used': lt_result_5y.get('model_used', 'unknown'),
                'power_params': lt_result_5y.get('fit_params', {}).get('power', {}),
                'exp_params': lt_result_5y.get('fit_params', {}).get('exponential', {})
            })

        st.session_state.ltv_results = ltv_results

        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("LTVç»¼åˆè®¡ç®—ç»“æœ")

        # åˆ›å»ºå®Œæ•´çš„ç»“æœè¡¨æ ¼
        display_data = []
        for result in ltv_results:
            # è·å–æ‹Ÿåˆå‚æ•°ä¿¡æ¯ - ä¿®æ”¹å¤‡æ³¨æ¢è¡Œ
            power_params = result['power_params']
            exp_params = result['exp_params']
            
            å¤‡æ³¨_parts = []
            if power_params:
                power_func = f"å¹‚å‡½æ•°: y = {power_params.get('a', 0):.4f} * x^{power_params.get('b', 0):.4f}"
                å¤‡æ³¨_parts.append(power_func)
            
            if exp_params:
                exp_func = f"æŒ‡æ•°å‡½æ•°: y = {exp_params.get('c', 0):.4f} * exp({exp_params.get('d', 0):.4f} * x)"
                å¤‡æ³¨_parts.append(exp_func)
            
            å¤‡æ³¨ = "\n".join(å¤‡æ³¨_parts) if å¤‡æ³¨_parts else "æœªçŸ¥"
            
            display_data.append({
                'æ¸ é“åç§°': result['data_source'],
                '5å¹´LT': round(result['lt_5y'], 2),
                '5å¹´ARPU': round(result['arpu_value'], 4),
                '5å¹´LTV': round(result['ltv_5y'], 2),
                '2å¹´LT': round(result['lt_2y'], 2),
                '2å¹´ARPU': round(result['arpu_value'], 4),
                '2å¹´LTV': round(result['ltv_2y'], 2),
                'å¤‡æ³¨': å¤‡æ³¨
            })

        results_df = pd.DataFrame(display_data)
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        column_order = ['æ¸ é“åç§°', '5å¹´LT', '5å¹´ARPU', '5å¹´LTV', '2å¹´LT', '2å¹´ARPU', '2å¹´LTV', 'å¤‡æ³¨']
        results_df = results_df[column_order]
        
        st.dataframe(results_df, use_container_width=True, height=600)
        st.markdown('</div>', unsafe_allow_html=True)

        # æ˜¾ç¤ºæ‰€æœ‰æ‹Ÿåˆæ›²çº¿ - ä¸€è¡Œå››ä¸ª
        if st.session_state.visualization_data_5y and st.session_state.original_data:
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            st.subheader("æ‰€æœ‰æ¸ é“æ‹Ÿåˆæ›²çº¿ï¼ˆ100å¤©ï¼‰")
            
            visualization_data_5y = st.session_state.visualization_data_5y
            original_data = st.session_state.original_data
            
            # æŒ‰LTå€¼æ’åº
            sorted_channels = sorted(visualization_data_5y.items(), key=lambda x: x[1]['lt'])
            
            # æ¯è¡Œæ˜¾ç¤º4ä¸ªå›¾è¡¨
            for i in range(0, len(sorted_channels), 4):
                cols = st.columns(4)
                for j, col in enumerate(cols):
                    if i + j < len(sorted_channels):
                        channel_name, curve_data = sorted_channels[i + j]
                        with col:
                            fig = create_individual_channel_chart(
                                channel_name, curve_data, original_data, max_days=100
                            )
                            st.pyplot(fig, use_container_width=True)
                            plt.close(fig)
            
            st.markdown('</div>', unsafe_allow_html=True)

        # æ•°æ®å¯¼å‡º
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("åˆ†æç»“æœå¯¼å‡º")

        col1, col2 = st.columns(2)

        with col1:
            # CSVå¯¼å‡º
            csv_data = results_df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ä¸‹è½½LTVåˆ†æç»“æœ (CSV)",
                data=csv_data.encode('utf-8-sig'),
                file_name=f"LTV_Analysis_Results_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                mime="text/csv",
                use_container_width=True
            )

        with col2:
            # è¯¦ç»†æŠ¥å‘Š
            data_source_desc = ""
            if st.session_state.excluded_dates_info and len(st.session_state.excluded_dates_info) > 0:
                excluded_dates_str = ", ".join(st.session_state.excluded_dates_info)
                data_source_desc = f"å·²å‰”é™¤ä»¥ä¸‹æ—¥æœŸæ•°æ®ï¼š{excluded_dates_str}"
            elif st.session_state.cleaned_data is not None:
                data_source_desc = "ä½¿ç”¨æ¸…ç†åæ•°æ®"
            else:
                data_source_desc = "ä½¿ç”¨åŸå§‹æ•°æ®"
            
            report_text = f"""
LTVç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†ææŠ¥å‘Š
===========================================
ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

æ‰§è¡Œæ‘˜è¦
-----------
æœ¬æŠ¥å‘ŠåŸºäºä¸‰é˜¶æ®µåˆ†å±‚æ•°å­¦å»ºæ¨¡æ–¹æ³•ï¼Œå¯¹ {len(results_df)} ä¸ªæ¸ é“è¿›è¡Œäº†ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†æã€‚

æ ¸å¿ƒæŒ‡æ ‡æ±‡æ€»
-----------
â€¢ å‚ä¸åˆ†æçš„æ¸ é“æ•°é‡: {len(results_df)}
â€¢ 5å¹´å¹³å‡LTV: {results_df['5å¹´LTV'].mean():.2f}
â€¢ 5å¹´æœ€é«˜LTV: {results_df['5å¹´LTV'].max():.2f}
â€¢ 5å¹´æœ€ä½LTV: {results_df['5å¹´LTV'].min():.2f}
â€¢ 2å¹´å¹³å‡LTV: {results_df['2å¹´LTV'].mean():.2f}
â€¢ å¹³å‡5å¹´LTå€¼: {results_df['5å¹´LT'].mean():.2f} å¤©
â€¢ å¹³å‡2å¹´LTå€¼: {results_df['2å¹´LT'].mean():.2f} å¤©
â€¢ å¹³å‡ARPU: {results_df['5å¹´ARPU'].mean():.4f}

è¯¦ç»†ç»“æœ
-----------
{results_df[['æ¸ é“åç§°', '5å¹´LT', '5å¹´ARPU', '5å¹´LTV', '2å¹´LT', '2å¹´ARPU', '2å¹´LTV']].to_string(index=False)}

æ•°æ®æ¥æºè¯´æ˜
-----------
{data_source_desc}

è®¡ç®—æ–¹æ³•
-----------
â€¢ LTæ‹Ÿåˆ: ä¸‰é˜¶æ®µåˆ†å±‚å»ºæ¨¡ï¼ˆ1-30å¤©å¹‚å‡½æ•° + 31-Xå¤©å¹‚å‡½æ•°å»¶ç»­ + Yå¤©åæŒ‡æ•°å‡½æ•°ï¼‰
â€¢ LTVå…¬å¼: LTV = LT Ã— ARPU
â€¢ æ¸ é“è§„åˆ™: æŒ‰åä¸ºã€å°ç±³ã€OPPOã€vivoã€iPhoneåˆ†ç±»è®¾å®šä¸åŒæ‹Ÿåˆå‚æ•°
â€¢ ç•™å­˜ç‡è®¡ç®—: OCPXæ ¼å¼è¡¨å„å¤©ç•™å­˜åˆ—ï¼ˆ1ã€2ã€3...ï¼‰å¹³å‡å€¼Ã·å›ä¼ æ–°å¢æ•°å¹³å‡å€¼

æŠ¥å‘Šç”Ÿæˆ: LTVæ™ºèƒ½åˆ†æå¹³å° v3.3
"""

            st.download_button(
                label="ä¸‹è½½è¯¦ç»†åˆ†ææŠ¥å‘Š (TXT)",
                data=report_text.encode('utf-8'),
                file_name=f"LTV_Detailed_Report_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.txt",
                mime="text/plain",
                use_container_width=True
            )

        st.markdown('</div>', unsafe_allow_html=True)
    else:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        missing_components = []
        if st.session_state.lt_results_5y is None:
            missing_components.append("LTæ‹Ÿåˆåˆ†æ")
        if st.session_state.arpu_data is None:
            missing_components.append("ARPUè®¡ç®—")
        
        st.info(f"è¯·å…ˆå®Œæˆï¼š{', '.join(missing_components)}")
        st.markdown('</div>', unsafe_allow_html=True)

# ==================== åº•éƒ¨ä¿¡æ¯ ====================
with st.sidebar:
    st.markdown("---")
    st.markdown("""
    <div class="nav-container">
        <h4 style="text-align: center; color: white;">ä½¿ç”¨æŒ‡å—</h4>
        <p style="font-size: 0.9rem; color: rgba(255,255,255,0.9); text-align: center;">
        æŒ‰æ­¥éª¤å®Œæˆåˆ†ææµç¨‹ï¼Œæ¯æ­¥éƒ½æœ‰è¯¦ç»†æŒ‡å¯¼ã€‚
        </p>
        <p style="font-size: 0.8rem; color: rgba(255,255,255,0.7); text-align: center;">
        LTVæ™ºèƒ½åˆ†æå¹³å° v3.3<br>
        åŸºäºä¸‰é˜¶æ®µæ•°å­¦å»ºæ¨¡
        </p>
    </div>
    """, unsafe_allow_html=True)
