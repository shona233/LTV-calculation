import streamlit as st
import os
import pandas as pd
import numpy as np
from pathlib import Path
import warnings
import datetime
import tempfile
import zipfile
import io
import matplotlib.pyplot as plt
import matplotlib as mpl
import re
from matplotlib.font_manager import FontProperties
import seaborn as sns
from scipy.optimize import curve_fit

# ==================== åŸºç¡€é…ç½® ====================
# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl.styles.stylesheet')
warnings.filterwarnings('ignore', category=UserWarning,
                        message="Could not infer format, so each element will be parsed individually")

# è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="LTV Analytics Platform",
    layout="wide",
    initial_sidebar_state="expanded",
    page_icon="ğŸ“Š"
)

# ==================== CSS æ ·å¼å®šä¹‰ ====================
# ç®€æ´CSSæ ·å¼
st.markdown("""
<style>
    /* å…¨å±€æ ·å¼ */
    .main {
        background: #f8f9fa;
        min-height: 100vh;
    }

    .block-container {
        padding: 1rem 1rem 3rem 1rem;
        max-width: 100%;
        background: transparent;
    }

    /* ä¸»æ ‡é¢˜åŒºåŸŸ */
    .main-header {
        background: white;
        padding: 1.2rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        border: 1px solid #e9ecef;
        margin-bottom: 1.5rem;
        text-align: center;
    }

    .main-title {
        font-size: 1.8rem;
        font-weight: 700;
        color: #2c3e50;
        margin-bottom: 0.3rem;
    }

    .main-subtitle {
        color: #6c757d;
        font-size: 1.1rem;
        font-weight: 400;
    }

    /* å¡ç‰‡æ ·å¼ */
    .glass-card {
        background: white;
        border-radius: 8px;
        padding: 1rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        border: 1px solid #e9ecef;
        margin-bottom: 1rem;
    }

    /* åˆ†ç•Œçº¿ */
    .section-divider {
        height: 1px;
        background: #dee2e6;
        margin: 1rem 0;
    }

    /* æŒ‡æ ‡å¡ç‰‡ */
    .metric-card {
        background: #f8f9fa;
        color: #495057;
        padding: 1rem;
        border-radius: 8px;
        text-align: center;
        border: 1px solid #dee2e6;
        margin-bottom: 0.8rem;
    }

    .metric-value {
        font-size: 2rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        color: #2c3e50;
    }

    .metric-label {
        font-size: 0.9rem;
        color: #6c757d;
    }

    /* çŠ¶æ€å¡ç‰‡ */
    .status-card {
        background: white;
        border-radius: 8px;
        padding: 1rem;
        border-left: 4px solid #28a745;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        margin-bottom: 0.8rem;
    }

    /* å¯¼èˆªæ­¥éª¤æ ·å¼ */
    .nav-container {
        background: white;
        border-radius: 8px;
        padding: 1rem;
        margin-bottom: 1rem;
        border: 1px solid #e9ecef;
    }

    /* æŒ‰é’®æ ·å¼ */
    .stButton > button {
        background: #28a745;
        color: white;
        border: none;
        border-radius: 6px;
        padding: 0.5rem 2rem;
        font-weight: 600;
        transition: all 0.3s ease;
    }

    .stButton > button:hover {
        background: #218838;
        transform: translateY(-1px);
    }

    /* é€‰æ‹©æ¡†æ ·å¼ */
    .stSelectbox label, .stMultiselect label, .stFileUploader label {
        font-weight: 600;
        color: #495057;
        margin-bottom: 0.5rem;
    }

    /* æ ‡é¢˜æ ·å¼ */
    h1, h2, h3, h4 {
        color: #2c3e50;
        font-weight: 600;
        font-size: 1.1rem !important;
    }

    /* è¯´æ˜æ–‡å­—æ ·å¼ */
    .step-explanation {
        background: #f8f9fa;
        border-left: 4px solid #28a745;
        padding: 1.5rem;
        margin-top: 2rem;
        border-radius: 0 8px 8px 0;
    }

    .step-explanation h4 {
        color: #2c3e50;
        margin-bottom: 0.8rem;
        font-size: 1.1rem;
        font-weight: 700;
    }

    .step-explanation ul {
        margin: 0.5rem 0;
        padding-left: 1.5rem;
        list-style-type: disc;
    }

    .step-explanation li {
        margin-bottom: 0.5rem;
        color: #495057;
        line-height: 1.5;
    }

    .step-explanation strong {
        color: #2c3e50;
        font-weight: 600;
    }

    /* éšè—é»˜è®¤çš„Streamlitå…ƒç´  */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# ==================== é»˜è®¤é…ç½®æ•°æ® ====================
# é»˜è®¤æ¸ é“æ˜ å°„æ•°æ®
DEFAULT_CHANNEL_MAPPING = {
    '9000': 'æ€»ä½“',
    '500345': 'æ–°åª’ä½“', '500346': 'æ–°åª’ä½“', '500447': 'æ–°åª’ä½“', '500449': 'æ–°åª’ä½“',
    '500450': 'æ–°åª’ä½“', '500531': 'æ–°åª’ä½“', '500542': 'æ–°åª’ä½“',
    '5007XS': 'åº”ç”¨å®', '500349': 'åº”ç”¨å®', '500350': 'åº”ç”¨å®',
    '500285': 'é¼ä¹-ç››ä¸–6', '500286': 'é¼ä¹-ç››ä¸–7',
    '5108': 'é…·æ´¾', '5528': 'é…·æ´¾',
    '500275': 'æ–°ç¾-åŒ—äº¬2', '500274': 'æ–°ç¾-åŒ—äº¬1',
    '500316': 'A_æ·±åœ³è›‹ä¸2',
    '500297': 'è£è€€', '5057': 'åä¸º', '5237': 'vivo', '5599': 'å°ç±³', '5115': 'OPPO',
    '500471': 'ç½‘æ˜“', '500480': 'ç½‘æ˜“', '500481': 'ç½‘æ˜“', '500482': 'ç½‘æ˜“',
    '500337': 'åä¸ºéå•†åº—-å“ä¼—', '500338': 'åä¸ºéå•†åº—-å“ä¼—', '500343': 'åä¸ºéå•†åº—-å“ä¼—', 
    '500445': 'åä¸ºéå•†åº—-å“ä¼—', '500383': 'åä¸ºéå•†åº—-å“ä¼—', '500444': 'åä¸ºéå•†åº—-å“ä¼—', '500441': 'åä¸ºéå•†åº—-å“ä¼—',
    '5072': 'é­…æ—',
    '500287': 'OPPOéå•†åº—', '500288': 'OPPOéå•†åº—',
    '5187': 'vivoéå•†åº—',
    '500398': 'ç™¾åº¦sem--ç™¾åº¦æ—¶ä»£å®‰å“', '500400': 'ç™¾åº¦sem--ç™¾åº¦æ—¶ä»£å®‰å“', '500404': 'ç™¾åº¦sem--ç™¾åº¦æ—¶ä»£å®‰å“',
    '500402': 'ç™¾åº¦sem--ç™¾åº¦æ—¶ä»£ios', '500403': 'ç™¾åº¦sem--ç™¾åº¦æ—¶ä»£ios', '500405': 'ç™¾åº¦sem--ç™¾åº¦æ—¶ä»£ios',
    '500377': 'ç™¾é’è—¤-å®‰å“', '500379': 'ç™¾é’è—¤-å®‰å“', '500435': 'ç™¾é’è—¤-å®‰å“', '500436': 'ç™¾é’è—¤-å®‰å“', 
    '500490': 'ç™¾é’è—¤-å®‰å“', '500491': 'ç™¾é’è—¤-å®‰å“', '500434': 'ç™¾é’è—¤-å®‰å“', '500492': 'ç™¾é’è—¤-å®‰å“',
    '500437': 'ç™¾é’è—¤-ios',
    '500170': 'å°ç±³éå•†åº—',
    '500532': 'åä¸ºéå•†åº—-æ˜Ÿç«', '500533': 'åä¸ºéå•†åº—-æ˜Ÿç«', '500534': 'åä¸ºéå•†åº—-æ˜Ÿç«', '500537': 'åä¸ºéå•†åº—-æ˜Ÿç«',
    '500538': 'åä¸ºéå•†åº—-æ˜Ÿç«', '500539': 'åä¸ºéå•†åº—-æ˜Ÿç«', '500540': 'åä¸ºéå•†åº—-æ˜Ÿç«', '500541': 'åä¸ºéå•†åº—-æ˜Ÿç«',
    '500504': 'å¾®åš-èœœæ©˜', '500505': 'å¾®åš-èœœæ©˜',
    '500367': 'å¾®åš-å¤®å¹¿', '500368': 'å¾®åš-å¤®å¹¿', '500369': 'å¾®åš-å¤®å¹¿',
    '500498': 'å¹¿ç‚¹é€šï¼ˆ5.22èµ·ï¼‰', '500497': 'å¹¿ç‚¹é€šï¼ˆ5.22èµ·ï¼‰', '500500': 'å¹¿ç‚¹é€šï¼ˆ5.22èµ·ï¼‰', 
    '500501': 'å¹¿ç‚¹é€šï¼ˆ5.22èµ·ï¼‰', '500496': 'å¹¿ç‚¹é€šï¼ˆ5.22èµ·ï¼‰', '500499': 'å¹¿ç‚¹é€šï¼ˆ5.22èµ·ï¼‰',
    '500514': 'ç½‘æ˜“æ˜“æ•ˆ', '500515': 'ç½‘æ˜“æ˜“æ•ˆ', '500516': 'ç½‘æ˜“æ˜“æ•ˆ'
}

# ==================== æ—¥æœŸå¤„ç†å‡½æ•° ====================
# è®¡ç®—é»˜è®¤ç›®æ ‡æœˆä»½ï¼ˆ2ä¸ªæœˆå‰ï¼‰
def get_default_target_month():
    today = datetime.datetime.now()
    if today.month <= 2:
        target_year = today.year - 1
        target_month = today.month + 10
    else:
        target_year = today.year
        target_month = today.month - 2
    return f"{target_year}-{target_month:02d}"

# ==================== æ•°æ®ç±»å‹è½¬æ¢å‡½æ•° ====================
def safe_convert_to_numeric(value):
    """
    å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
    """
    if pd.isna(value) or value == '' or value is None:
        return 0
    try:
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå…ˆå»é™¤ç©ºæ ¼
        if isinstance(value, str):
            value = value.strip()
            if value == '' or value.lower() in ['nan', 'null', 'none']:
                return 0
        return pd.to_numeric(value, errors='coerce')
    except:
        return 0

# ==================== æ•°æ®ç»“æ„æ ‡å‡†åŒ–å‡½æ•° ====================
def standardize_output_columns(df):
    """
    æ ‡å‡†åŒ–è¾“å‡ºåˆ—ç»“æ„ï¼Œç¡®ä¿åŒ…å«æŒ‡å®šçš„åˆ—é¡ºåºï¼Œå¹¶æ­£ç¡®å¤„ç†æ•°æ®ç±»å‹
    """
    print("æ­£åœ¨æ ‡å‡†åŒ–è¾“å‡ºåˆ—ç»“æ„...")

    # å®šä¹‰ç›®æ ‡åˆ—é¡ºåº
    target_columns = [
        'æ•°æ®æ¥æº', 'date', 'æ•°æ®æ¥æº_date',
        '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',
        '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',
        '21', '22', '23', '24', '25', '26', '27', '28', '29', '30',
        'æ•°æ®æ¥æº_æ—¥æœŸ', 'æ—¥æœŸ', 'å›ä¼ æ–°å¢æ•°', 'is_target_month', 'month', 'stat_date'
    ]

    # æŒ‰ç›®æ ‡åˆ—é¡ºåºåˆ›å»ºç»“æœDataFrame
    result_df = pd.DataFrame()

    # æŒ‰ç²¾ç¡®é¡ºåºæ·»åŠ æ¯ä¸€åˆ—
    for col_name in target_columns:
        if col_name == 'æ•°æ®æ¥æº':
            result_df[col_name] = df[col_name].astype(str) if col_name in df.columns else ''
        elif col_name == 'date':
            if col_name in df.columns:
                result_df[col_name] = df[col_name].astype(str)
            elif 'stat_date' in df.columns:
                result_df[col_name] = df['stat_date'].astype(str)
            else:
                result_df[col_name] = ''
        elif col_name == 'æ•°æ®æ¥æº_date':
            # åˆ›å»ºæ•°æ®æ¥æº_dateåˆ—
            data_source = df['æ•°æ®æ¥æº'].astype(str) if 'æ•°æ®æ¥æº' in df.columns else pd.Series([''] * len(df))
            
            if 'date' in df.columns:
                date_col_str = df['date'].astype(str)
            elif 'stat_date' in df.columns:
                date_col_str = df['stat_date'].astype(str)
            else:
                date_col_str = pd.Series([''] * len(df))

            result_df[col_name] = data_source + date_col_str
        elif col_name == 'æ•°æ®æ¥æº_æ—¥æœŸ':
            # åˆ›å»ºæ•°æ®æ¥æº_æ—¥æœŸåˆ—
            data_source = df['æ•°æ®æ¥æº'].astype(str) if 'æ•°æ®æ¥æº' in df.columns else pd.Series([''] * len(df))

            if 'æ—¥æœŸ' in df.columns:
                date_col_str = df['æ—¥æœŸ'].astype(str)
            elif 'date' in df.columns:
                date_col_str = df['date'].astype(str)
            elif 'stat_date' in df.columns:
                date_col_str = df['stat_date'].astype(str)
            else:
                date_col_str = pd.Series([''] * len(df))

            result_df[col_name] = data_source + date_col_str
        elif col_name in ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10',
                          '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',
                          '21', '22', '23', '24', '25', '26', '27', '28', '29', '30']:
            # æ•°å­—åˆ—ï¼šç¡®ä¿è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            if col_name in df.columns:
                result_df[col_name] = df[col_name].apply(safe_convert_to_numeric)
            else:
                result_df[col_name] = 0
        elif col_name == 'å›ä¼ æ–°å¢æ•°':
            # å›ä¼ æ–°å¢æ•°åˆ—ï¼šç¡®ä¿è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            if col_name in df.columns:
                result_df[col_name] = df[col_name].apply(safe_convert_to_numeric)
            else:
                result_df[col_name] = 0
        else:
            # å…¶ä»–åˆ—ç›´æ¥å¤åˆ¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å¡«ç©º
            if col_name in df.columns:
                result_df[col_name] = df[col_name]
            else:
                result_df[col_name] = ''

    # æ·»åŠ åŸæ•°æ®ä¸­å­˜åœ¨ä½†ä¸åœ¨ç›®æ ‡åˆ—è¡¨ä¸­çš„å…¶ä»–åˆ—ï¼ˆä¿æŒåœ¨æœ€åï¼‰
    for col in df.columns:
        if col not in target_columns:
            result_df[col] = df[col]

    print(f"å·²æ ‡å‡†åŒ–è¾“å‡ºåˆ—ç»“æ„ï¼Œå…± {len(result_df.columns)} åˆ—ï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—")
    return result_df

# ==================== æ¸ é“æ˜ å°„å¤„ç†å‡½æ•° ====================
def parse_channel_mapping_from_excel(channel_file):
    """
    ä»ä¸Šä¼ çš„Excelæ–‡ä»¶è§£ææ¸ é“æ˜ å°„
    """
    try:
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(channel_file)
        
        pid_to_channel = {}
        
        # éå†æ¯ä¸€è¡Œ
        for _, row in df.iterrows():
            # ç¬¬ä¸€åˆ—æ˜¯æ¸ é“åç§°
            channel_name = str(row.iloc[0]).strip()
            if pd.isna(channel_name) or channel_name == '' or channel_name == 'nan':
                continue
                
            # ä»ç¬¬äºŒåˆ—å¼€å§‹æ˜¯æ¸ é“å·
            for col_idx in range(1, len(row)):
                pid = row.iloc[col_idx]
                if pd.isna(pid) or str(pid).strip() in ['', 'nan', 'ã€€', ' ']:
                    continue
                pid_str = str(pid).strip()
                if pid_str:
                    pid_to_channel[pid_str] = channel_name
                    
        return pid_to_channel
    except Exception as e:
        st.error(f"è§£ææ¸ é“æ˜ å°„æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
        return {}

# ==================== æ–‡ä»¶æ•´åˆæ ¸å¿ƒå‡½æ•° ====================
def integrate_excel_files_streamlit(uploaded_files, target_month=None, channel_mapping=None):
    """
    æ•´åˆä¸Šä¼ çš„Excelæ–‡ä»¶ï¼Œæ”¯æŒæ–°æ ¼å¼è¡¨å’Œä¼ ç»Ÿæ ¼å¼è¡¨
    """
    if target_month is None:
        target_month = get_default_target_month()

    all_data = pd.DataFrame()
    processed_count = 0
    mapping_warnings = []

    for uploaded_file in uploaded_files:
        source_name = os.path.splitext(uploaded_file.name)[0]

        # æ¸ é“æ˜ å°„å¤„ç†
        if channel_mapping and source_name in channel_mapping:
            mapped_source = channel_mapping[source_name]
        else:
            mapped_source = source_name
            if channel_mapping:
                mapping_warnings.append(f"æ–‡ä»¶ '{source_name}' æœªåœ¨æ¸ é“æ˜ å°„è¡¨ä¸­æ‰¾åˆ°å¯¹åº”é¡¹")

        try:
            # è¯»å–Excelæ–‡ä»¶
            xls = pd.ExcelFile(uploaded_file)
            sheet_names = xls.sheet_names

            # æŸ¥æ‰¾ç›®æ ‡å·¥ä½œè¡¨
            ocpx_sheet = None
            for sheet in sheet_names:
                if "ocpxç›‘æµ‹ç•™å­˜æ•°" in sheet:
                    ocpx_sheet = sheet
                    break

            if ocpx_sheet:
                file_data = pd.read_excel(uploaded_file, sheet_name=ocpx_sheet)
            else:
                file_data = pd.read_excel(uploaded_file, sheet_name=0)

            if file_data is not None and not file_data.empty:
                file_data_copy = file_data.copy()

                # ========== æ£€æµ‹æ–‡ä»¶æ ¼å¼ç±»å‹ ==========
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°æ ¼å¼è¡¨ï¼ˆåŒ…å«stat_dateå’Œç•™å­˜åˆ—ï¼‰
                is_new_format = False
                has_stat_date = 'stat_date' in file_data_copy.columns
                retain_columns = [f'new_retain_{i}' for i in range(1, 31)]
                has_retain_columns = any(col in file_data_copy.columns for col in retain_columns)

                # ========== å¤„ç†æ–°æ ¼å¼è¡¨ ==========
                if has_stat_date and has_retain_columns:
                    is_new_format = True
                    print(f"æ£€æµ‹åˆ° {uploaded_file.name} æ˜¯æ–°æ ¼å¼è¡¨ï¼ˆå«stat_dateå’Œç•™å­˜åˆ—ï¼‰")

                    # åˆ›å»ºæ ‡å‡†çš„è¾“å‡ºç»“æ„
                    standardized_data = file_data_copy.copy()

                    # å°†newåˆ—çš„å€¼æ˜ å°„åˆ°"å›ä¼ æ–°å¢æ•°"åˆ—ï¼Œç¡®ä¿æ˜¯æ•°å€¼ç±»å‹
                    if 'new' in standardized_data.columns:
                        standardized_data['å›ä¼ æ–°å¢æ•°'] = standardized_data['new'].apply(safe_convert_to_numeric)

                    # å°†new_retain_1åˆ°new_retain_30çš„å€¼åˆ†åˆ«æ˜ å°„åˆ°æ•°å­—åˆ—å(1åˆ°30)ï¼Œç¡®ä¿æ˜¯æ•°å€¼ç±»å‹
                    for i in range(1, 31):
                        retain_col = f'new_retain_{i}'
                        if retain_col in standardized_data.columns:
                            standardized_data[str(i)] = standardized_data[retain_col].apply(safe_convert_to_numeric)

                    # ç¡®ä¿stat_dateè¢«è§†ä¸ºæ—¥æœŸåˆ—
                    date_col = 'stat_date'
                    standardized_data[date_col] = pd.to_datetime(standardized_data[date_col], errors='coerce')
                    # å°†æ—¥æœŸè½¬ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                    standardized_data[date_col] = standardized_data[date_col].dt.strftime('%Y-%m-%d')
                    # æ·»åŠ "æ—¥æœŸ"åˆ—ï¼Œä¸stat_dateç›¸åŒ
                    standardized_data['æ—¥æœŸ'] = standardized_data[date_col]
                    # æ·»åŠ æœˆä»½åˆ—
                    standardized_data['month'] = standardized_data[date_col].str[:7]

                    # ç­›é€‰ç›®æ ‡æœˆä»½çš„æ•°æ®
                    filtered_data = standardized_data[standardized_data['month'] == target_month].copy()

                    # å¦‚æœå­˜åœ¨ç•™å­˜å¤©æ•°åˆ—ï¼Œè¿˜è¦ä¿ç•™ç‰¹æ®Šè¡Œ
                    retention_col = None
                    for col in standardized_data.columns:
                        if 'ç•™å­˜å¤©æ•°' in str(col):
                            retention_col = col
                            break

                    if retention_col is not None:
                        # æ‰¾å‡ºç•™å­˜å¤©æ•°ä¸ºç‰¹å®šå€¼çš„è¡Œ
                        special_rows = standardized_data[(standardized_data[retention_col] == "2025-02-01") |
                                                         (standardized_data[retention_col] == "åˆè®¡") |
                                                         (standardized_data[retention_col].astype(
                                                             str) == "2025-02-01") |
                                                         (standardized_data[retention_col].astype(str) == "åˆè®¡")]

                        # åˆå¹¶æ•°æ®æ¡†ï¼šç›®æ ‡æœˆä»½çš„æ•°æ®å’Œç‰¹æ®Šè¡Œ
                        if not special_rows.empty:
                            filtered_data = pd.concat([filtered_data, special_rows]).drop_duplicates()

                    if not filtered_data.empty:
                        # æ·»åŠ æ•°æ®æ¥æºåˆ—
                        filtered_data.insert(0, 'æ•°æ®æ¥æº', mapped_source)

                        # å¤„ç†dateåˆ—
                        if retention_col is not None:
                            filtered_data.rename(columns={retention_col: 'date'}, inplace=True)
                        elif 'stat_date' in filtered_data.columns:
                            filtered_data['date'] = filtered_data['stat_date']

                        # å°†ç­›é€‰åçš„æ•°æ®æ·»åŠ åˆ°æ€»æ•°æ®æ¡†
                        all_data = pd.concat([all_data, filtered_data], ignore_index=True)
                        processed_count += 1

                # ========== å¤„ç†ä¼ ç»Ÿæ ¼å¼è¡¨ ==========
                else:
                    # æŸ¥æ‰¾ç•™å­˜å¤©æ•°åˆ—
                    retention_col = None
                    for col in file_data_copy.columns:
                        if 'ç•™å­˜å¤©æ•°' in str(col):
                            retention_col = col
                            break

                    # æŸ¥æ‰¾å›ä¼ æ–°å¢æ•°åˆ—æˆ–total_new_usersåˆ—
                    report_users_col = None
                    for col in file_data_copy.columns:
                        if 'å›ä¼ æ–°å¢æ•°' in str(col):
                            report_users_col = col
                            break
                        if 'total_new_users' in str(col).lower():
                            report_users_col = col
                            break

                    # è·å–ç¬¬äºŒåˆ—ä½œä¸ºå¤‡é€‰
                    column_b = file_data_copy.columns[1] if len(file_data_copy.columns) > 1 else None

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ—¥æœŸåˆ—
                    date_col = None
                    for col in file_data_copy.columns:
                        if 'æ—¥æœŸ' in str(col):
                            date_col = col
                            break

                    # ========== å¤„ç†å›ä¼ æ–°å¢æ•°åˆ—æ˜ å°„ ==========
                    if report_users_col and report_users_col != 'å›ä¼ æ–°å¢æ•°':
                        file_data_copy['å›ä¼ æ–°å¢æ•°'] = file_data_copy[report_users_col].apply(safe_convert_to_numeric)
                    elif not report_users_col and column_b:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³åˆ—ï¼Œä½¿ç”¨ç¬¬äºŒåˆ—ä½œä¸º"å›ä¼ æ–°å¢æ•°"
                        file_data_copy['å›ä¼ æ–°å¢æ•°'] = file_data_copy[column_b].apply(safe_convert_to_numeric)

                    # ========== å¤„ç†æ•°å­—åˆ—ï¼ˆ1-30å¤©ç•™å­˜æ•°æ®ï¼‰==========
                    for i in range(1, 31):
                        col_name = str(i)
                        if col_name in file_data_copy.columns:
                            file_data_copy[col_name] = file_data_copy[col_name].apply(safe_convert_to_numeric)

                    # ========== å¤„ç†æ—¥æœŸåˆ—æ•°æ®å’Œç­›é€‰ ==========
                    if date_col:
                        # å°†æ—¥æœŸåˆ—è½¬æ¢ä¸ºæ—¥æœŸç±»å‹ä»¥ä¾¿è®¡ç®—å‰åèŒƒå›´
                        try:
                            if not pd.api.types.is_datetime64_dtype(file_data_copy[date_col]):
                                temp_dates = pd.to_datetime(file_data_copy[date_col], errors='coerce')
                                file_data_copy[date_col] = temp_dates

                            # æå–ç›®æ ‡æœˆä»½çš„ç¬¬ä¸€å¤©å’Œæœ€åä¸€å¤©
                            target_year, target_month_num = map(int, target_month.split('-'))
                            first_day_of_month = pd.Timestamp(year=target_year, month=target_month_num, day=1)

                            # è®¡ç®—ä¸‹ä¸€ä¸ªæœˆçš„ç¬¬ä¸€å¤©
                            if target_month_num == 12:
                                next_month = pd.Timestamp(year=target_year + 1, month=1, day=1)
                            else:
                                next_month = pd.Timestamp(year=target_year, month=target_month_num + 1, day=1)

                            # æœ€åä¸€å¤©æ˜¯ä¸‹ä¸€ä¸ªæœˆçš„ç¬¬ä¸€å¤©å‡ä¸€å¤©
                            last_day_of_month = next_month - pd.Timedelta(days=1)

                            # è®¡ç®—ç›®æ ‡èŒƒå›´ï¼ˆæœˆä»½å‰å5å¤©ï¼‰
                            start_date = first_day_of_month - pd.Timedelta(days=5)
                            end_date = last_day_of_month + pd.Timedelta(days=5)

                            # ç­›é€‰æ—¥æœŸåœ¨èŒƒå›´å†…çš„æ•°æ®
                            mask = (file_data_copy[date_col] >= start_date) & (file_data_copy[date_col] <= end_date)

                            # å¦‚æœå­˜åœ¨ç•™å­˜å¤©æ•°åˆ—ï¼Œè¿˜è¦ä¿ç•™ç‰¹æ®Šå€¼çš„è¡Œ
                            if retention_col is not None:
                                # ç”ŸæˆæŒ‡å®šæœˆä»½çš„æ‰€æœ‰æ—¥æœŸå€¼çš„åˆ—è¡¨
                                all_month_dates = []
                                current_date = first_day_of_month
                                while current_date <= last_day_of_month:
                                    date_str = current_date.strftime('%Y-%m-%d')
                                    all_month_dates.append(date_str)
                                    current_date += pd.Timedelta(days=1)

                                # æ·»åŠ "åˆè®¡"ä½œä¸ºç‰¹æ®Šå€¼
                                all_month_dates.append("åˆè®¡")

                                # æ‰¾å‡ºç•™å­˜å¤©æ•°ä¸ºç‰¹å®šå€¼çš„è¡Œ
                                retention_col_str = file_data_copy[retention_col].astype(str)
                                special_rows_mask = np.zeros(len(file_data_copy), dtype=bool)

                                for date_val in all_month_dates:
                                    special_rows_mask = special_rows_mask | (retention_col_str == date_val)

                                # åˆå¹¶ç­›é€‰æ¡ä»¶ï¼šæ—¥æœŸåœ¨èŒƒå›´å†…ï¼Œæˆ–ç•™å­˜å¤©æ•°ä¸ºç‰¹å®šå€¼
                                mask = mask | special_rows_mask

                            filtered_data = file_data_copy[mask].copy()

                            # æ·»åŠ æ ‡è®°åˆ—å’Œæœˆä»½ä¿¡æ¯åˆ—
                            filtered_data['is_target_month'] = (filtered_data[date_col] >= first_day_of_month) & (
                                    filtered_data[date_col] <= last_day_of_month)
                            filtered_data['month'] = filtered_data[date_col].dt.strftime('%Y-%m')

                            # å°†æ—¥æœŸè½¬å›å­—ç¬¦ä¸²æ ¼å¼
                            filtered_data[date_col] = filtered_data[date_col].dt.strftime('%Y-%m-%d')

                            if not filtered_data.empty:
                                # æ·»åŠ æ•°æ®æ¥æºåˆ—
                                filtered_data.insert(0, 'æ•°æ®æ¥æº', mapped_source)

                                # å°†"ç•™å­˜å¤©æ•°"åˆ—é‡å‘½åä¸º"date"
                                if retention_col is not None:
                                    filtered_data.rename(columns={retention_col: 'date'}, inplace=True)
                                elif date_col != 'date':
                                    filtered_data['date'] = filtered_data[date_col]

                                # å°†ç­›é€‰åçš„æ•°æ®æ·»åŠ åˆ°æ€»æ•°æ®æ¡†
                                all_data = pd.concat([all_data, filtered_data], ignore_index=True)
                                processed_count += 1

                        except Exception as e:
                            print(f"å¤„ç†æ—¥æœŸèŒƒå›´æ—¶å‡ºé”™: {str(e)}")
                            # å‘ç”Ÿé”™è¯¯æ—¶ï¼Œé€€å›åˆ°ä»…ç­›é€‰æœˆä»½çš„æ–¹æ³•
                            file_data_copy['month'] = file_data_copy[date_col].apply(
                                lambda x: x[:7] if isinstance(x, str) else None
                            )

                            # ç­›é€‰ç›®æ ‡æœˆä»½çš„æ•°æ®
                            filtered_data = file_data_copy[file_data_copy['month'] == target_month].copy()

                            if 'month' in filtered_data.columns:
                                filtered_data.drop('month', axis=1, inplace=True)

                            if not filtered_data.empty:
                                # æ·»åŠ æ•°æ®æ¥æºåˆ—
                                filtered_data.insert(0, 'æ•°æ®æ¥æº', mapped_source)

                                # å¤„ç†dateåˆ—
                                if retention_col is not None:
                                    filtered_data.rename(columns={retention_col: 'date'}, inplace=True)
                                elif date_col != 'date':
                                    filtered_data['date'] = filtered_data[date_col]

                                # å°†ç­›é€‰åçš„æ•°æ®æ·»åŠ åˆ°æ€»æ•°æ®æ¡†
                                all_data = pd.concat([all_data, filtered_data], ignore_index=True)
                                processed_count += 1

                    else:
                        # å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®
                        # æ·»åŠ æ•°æ®æ¥æºåˆ—
                        file_data_copy.insert(0, 'æ•°æ®æ¥æº', mapped_source)

                        # å°†"ç•™å­˜å¤©æ•°"åˆ—é‡å‘½åä¸º"date"ï¼Œå¦‚æœæœ‰çš„è¯
                        if retention_col is not None:
                            file_data_copy.rename(columns={retention_col: 'date'}, inplace=True)

                        # å°†æ‰€æœ‰æ•°æ®æ·»åŠ åˆ°æ€»æ•°æ®æ¡†
                        all_data = pd.concat([all_data, file_data_copy], ignore_index=True)
                        processed_count += 1

        except Exception as e:
            st.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‡ºé”™: {str(e)}")

    # ========== å¤„ç†åˆå¹¶åçš„æ•°æ® ==========
    if not all_data.empty:
        # æŸ¥æ‰¾dateåˆ—
        date_col = None
        for col in all_data.columns:
            if col == 'date':
                date_col = col
                break

        # ä¸ºæ–°æ ¼å¼è¡¨åˆ›å»ºdateåˆ—
        if date_col is None and 'stat_date' in all_data.columns:
            all_data['date'] = all_data['stat_date']
            date_col = 'date'

        # æ’åºå¤„ç†
        sort_columns = []
        if 'æ•°æ®æ¥æº' in all_data.columns:
            sort_columns.append('æ•°æ®æ¥æº')

        if date_col:
            try:
                all_data[date_col] = pd.to_datetime(all_data[date_col], errors='coerce')
                sort_columns.append(date_col)
            except:
                sort_columns.append(date_col)

        # æ‰§è¡Œæ’åº
        if sort_columns:
            all_data = all_data.sort_values(by=sort_columns)
            # å°†æ—¥æœŸåˆ—è½¬å›å­—ç¬¦ä¸²æ ¼å¼
            if date_col and pd.api.types.is_datetime64_dtype(all_data[date_col]):
                all_data[date_col] = all_data[date_col].dt.strftime('%Y-%m-%d')

        # æ ‡å‡†åŒ–è¾“å‡ºåˆ—ç»“æ„
        standardized_df = standardize_output_columns(all_data)
        return standardized_df, processed_count, mapping_warnings
    else:
        return None, 0, mapping_warnings

# ==================== æ•°å­¦å»ºæ¨¡å‡½æ•° ====================
# å®šä¹‰æ•°å­¦å‡½æ•°
def power_function(x, a, b):
    """å¹‚å‡½æ•°ï¼šy = a * x^b"""
    return a * np.power(x, b)

def exponential_function(x, c, d):
    """æŒ‡æ•°å‡½æ•°ï¼šy = c * exp(d * x)"""
    return c * np.exp(d * x)

# ==================== ç•™å­˜ç‡è®¡ç®—å‡½æ•° ====================
# ç•™å­˜ç‡è®¡ç®—
def calculate_retention_rates_advanced(df):
    retention_results = []
    data_sources = df['æ•°æ®æ¥æº'].unique()

    for source in data_sources:
        source_data = df[df['æ•°æ®æ¥æº'] == source].copy()
        all_days = []
        all_rates = []

        for _, row in source_data.iterrows():
            new_users = safe_convert_to_numeric(row.get('å›ä¼ æ–°å¢æ•°', 0))
            if pd.isna(new_users) or new_users <= 0:
                continue

            for day in range(1, 31):
                day_col = str(day)
                if day_col in row and not pd.isna(row[day_col]):
                    retain_count = safe_convert_to_numeric(row[day_col])
                    if retain_count > 0:
                        retention_rate = retain_count / new_users
                        if 0 < retention_rate <= 1.5:
                            all_days.append(day)
                            all_rates.append(retention_rate)

        if all_days:
            df_temp = pd.DataFrame({'day': all_days, 'rate': all_rates})
            df_avg = df_temp.groupby('day')['rate'].mean().reset_index()

            retention_data = {
                'data_source': source,
                'days': df_avg['day'].values,
                'rates': df_avg['rate'].values
            }
            retention_results.append(retention_data)

    return retention_results

# ==================== è®¡ç®—æŒ‡å®šå¤©æ•°çš„ç´¯ç§¯LTå€¼å‡½æ•° ====================
def calculate_cumulative_lt(days_array, rates_array, target_days):
    """è®¡ç®—æŒ‡å®šå¤©æ•°çš„ç´¯ç§¯LTå€¼"""
    result = {}
    for day in target_days:
        idx = np.searchsorted(days_array, day, side='right')
        if idx > 0:
            # è®¡ç®—åˆ°æŒ‡å®šå¤©æ•°çš„ç´¯ç§¯LTå€¼ï¼ˆåŒ…æ‹¬ç¬¬0å¤©çš„1.0ï¼‰
            cumulative_lt = 1.0 + np.sum(rates_array[1:idx])
            result[day] = cumulative_lt
    return result

# ==================== LTæ‹Ÿåˆåˆ†æå‡½æ•° ====================
# LTæ‹Ÿåˆåˆ†æ - ä½¿ç”¨ç¬¬äºŒæ®µä»£ç çš„é€»è¾‘
def calculate_lt_advanced(retention_result, channel_name, lt_years=5, return_curve_data=False, key_days=None):
    """
    æŒ‰æ¸ é“è§„åˆ™è®¡ç®— LTï¼Œå…è®¸ 1-30 å¤©æ•°æ®ä¸è¿ç»­ã€‚
    å‚æ•°:
        lt_years: è®¡ç®—å‡ å¹´çš„LTï¼Œé»˜è®¤5å¹´
        return_curve_data: æ˜¯å¦è¿”å›æ›²çº¿æ•°æ®ç”¨äºå¯è§†åŒ–
        key_days: å…³é”®æ—¶é—´ç‚¹åˆ—è¡¨ï¼Œç”¨äºè®¡ç®—è¿™äº›æ—¶é—´ç‚¹çš„ç´¯ç§¯LTå€¼
    """
    # æ¸ é“è§„åˆ™
    CHANNEL_RULES = {
        "åä¸º": {"stage_2": [30, 120], "stage_3_base": [120, 220]},
        "å°ç±³": {"stage_2": [30, 190], "stage_3_base": [190, 290]},
        "oppo": {"stage_2": [30, 160], "stage_3_base": [160, 260]},
        "vivo": {"stage_2": [30, 150], "stage_3_base": [150, 250]},
        "iphone": {"stage_2": [30, 150], "stage_3_base": [150, 250]},
        "å…¶ä»–": {"stage_2": [30, 100], "stage_3_base": [100, 200]}
    }

    if re.search(r'åä¸º', channel_name):
        rules = CHANNEL_RULES["åä¸º"]
    elif re.search(r'å°ç±³', channel_name):
        rules = CHANNEL_RULES["å°ç±³"]
    elif re.search(r'oppo|OPPO', channel_name):
        rules = CHANNEL_RULES["oppo"]
    elif re.search(r'vivo', channel_name):
        rules = CHANNEL_RULES["vivo"]
    elif re.search(r'iphone|iPhone', channel_name):
        rules = CHANNEL_RULES["iphone"]
    else:
        rules = CHANNEL_RULES["å…¶ä»–"]

    stage_2_start, stage_2_end = rules["stage_2"]
    stage_3_base_start, stage_3_base_end = rules["stage_3_base"]

    # è®¡ç®—æœ€å¤§å¤©æ•°ï¼ˆæ ¹æ®æŒ‡å®šå¹´æ•°ï¼‰
    max_days = lt_years * 365

    days = retention_result["days"]
    rates = retention_result["rates"]

    # å­˜å‚¨æ‹Ÿåˆå‚æ•°ï¼Œç”¨äºåç»­åˆ†æ
    fit_params = {}

    # ----- ç¬¬ä¸€é˜¶æ®µ -----
    try:
        # ç”¨å·²æœ‰æ•°æ®å¯¹ 1-30 å¤©çš„ç•™å­˜ç‡è¿›è¡Œæ‹Ÿåˆ
        popt_power, _ = curve_fit(power_function, days, rates)
        a, b = popt_power
        fit_params["power"] = {"a": a, "b": b}

        # ç”¨æ‹Ÿåˆå‡½æ•°ç”Ÿæˆå®Œæ•´çš„ 1-30 å¤©ç•™å­˜ç‡
        days_full = np.arange(1, 31)  # è¿ç»­çš„ 1-30 å¤©
        rates_full = power_function(days_full, a, b)

        # ç¬¬ä¸€é˜¶æ®µçš„ LT ç´¯åŠ å€¼
        lt1_to_30 = np.sum(rates_full)
    except Exception as e:
        lt1_to_30 = 0.0
        a, b = 1.0, -1.0  # é»˜è®¤å‚æ•°

    # ----- ç¬¬äºŒé˜¶æ®µ -----
    try:
        days_stage_2 = np.arange(stage_2_start, stage_2_end + 1)
        rates_stage_2 = power_function(days_stage_2, a, b)
        lt_stage_2 = np.sum(rates_stage_2)
    except Exception as e:
        lt_stage_2 = 0.0
        rates_stage_2 = np.array([])

    # ----- ç¬¬ä¸‰é˜¶æ®µ -----
    try:
        days_stage_3_base = np.arange(stage_3_base_start, stage_3_base_end + 1)
        rates_stage_3_base = power_function(days_stage_3_base, a, b)

        # æŒ‡æ•°æ‹Ÿåˆ
        initial_c = rates_stage_3_base[0]
        initial_d = -0.001
        popt_exp, _ = curve_fit(
            exponential_function,
            days_stage_3_base,
            rates_stage_3_base,
            p0=[initial_c, initial_d],
            bounds=([0, -np.inf], [np.inf, 0])  # é™åˆ¶ d < 0
        )
        c, d = popt_exp
        fit_params["exponential"] = {"c": c, "d": d}
        days_stage_3 = np.arange(stage_3_base_start, max_days + 1)
        rates_stage_3 = exponential_function(days_stage_3, c, d)
        lt_stage_3 = np.sum(rates_stage_3)
    except Exception as e:
        days_stage_3 = np.arange(stage_3_base_start, max_days + 1)
        rates_stage_3 = power_function(days_stage_3, a, b) if 'a' in locals() else np.zeros(len(days_stage_3))
        lt_stage_3 = np.sum(rates_stage_3)

    # ----- æ€» LT è®¡ç®— -----
    total_lt = 1.0 + lt1_to_30 + lt_stage_2 + lt_stage_3

    # è®¡ç®—RÂ²ç”¨äºè¯„ä¼°æ‹Ÿåˆè´¨é‡
    try:
        predicted_rates = power_function(days, a, b)
        r2_score = 1 - np.sum((rates - predicted_rates) ** 2) / np.sum((rates - np.mean(rates)) ** 2)
    except:
        r2_score = 0.0

    if return_curve_data:
        # è¿”å›ä¸åŒ…å«ç¬¬0å¤©çš„æ›²çº¿æ•°æ®ç”¨äºå¯è§†åŒ–
        all_days = np.concatenate([
            days_full,      # ç¬¬1-30å¤©
            days_stage_2,   # ç¬¬äºŒé˜¶æ®µ
            days_stage_3    # ç¬¬ä¸‰é˜¶æ®µ
        ])
        
        if 'rates_stage_2' not in locals():
            rates_stage_2 = power_function(days_stage_2, a, b)
        
        all_rates = np.concatenate([
            rates_full,                # ç¬¬1-30å¤©
            rates_stage_2,             # ç¬¬äºŒé˜¶æ®µ
            rates_stage_3              # ç¬¬ä¸‰é˜¶æ®µ
        ])

        # æŒ‰å¤©æ•°æ’åº
        sort_idx = np.argsort(all_days)
        all_days = all_days[sort_idx]
        all_rates = all_rates[sort_idx]

        # åªè¿”å›åˆ°æŒ‡å®šå¹´æ•°çš„æ•°æ®
        max_idx = np.searchsorted(all_days, lt_years * 365, side='right')
        all_days = all_days[:max_idx]
        all_rates = all_rates[:max_idx]

        # è®¡ç®—å…³é”®æ—¶é—´ç‚¹çš„ç´¯ç§¯LTå€¼
        key_days_lt = {}
        if key_days:
            key_days_lt = calculate_cumulative_lt(all_days, all_rates, key_days)

        return {
            'lt_value': total_lt,
            'fit_params': fit_params,
            'power_r2': max(0, min(1, r2_score)),
            'success': True,
            'model_used': 'power+exponential',
            'curve_days': all_days,
            'curve_rates': all_rates,
            'key_days_lt': key_days_lt
        }

    return total_lt

# ==================== å¯è§†åŒ–å‡½æ•° - ä½¿ç”¨ç¬¬äºŒæ®µä»£ç çš„é€»è¾‘ ====================
def visualize_lt_curves(visualization_data, years=2):
    """
    åˆ›å»ºçº¿æ€§åæ ‡LTæ›²çº¿å›¾
    æ¸ é“æŒ‰LTå€¼ä»ä½åˆ°é«˜æ’åº
    """
    # æŒ‰LTå€¼ä»ä½åˆ°é«˜æ’åºæ¸ é“
    sorted_channels = sorted(visualization_data.items(), key=lambda x: x[1]['lt'])

    # åˆ›å»ºå›¾è¡¨
    fig = plt.figure(figsize=(14, 8))
    ax = fig.add_subplot(111)

    # è®¾ç½®é¢œè‰²å¾ªç¯
    colors = plt.cm.tab10.colors

    # ä¸ºæ¯ä¸ªæ¸ é“ç»˜åˆ¶æ›²çº¿
    for idx, (channel_name, data) in enumerate(sorted_channels):
        color = colors[idx % len(colors)]

        # çº¿æ€§åæ ‡å›¾
        ax.plot(
            data["days"],
            data["rates"],
            label=f"{channel_name} (LT={data['lt']:.2f})",
            color=color,
            linewidth=2
        )

    # çº¿æ€§åæ ‡è®¾ç½® - ä¿®æ”¹ä¸º 0-60%
    ax.set_ylim(0, 0.6)
    ax.set_yticks([0, 0.15, 0.3, 0.45, 0.6])
    ax.set_yticklabels(['0%', '15%', '30%', '45%', '60%'])
    ax.grid(True, ls="--", alpha=0.5)
    ax.set_xlabel('ç•™å­˜å¤©æ•°')
    ax.set_ylabel('ç•™å­˜ç‡')
    ax.set_title(f'æ‰€æœ‰æ¸ é“{years}å¹´LTç•™å­˜æ›²çº¿æ¯”è¾ƒ (æŒ‰LTå€¼ä»ä½åˆ°é«˜æ’åº)')
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

    plt.tight_layout()
    return fig, colors, sorted_channels  # è¿”å›é¢œè‰²å’Œæ’åºåçš„æ¸ é“ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨

def visualize_log_comparison(visualization_data_2y, visualization_data_5y, colors=None, sorted_channels_2y=None):
    """
    åˆ›å»º2å¹´å’Œ5å¹´å¯¹æ•°åæ ‡æ¯”è¾ƒå›¾ä½œä¸ºå·¦å³å­å›¾
    ä½¿ç”¨ä¸çº¿æ€§å›¾ç›¸åŒçš„é¢œè‰²å’Œæ’åº
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # å¦‚æœæ²¡æœ‰æä¾›æ’åºæ¸ é“å’Œé¢œè‰²ï¼Œåˆ™é‡æ–°è®¡ç®—
    if sorted_channels_2y is None:
        sorted_channels_2y = sorted(visualization_data_2y.items(), key=lambda x: x[1]['lt'])
    if colors is None:
        colors = plt.cm.tab10.colors

    sorted_channels_5y = sorted(visualization_data_5y.items(), key=lambda x: x[1]['lt'])

    # ç»˜åˆ¶2å¹´å¯¹æ•°å›¾
    for idx, (channel_name, data) in enumerate(sorted_channels_2y):
        color = colors[idx % len(colors)]
        ax1.plot(
            data["days"],
            data["rates"],
            color=color,
            linewidth=2
        )

    # ç»˜åˆ¶5å¹´å¯¹æ•°å›¾
    for idx, (channel_name, data) in enumerate(sorted_channels_5y):
        color = colors[idx % len(colors)]
        ax2.plot(
            data["days"],
            data["rates"],
            color=color,
            linewidth=2
        )

    # å¯¹æ•°åæ ‡è®¾ç½® - 2å¹´å›¾
    ax1.set_yscale('log')
    ax1.set_ylim(0.001, 0.6)
    ax1.set_yticks([0.001, 0.01, 0.1, 0.6])
    ax1.set_yticklabels(['0.1%', '1%', '10%', '60%'])
    ax1.grid(True, ls="--", alpha=0.5)
    ax1.set_xlabel('ç•™å­˜å¤©æ•°')
    ax1.set_ylabel('ç•™å­˜ç‡ (å¯¹æ•°åæ ‡)')
    ax1.set_title('2å¹´LTç•™å­˜æ›²çº¿ (å¯¹æ•°åæ ‡)')

    # å¯¹æ•°åæ ‡è®¾ç½® - 5å¹´å›¾
    ax2.set_yscale('log')
    ax2.set_ylim(0.001, 0.6)
    ax2.set_yticks([0.001, 0.01, 0.1, 0.6])
    ax2.set_yticklabels(['0.1%', '1%', '10%', '60%'])
    ax2.grid(True, ls="--", alpha=0.5)
    ax2.set_xlabel('ç•™å­˜å¤©æ•°')
    ax2.set_title('5å¹´LTç•™å­˜æ›²çº¿ (å¯¹æ•°åæ ‡)')

    plt.tight_layout()
    return fig

def visualize_fitting_comparison(original_data, visualization_data):
    """å¯è§†åŒ–æ‹Ÿåˆæ•ˆæœæ¯”è¾ƒï¼ˆå®é™…æ•°æ®vsæ‹Ÿåˆæ›²çº¿ï¼‰- æ˜¾ç¤ºæ‰€æœ‰æ¸ é“"""
    # æŒ‰LTå€¼ä»ä½åˆ°é«˜æ’åºæ¸ é“
    channels = sorted(visualization_data.keys(), key=lambda x: visualization_data[x]['lt'])

    # è®¡ç®—éœ€è¦å¤šå°‘è¡Œ
    n_channels = len(channels)
    n_cols = 3
    n_rows = (n_channels + n_cols - 1) // n_cols  # å‘ä¸Šå–æ•´

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows), squeeze=False)

    for i, channel_name in enumerate(channels):
        row = i // n_cols
        col = i % n_cols
        ax = axes[row, col]

        data = visualization_data[channel_name]

        # ç»˜åˆ¶åŸå§‹æ•°æ®ç‚¹
        if channel_name in original_data:
            ax.scatter(
                original_data[channel_name]["days"],
                original_data[channel_name]["rates"],
                color='red',
                s=50,
                alpha=0.7,
                label='å®é™…æ•°æ®'
            )

        # ç»˜åˆ¶æ‹Ÿåˆæ›²çº¿ï¼ˆé™åˆ¶åœ¨0-100å¤©èŒƒå›´å†…æ›´æ¸…æ™°å±•ç¤ºæ‹Ÿåˆæ•ˆæœï¼‰
        fit_days = data["days"]
        fit_rates = data["rates"]

        # é™åˆ¶æ˜¾ç¤ºèŒƒå›´åˆ°100å¤©ä»¥å†…
        idx_100 = np.searchsorted(fit_days, 100, side='right')
        ax.plot(
            fit_days[:idx_100],
            fit_rates[:idx_100],
            color='blue',
            linewidth=2,
            label='æ‹Ÿåˆæ›²çº¿'
        )

        ax.set_title(f'{channel_name} (LT={data["lt"]:.2f})')
        ax.set_xlabel('ç•™å­˜å¤©æ•°')
        ax.set_ylabel('ç•™å­˜ç‡')
        ax.set_ylim(0, 0.6)
        ax.set_yticks([0, 0.15, 0.3, 0.45, 0.6])
        ax.set_yticklabels(['0%', '15%', '30%', '45%', '60%'])
        ax.grid(True, ls="--", alpha=0.3)
        ax.legend()

    # éšè—æœªä½¿ç”¨çš„å­å›¾
    for i in range(len(channels), n_rows * n_cols):
        row = i // n_cols
        col = i % n_cols
        fig.delaxes(axes[row, col])

    plt.tight_layout()
    return fig

# ==================== é¡µé¢åˆå§‹åŒ– ====================
# ä¸»æ ‡é¢˜
st.markdown("""
<div class="main-header">
    <div class="main-title">ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†æç³»ç»Ÿ</div>
    <div class="main-subtitle">åŸºäºåˆ†é˜¶æ®µæ•°å­¦å»ºæ¨¡çš„LTVé¢„æµ‹</div>
</div>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–session state
session_keys = [
    'channel_mapping', 'merged_data', 'cleaned_data', 'retention_data',
    'lt_results', 'arpu_data', 'ltv_results', 'current_step', 'excluded_data'
]
for key in session_keys:
    if key not in st.session_state:
        st.session_state[key] = None

# è®¾ç½®é»˜è®¤å€¼
if st.session_state.channel_mapping is None:
    st.session_state.channel_mapping = DEFAULT_CHANNEL_MAPPING
if st.session_state.current_step is None:
    st.session_state.current_step = 0
if st.session_state.excluded_data is None:
    st.session_state.excluded_data = []

# ==================== åˆ†ææ­¥éª¤å®šä¹‰ ====================
# åˆ†ææ­¥éª¤å®šä¹‰
ANALYSIS_STEPS = [
    {"name": "æ•°æ®ä¸Šä¼ ä¸æ±‡æ€»"},
    {"name": "å¼‚å¸¸æ•°æ®å‰”é™¤"},
    {"name": "ç•™å­˜ç‡è®¡ç®—"},
    {"name": "LTæ‹Ÿåˆåˆ†æ"},
    {"name": "ARPUè®¡ç®—"},
    {"name": "LTVç»“æœæŠ¥å‘Š"}
]

# ==================== æ­¥éª¤çŠ¶æ€æ£€æŸ¥å‡½æ•° ====================
# æ£€æŸ¥æ­¥éª¤å®ŒæˆçŠ¶æ€
def get_step_status(step_index):
    if step_index == st.session_state.current_step:
        return "active"
    if step_index == 0 and st.session_state.merged_data is not None:
        return "completed"
    elif step_index == 1 and st.session_state.cleaned_data is not None:
        return "completed"
    elif step_index == 2 and st.session_state.retention_data is not None:
        return "completed"
    elif step_index == 3 and st.session_state.lt_results is not None:
        return "completed"
    elif step_index == 4 and st.session_state.arpu_data is not None:
        return "completed"
    elif step_index == 5 and st.session_state.ltv_results is not None:
        return "completed"
    return "normal"

# ==================== ä¾§è¾¹æ å¯¼èˆª ====================
# ä¾§è¾¹æ å¯¼èˆª
with st.sidebar:
    st.markdown('<div class="nav-container">', unsafe_allow_html=True)
    st.markdown('<h4 style="text-align: center; margin-bottom: 1rem; color: #495057;">åˆ†ææµç¨‹</h4>',
                unsafe_allow_html=True)

    for i, step in enumerate(ANALYSIS_STEPS):
        if st.button(f"{i + 1}. {step['name']}", key=f"nav_{i}",
                     use_container_width=True,
                     type="primary" if get_step_status(i) == "active" else "secondary"):
            st.session_state.current_step = i
            st.rerun()

    st.markdown('</div>', unsafe_allow_html=True)

# ==================== è¾…åŠ©å‡½æ•° ====================
def show_dependency_warning(required_step):
    st.warning(f"âš ï¸ æ­¤æ­¥éª¤éœ€è¦å…ˆå®Œæˆã€Œ{required_step}ã€")
    st.info("æ‚¨å¯ä»¥ç‚¹å‡»å·¦ä¾§å¯¼èˆªç›´æ¥è·³è½¬åˆ°å¯¹åº”æ­¥éª¤ï¼Œæˆ–è€…ç»§ç»­æŸ¥çœ‹å½“å‰æ­¥éª¤çš„åŠŸèƒ½ä»‹ç»ã€‚")

# ==================== é¡µé¢è·¯ç”± ====================
# è·å–å½“å‰é¡µé¢
current_page = ANALYSIS_STEPS[st.session_state.current_step]["name"]

# ==================== é¡µé¢å†…å®¹ ====================

if current_page == "æ•°æ®ä¸Šä¼ ä¸æ±‡æ€»":
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.subheader("æ¸ é“æ˜ å°„æ–‡ä»¶è®¾ç½®")
    
    # æ¸ é“æ˜ å°„æ–‡ä»¶ä¸Šä¼ 
    channel_mapping_file = st.file_uploader(
        "ä¸Šä¼ æ¸ é“æ˜ å°„æ–‡ä»¶ (Excelæ ¼å¼ï¼Œå¯é€‰)",
        type=['xlsx', 'xls'],
        help="æ ¼å¼ï¼šç¬¬ä¸€åˆ—ä¸ºæ¸ é“åç§°ï¼Œåç»­åˆ—ä¸ºå¯¹åº”çš„æ¸ é“å·"
    )
    
    if channel_mapping_file:
        try:
            custom_mapping = parse_channel_mapping_from_excel(channel_mapping_file)
            if custom_mapping and isinstance(custom_mapping, dict) and len(custom_mapping) > 0:
                st.session_state.channel_mapping = custom_mapping
                st.success(f"æ¸ é“æ˜ å°„æ–‡ä»¶åŠ è½½æˆåŠŸï¼å…±åŒ…å« {len(custom_mapping)} ä¸ªæ˜ å°„å…³ç³»")
                
                # æ˜¾ç¤ºæ˜ å°„é¢„è§ˆ
                with st.expander("æŸ¥çœ‹æ¸ é“æ˜ å°„è¯¦æƒ…"):
                    mapping_df = pd.DataFrame([
                        {'æ¸ é“å·': pid, 'æ¸ é“åç§°': channel}
                        for pid, channel in sorted(custom_mapping.items())
                    ])
                    st.dataframe(mapping_df, use_container_width=True)
            else:
                st.error("æ¸ é“æ˜ å°„æ–‡ä»¶è§£æå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤æ˜ å°„")
        except Exception as e:
            st.error(f"è¯»å–æ¸ é“æ˜ å°„æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")
    else:
        st.info("æœªä¸Šä¼ æ¸ é“æ˜ å°„æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤æ˜ å°„å…³ç³»")
        
        # æ˜¾ç¤ºé»˜è®¤æ˜ å°„
        with st.expander("æŸ¥çœ‹é»˜è®¤æ¸ é“æ˜ å°„"):
            default_mapping_df = pd.DataFrame([
                {'æ¸ é“å·': pid, 'æ¸ é“åç§°': channel}
                for pid, channel in sorted(DEFAULT_CHANNEL_MAPPING.items())
            ])
            st.dataframe(default_mapping_df, use_container_width=True)
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.subheader("æ•°æ®æ–‡ä»¶å¤„ç†")

    uploaded_files = st.file_uploader(
        "é€‰æ‹©Excelæ•°æ®æ–‡ä»¶",
        type=['xlsx', 'xls'],
        accept_multiple_files=True,
        help="æ”¯æŒä¸Šä¼ å¤šä¸ªExcelæ–‡ä»¶"
    )

    default_month = get_default_target_month()
    target_month = st.text_input("ç›®æ ‡æœˆä»½ (YYYY-MM)", value=default_month)

    if uploaded_files:
        st.info(f"å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")

        if st.button("å¼€å§‹å¤„ç†æ•°æ®", type="primary", use_container_width=True):
            with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®æ–‡ä»¶..."):
                try:
                    merged_data, processed_count, mapping_warnings = integrate_excel_files_streamlit(
                        uploaded_files, target_month, st.session_state.channel_mapping
                    )

                    if merged_data is not None and not merged_data.empty:
                        st.session_state.merged_data = merged_data
                        st.success(f"æ•°æ®å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {processed_count} ä¸ªæ–‡ä»¶")

                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("æ€»è®°å½•æ•°", f"{len(merged_data):,}")
                        with col2:
                            st.metric("æ•°æ®æ¥æºæ•°", merged_data['æ•°æ®æ¥æº'].nunique())
                        with col3:
                            if 'å›ä¼ æ–°å¢æ•°' in merged_data.columns:
                                total_users = merged_data['å›ä¼ æ–°å¢æ•°'].sum()
                                st.metric("æ€»æ–°å¢ç”¨æˆ·", f"{total_users:,.0f}")

                        # æ˜¾ç¤ºæ˜ å°„è­¦å‘Š
                        if mapping_warnings:
                            st.warning("ä»¥ä¸‹æ–‡ä»¶æœªåœ¨æ¸ é“æ˜ å°„ä¸­æ‰¾åˆ°å¯¹åº”å…³ç³»ï¼š")
                            for warning in mapping_warnings:
                                st.text(f"â€¢ {warning}")

                        st.subheader("æ•°æ®é¢„è§ˆ")
                        st.dataframe(merged_data.head(10), use_container_width=True)
                    else:
                        st.error("æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
                except Exception as e:
                    st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")
    else:
        st.info("è¯·é€‰æ‹©Excelæ–‡ä»¶å¼€å§‹æ•°æ®å¤„ç†")

    st.markdown('</div>', unsafe_allow_html=True)

elif current_page == "å¼‚å¸¸æ•°æ®å‰”é™¤":
    if st.session_state.merged_data is None:
        show_dependency_warning("æ•°æ®ä¸Šä¼ ä¸æ±‡æ€»")
    else:
        merged_data = st.session_state.merged_data

        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("å¼‚å¸¸æ•°æ®å‰”é™¤é…ç½®")
        st.info("æ³¨æ„ï¼šæ‰€æœ‰å‰”é™¤æ¡ä»¶å¿…é¡»åŒæ—¶æ»¡è¶³æ‰ä¼šè¢«å‰”é™¤ï¼ˆä¸”å…³ç³»ï¼‰")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("### æŒ‰æ•°æ®æ¥æºå‰”é™¤")
            all_sources = merged_data['æ•°æ®æ¥æº'].unique().tolist()
            excluded_sources = st.multiselect("é€‰æ‹©è¦å‰”é™¤çš„æ•°æ®æ¥æº", options=all_sources)

        with col2:
            st.markdown("### æŒ‰æ—¥æœŸå‰”é™¤")
            if 'date' in merged_data.columns:
                all_dates = sorted(merged_data['date'].unique().tolist())
                excluded_dates = st.multiselect("é€‰æ‹©è¦å‰”é™¤çš„æ—¥æœŸ", options=all_dates)
            else:
                st.info("æ•°æ®ä¸­æ— æ—¥æœŸå­—æ®µ")
                excluded_dates = []

        try:
            exclusion_mask = pd.Series([True] * len(merged_data), index=merged_data.index)

            if excluded_sources:
                source_mask = merged_data['æ•°æ®æ¥æº'].isin(excluded_sources)
                exclusion_mask &= source_mask

            if 'date' in merged_data.columns and excluded_dates:
                date_mask = merged_data['date'].isin(excluded_dates)
                exclusion_mask &= date_mask

            if not excluded_sources and not excluded_dates:
                exclusion_mask = pd.Series([False] * len(merged_data), index=merged_data.index)

            to_exclude = merged_data[exclusion_mask]
            to_keep = merged_data[~exclusion_mask]

        except Exception as e:
            st.error(f"è®¡ç®—å‰”é™¤æ¡ä»¶æ—¶å‡ºé”™: {str(e)}")
            to_exclude = pd.DataFrame()
            to_keep = merged_data.copy()

        col1, col2 = st.columns(2)
        with col1:
            st.markdown(f"### å°†è¢«å‰”é™¤çš„æ•°æ® ({len(to_exclude)} æ¡)")
            if len(to_exclude) > 0:
                st.dataframe(to_exclude.head(5), use_container_width=True)

        with col2:
            st.markdown(f"### ä¿ç•™çš„æ•°æ® ({len(to_keep)} æ¡)")
            if len(to_keep) > 0:
                st.dataframe(to_keep.head(5), use_container_width=True)

        if st.button("ç¡®è®¤å‰”é™¤å¼‚å¸¸æ•°æ®", type="primary", use_container_width=True):
            try:
                if len(to_exclude) > 0:
                    excluded_records = [f"{row.get('æ•°æ®æ¥æº', 'Unknown')} - {row.get('date', 'Unknown')}"
                                        for _, row in to_exclude.iterrows()]
                    st.session_state.excluded_data = excluded_records
                    st.session_state.cleaned_data = to_keep.copy()
                    st.success(f"æˆåŠŸå‰”é™¤ {len(to_exclude)} æ¡å¼‚å¸¸æ•°æ®")
                else:
                    st.session_state.cleaned_data = merged_data.copy()
                    st.info("æœªå‘ç°éœ€è¦å‰”é™¤çš„å¼‚å¸¸æ•°æ®")
            except Exception as e:
                st.error(f"å‰”é™¤æ•°æ®æ—¶å‡ºé”™: {str(e)}")

        st.markdown('</div>', unsafe_allow_html=True)

elif current_page == "ç•™å­˜ç‡è®¡ç®—":
    if st.session_state.cleaned_data is not None:
        working_data = st.session_state.cleaned_data
        data_source_info = "ä½¿ç”¨æ¸…ç†åçš„æ•°æ®"
    elif st.session_state.merged_data is not None:
        working_data = st.session_state.merged_data
        data_source_info = "ä½¿ç”¨åŸå§‹æ•°æ®"
    else:
        working_data = None
        data_source_info = "æ— å¯ç”¨æ•°æ®"

    if working_data is None:
        show_dependency_warning("æ•°æ®ä¸Šä¼ ä¸æ±‡æ€»")
    else:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("ç•™å­˜ç‡è®¡ç®—é…ç½®")
        st.info(data_source_info)

        data_sources = working_data['æ•°æ®æ¥æº'].unique()
        selected_sources = st.multiselect("é€‰æ‹©è¦åˆ†æçš„æ•°æ®æ¥æº", options=data_sources, default=data_sources)

        if st.button("è®¡ç®—ç•™å­˜ç‡", type="primary", use_container_width=True):
            if selected_sources:
                with st.spinner("æ­£åœ¨è®¡ç®—ç•™å­˜ç‡..."):
                    filtered_data = working_data[working_data['æ•°æ®æ¥æº'].isin(selected_sources)]
                    retention_results = calculate_retention_rates_advanced(filtered_data)
                    st.session_state.retention_data = retention_results

                    st.success("ç•™å­˜ç‡è®¡ç®—å®Œæˆï¼")

                    for result in retention_results:
                        with st.expander(f"{result['data_source']} - ç•™å­˜ç‡è¯¦æƒ…"):
                            days = result['days']
                            rates = result['rates']

                            if len(days) > 0:
                                fig, ax = plt.subplots(figsize=(10, 6))
                                ax.scatter(days, rates, color='orange', s=80, alpha=0.8)
                                ax.plot(days, rates, '--', color='green', linewidth=2)
                                ax.set_xlabel('ç•™å­˜å¤©æ•°')
                                ax.set_ylabel('ç•™å­˜ç‡')
                                ax.set_title(f'{result["data_source"]} ç•™å­˜ç‡æ›²çº¿')
                                ax.grid(True, alpha=0.3)
                                plt.tight_layout()
                                st.pyplot(fig)
                                plt.close()
            else:
                st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ•°æ®æ¥æº")

        st.markdown('</div>', unsafe_allow_html=True)

elif current_page == "LTæ‹Ÿåˆåˆ†æ":
    if st.session_state.retention_data is None:
        show_dependency_warning("ç•™å­˜ç‡è®¡ç®—")
    else:
        retention_data = st.session_state.retention_data

        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("åˆ†é˜¶æ®µæ‹Ÿåˆå‚æ•°é…ç½®")

        lt_years = st.number_input("LTè®¡ç®—å¹´é™", min_value=1, max_value=10, value=5)
        st.info("ç³»ç»Ÿå°†é‡‡ç”¨ä¸‰é˜¶æ®µåˆ†å±‚å»ºæ¨¡")

        if st.button("å¼€å§‹LTæ‹Ÿåˆåˆ†æ", type="primary", use_container_width=True):
            with st.spinner("æ­£åœ¨è¿›è¡Œæ‹Ÿåˆè®¡ç®—..."):
                lt_results = []
                visualization_data = {}
                original_data = {}
                
                # å…³é”®æ—¶é—´ç‚¹åˆ—è¡¨
                key_days = [1, 7, 30, 60, 90, 100, 150, 200, 300]

                for retention_result in retention_data:
                    channel_name = retention_result['data_source']
                    lt_result = calculate_lt_advanced(retention_result, channel_name, lt_years, 
                                                    return_curve_data=True, key_days=key_days)

                    lt_results.append({
                        'data_source': channel_name,
                        'lt_value': lt_result['lt_value'],
                        'fit_success': lt_result['success'],
                        'fit_params': lt_result['fit_params'],
                        'power_r2': lt_result['power_r2'],
                        'model_used': lt_result['model_used']
                    })

                    visualization_data[channel_name] = {
                        "days": lt_result['curve_days'],
                        "rates": lt_result['curve_rates'],
                        "lt": lt_result['lt_value']
                    }

                    original_data[channel_name] = {
                        "days": retention_result['days'],
                        "rates": retention_result['rates']
                    }

                st.session_state.lt_results = lt_results
                st.success("LTæ‹Ÿåˆåˆ†æå®Œæˆï¼")

                # æ˜¾ç¤ºLTå€¼è¡¨æ ¼
                if lt_results:
                    st.subheader("LTåˆ†æç»“æœ")
                    results_df = pd.DataFrame([
                        {
                            'æ¸ é“åç§°': r['data_source'],
                            f'{lt_years}å¹´LT': round(r['lt_value'], 2),
                            'æ‹ŸåˆçŠ¶æ€': 'æˆåŠŸ' if r['fit_success'] else 'å¤±è´¥',
                            'RÂ²å¾—åˆ†': round(r['power_r2'], 3),
                            'ä½¿ç”¨æ¨¡å‹': r['model_used']
                        }
                        for r in lt_results
                    ])
                    st.dataframe(results_df, use_container_width=True)

                # æ˜¾ç¤ºLTå€¼å¯¹æ¯”æŸ±çŠ¶å›¾
                if lt_results:
                    sorted_results = sorted(lt_results, key=lambda x: x['lt_value'])
                    sources = [r['data_source'] for r in sorted_results]
                    lt_values = [r['lt_value'] for r in sorted_results]

                    fig, ax = plt.subplots(figsize=(12, 8))
                    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(sources)))
                    bars = ax.bar(sources, lt_values, color=colors, alpha=0.8)

                    for bar, value in zip(bars, lt_values):
                        height = bar.get_height()
                        ax.text(bar.get_x() + bar.get_width() / 2., height + height * 0.01,
                                f'{value:.1f}', ha='center', va='bottom', fontweight='bold')

                    ax.set_xlabel('æ•°æ®æ¥æº')
                    ax.set_ylabel('LTå€¼ (å¤©)')
                    ax.set_title(f'å„æ¸ é“{lt_years}å¹´LTå€¼å¯¹æ¯”')
                    ax.tick_params(axis='x', rotation=45)
                    plt.tight_layout()
                    st.pyplot(fig)
                    plt.close()

                # LTæ›²çº¿æ¯”è¾ƒ
                if visualization_data:
                    fig_curves, _, _ = visualize_lt_curves(visualization_data, years=lt_years)
                    st.pyplot(fig_curves)
                    plt.close()

                # æ‹Ÿåˆæ•ˆæœæ¯”è¾ƒ
                if visualization_data and original_data:
                    fig_fitting = visualize_fitting_comparison(original_data, visualization_data)
                    st.pyplot(fig_fitting)
                    plt.close()

        st.markdown('</div>', unsafe_allow_html=True)

elif current_page == "ARPUè®¡ç®—":
    if st.session_state.lt_results is None:
        show_dependency_warning("LTæ‹Ÿåˆåˆ†æ")

    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.subheader("ARPUæ•°æ®å¤„ç†")

    arpu_file = st.file_uploader("é€‰æ‹©ARPUæ•°æ®æ–‡ä»¶ (Excelæ ¼å¼)", type=['xlsx', 'xls'])

    if arpu_file:
        try:
            arpu_df = pd.read_excel(arpu_file)
            st.success("ARPUæ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
            st.dataframe(arpu_df.head(10), use_container_width=True)

            col1, col2 = st.columns(2)
            with col1:
                source_col = st.selectbox("æ•°æ®æ¥æºåˆ—", options=arpu_df.columns)
                arpu_col = st.selectbox("ARPUå€¼åˆ—", options=arpu_df.columns)

            with col2:
                if arpu_col in arpu_df.columns:
                    arpu_values = pd.to_numeric(arpu_df[arpu_col], errors='coerce')
                    st.metric("å¹³å‡ARPU", f"{arpu_values.mean():.2f}")
                    st.metric("æœ‰æ•ˆè®°å½•æ•°", f"{arpu_values.notna().sum():,}")

            if st.button("å¤„ç†å¹¶ä¿å­˜ARPUæ•°æ®", type="primary", use_container_width=True):
                try:
                    processed_arpu = arpu_df.copy()
                    processed_arpu['data_source'] = processed_arpu[source_col].astype(str).str.strip()
                    processed_arpu['arpu_value'] = pd.to_numeric(processed_arpu[arpu_col], errors='coerce')

                    valid_data = processed_arpu[
                        processed_arpu['arpu_value'].notna() & (processed_arpu['arpu_value'] > 0)]
                    arpu_summary = valid_data.groupby('data_source')['arpu_value'].agg(['mean', 'count']).reset_index()
                    arpu_summary.columns = ['data_source', 'arpu_value', 'record_count']

                    st.session_state.arpu_data = arpu_summary
                    st.success("ARPUæ•°æ®å¤„ç†å®Œæˆï¼")
                    st.dataframe(arpu_summary, use_container_width=True)

                except Exception as e:
                    st.error(f"ARPUæ•°æ®å¤„ç†å¤±è´¥ï¼š{str(e)}")

        except Exception as e:
            st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")

    else:
        st.info("è¯·ä¸Šä¼ ARPUæ•°æ®æ–‡ä»¶ï¼Œæˆ–ä½¿ç”¨æ‰‹åŠ¨è®¾ç½®åŠŸèƒ½")

        if st.session_state.lt_results:
            st.subheader("æ‰‹åŠ¨è®¾ç½®ARPUå€¼")
            arpu_inputs = {}

            col1, col2 = st.columns(2)
            for i, result in enumerate(st.session_state.lt_results):
                source = result['data_source']
                with col1 if i % 2 == 0 else col2:
                    arpu_value = st.number_input(
                        f"{source}", min_value=0.0, value=10.0, step=0.01,
                        format="%.2f", key=f"arpu_{source}"
                    )
                    arpu_inputs[source] = arpu_value

            if st.button("ä¿å­˜æ‰‹åŠ¨ARPUè®¾ç½®", type="primary", use_container_width=True):
                arpu_df = pd.DataFrame([
                    {'data_source': source, 'arpu_value': value, 'record_count': 1}
                    for source, value in arpu_inputs.items()
                ])
                st.session_state.arpu_data = arpu_df
                st.success("ARPUè®¾ç½®å·²ä¿å­˜ï¼")
                st.dataframe(arpu_df, use_container_width=True)

    st.markdown('</div>', unsafe_allow_html=True)

elif current_page == "LTVç»“æœæŠ¥å‘Š":
    if st.session_state.lt_results is None:
        show_dependency_warning("LTæ‹Ÿåˆåˆ†æ")
    elif st.session_state.arpu_data is None:
        show_dependency_warning("ARPUè®¡ç®—")
    else:
        lt_results = st.session_state.lt_results
        arpu_data = st.session_state.arpu_data
        retention_data = st.session_state.retention_data

        ltv_results = []
        ltv_2y_results = []
        ltv_5y_results = []

        for lt_result in lt_results:
            source = lt_result['data_source']
            lt_value = lt_result['lt_value']

            arpu_row = arpu_data[arpu_data['data_source'] == source]
            if not arpu_row.empty:
                arpu_value = arpu_row.iloc[0]['arpu_value']
            else:
                arpu_value = 0
                st.warning(f"æ¸ é“ '{source}' æœªæ‰¾åˆ°ARPUæ•°æ®")

            lt_value_2y = lt_value * 0.6
            ltv_value = lt_value * arpu_value
            ltv_value_2y = lt_value_2y * arpu_value

            ltv_results.append({
                'data_source': source,
                'lt_value': lt_value,
                'arpu_value': arpu_value,
                'ltv_value': ltv_value,
                'fit_success': lt_result['fit_success'],
                'model_used': lt_result.get('model_used', 'unknown')
            })

            ltv_2y_results.append({'data_source': source, 'ltv_2y': ltv_value_2y})
            ltv_5y_results.append({'data_source': source, 'ltv_5y': ltv_value})

        st.session_state.ltv_results = ltv_results

        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("LTVç»¼åˆè®¡ç®—ç»“æœ")

        ltv_df = pd.DataFrame(ltv_results)
        display_df = ltv_df.rename(columns={
            'data_source': 'æ•°æ®æ¥æº',
            'lt_value': 'LTå€¼',
            'arpu_value': 'ARPU',
            'ltv_value': 'LTV',
            'fit_success': 'æ‹ŸåˆçŠ¶æ€',
            'model_used': 'ä½¿ç”¨æ¨¡å‹'
        })

        display_df['LTå€¼'] = display_df['LTå€¼'].round(2)
        display_df['ARPU'] = display_df['ARPU'].round(2)
        display_df['LTV'] = display_df['LTV'].round(2)
        display_df['æ‹ŸåˆçŠ¶æ€'] = display_df['æ‹ŸåˆçŠ¶æ€'].map({True: 'æˆåŠŸ', False: 'å¤±è´¥'})

        st.dataframe(display_df, use_container_width=True)
        st.markdown('</div>', unsafe_allow_html=True)

        # 2å¹´å’Œ5å¹´LTVæ’åå¯¹æ¯”
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("å„æ¸ é“2å¹´5å¹´LTVæ’åå¯¹æ¯”")

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))

        # 2å¹´LTVæ’å
        ltv_2y_df = pd.DataFrame(ltv_2y_results).sort_values('ltv_2y', ascending=True)
        colors_2y = plt.cm.Set1(np.linspace(0, 1, len(ltv_2y_df)))
        bars1 = ax1.barh(ltv_2y_df['data_source'], ltv_2y_df['ltv_2y'], color=colors_2y, alpha=0.8)

        for bar, value in zip(bars1, ltv_2y_df['ltv_2y']):
            width = bar.get_width()
            ax1.text(width + width * 0.01, bar.get_y() + bar.get_height() / 2,
                     f'{value:.1f}', ha='left', va='center', fontweight='bold')

        ax1.set_xlabel('2å¹´LTVå€¼')
        ax1.set_ylabel('æ•°æ®æ¥æº')
        ax1.set_title('å„æ¸ é“2å¹´LTVæ’å')
        if retention_data:
            # å‡†å¤‡å¯è§†åŒ–æ•°æ®
            visualization_data_2y = {}
            visualization_data_5y = {}
            original_data = {}

            for retention_result in retention_data:
                channel_name = retention_result['data_source']
                
                # è®¡ç®—2å¹´LTå’Œæ›²çº¿æ•°æ®
                lt_result_2y = calculate_lt_advanced(
                    retention_result, channel_name, lt_years=2, 
                    return_curve_data=True, key_days=[1, 7, 30, 60, 90]
                )
                
                # è®¡ç®—5å¹´LTå’Œæ›²çº¿æ•°æ®
                lt_result_5y = calculate_lt_advanced(
                    retention_result, channel_name, lt_years=5, 
                    return_curve_data=True, key_days=[1, 7, 30, 60, 90]
                )

                # ä¿å­˜å¯è§†åŒ–æ•°æ®
                visualization_data_2y[channel_name] = {
                    "days": lt_result_2y['curve_days'],
                    "rates": lt_result_2y['curve_rates'],
                    "lt": lt_result_2y['lt_value']
                }

                visualization_data_5y[channel_name] = {
                    "days": lt_result_5y['curve_days'],
                    "rates": lt_result_5y['curve_rates'],
                    "lt": lt_result_5y['lt_value']
                }

                # ä¿å­˜åŸå§‹æ•°æ®
                original_data[channel_name] = {
                    "days": retention_result['days'],
                    "rates": retention_result['rates']
                }

            # 1. æ‹Ÿåˆæ•ˆæœæ¯”è¾ƒå›¾
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            st.subheader("æ‹Ÿåˆæ•ˆæœæ¯”è¾ƒå›¾")
            if visualization_data_2y and original_data:
                fig_fitting = visualize_fitting_comparison(original_data, visualization_data_2y)
                st.pyplot(fig_fitting)
                plt.close()
            st.markdown('</div>', unsafe_allow_html=True)

            # 2. 2å¹´LTæ›²çº¿
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            st.subheader("2å¹´LTç•™å­˜æ›²çº¿æ¯”è¾ƒ")
            if visualization_data_2y:
                fig_lt_2y, colors_2y, sorted_channels_2y = visualize_lt_curves(visualization_data_2y, years=2)
                st.pyplot(fig_lt_2y)
                plt.close()
            st.markdown('</div>', unsafe_allow_html=True)

            # 3. 5å¹´LTæ›²çº¿
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            st.subheader("5å¹´LTç•™å­˜æ›²çº¿æ¯”è¾ƒ")
            if visualization_data_5y:
                fig_lt_5y, colors_5y, sorted_channels_5y = visualize_lt_curves(visualization_data_5y, years=5)
                st.pyplot(fig_lt_5y)
                plt.close()
            st.markdown('</div>', unsafe_allow_html=True)

            # 4. å¯¹æ•°åæ ‡æ¯”è¾ƒå›¾
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            st.subheader("å¯¹æ•°åæ ‡æ¯”è¾ƒå›¾")
            if visualization_data_2y and visualization_data_5y:
                fig_log_comparison = visualize_log_comparison(
                    visualization_data_2y, visualization_data_5y, 
                    colors_2y if 'colors_2y' in locals() else None, 
                    sorted_channels_2y if 'sorted_channels_2y' in locals() else None
                )
                st.pyplot(fig_log_comparison)
                plt.close()
            st.markdown('</div>', unsafe_allow_html=True)

        # æ•°æ®å¯¼å‡º
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.subheader("åˆ†æç»“æœå¯¼å‡º")

        col1, col2 = st.columns(2)

        with col1:
            csv_data = display_df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ä¸‹è½½LTVåˆ†æç»“æœ (CSV)",
                data=csv_data,
                file_name=f"LTV_Analysis_Results_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                mime="text/csv",
                use_container_width=True
            )

        with col2:
            report_text = f"""
LTVç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†ææŠ¥å‘Š
===========================================
ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

æ‰§è¡Œæ‘˜è¦
-----------
æœ¬æŠ¥å‘ŠåŸºäºåˆ†é˜¶æ®µæ•°å­¦å»ºæ¨¡æ–¹æ³•ï¼Œå¯¹ {len(display_df)} ä¸ªæ•°æ®æ¥æºè¿›è¡Œäº†ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†æã€‚

æ ¸å¿ƒæŒ‡æ ‡æ±‡æ€»
-----------
â€¢ å‚ä¸åˆ†æçš„æ¸ é“æ•°é‡: {len(display_df)}
â€¢ å¹³å‡LTV: {display_df['LTV'].mean():.2f}
â€¢ æœ€é«˜LTV: {display_df['LTV'].max():.2f}
â€¢ æœ€ä½LTV: {display_df['LTV'].min():.2f}
â€¢ å¹³å‡LTå€¼: {display_df['LTå€¼'].mean():.2f} å¤©
â€¢ å¹³å‡ARPU: {display_df['ARPU'].mean():.2f}

è®¡ç®—å…¬å¼: LTV = LT Ã— ARPU

æŠ¥å‘Šç”Ÿæˆ: LTVæ™ºèƒ½åˆ†æå¹³å° v2.0
"""

            st.download_button(
                label="ä¸‹è½½è¯¦ç»†åˆ†ææŠ¥å‘Š (TXT)",
                data=report_text,
                file_name=f"LTV_Detailed_Report_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.txt",
                mime="text/plain",
                use_container_width=True
            )

        st.markdown('</div>', unsafe_allow_html=True)

# ==================== åº•éƒ¨ä¿¡æ¯ ====================
# åº•éƒ¨ä¿¡æ¯
with st.sidebar:
    st.markdown("---")
    st.markdown("""
    <div class="nav-container">
        <h4 style="text-align: center; color: #495057;">ä½¿ç”¨æŒ‡å—</h4>
        <p style="font-size: 0.9rem; color: #6c757d; text-align: center;">
        ç‚¹å‡»ä¸Šæ–¹æ­¥éª¤å¯ç›´æ¥è·³è½¬ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æŸ¥ä¾èµ–å…³ç³»å¹¶æä¾›ç›¸åº”æç¤ºã€‚
        </p>
        <p style="font-size: 0.8rem; color: #adb5bd; text-align: center;">
        LTVæ™ºèƒ½åˆ†æå¹³å° v2.0<br>
        åŸºäºåˆ†é˜¶æ®µæ•°å­¦å»ºæ¨¡
        </p>
    </div>
    """, unsafe_allow_html=True)
